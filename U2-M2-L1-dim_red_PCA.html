
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Dimensionality Reduction and PCA &#8212; Lecture Notes in Unsupervised and Reinforcement Learning</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Principal Component Analysis" href="assignments-dummy/U2-M2-L1-dim_red_PCA.html" />
    <link rel="prev" title="Warning:" href="assignments-dummy/U2-M1-L1-similarity_metrics.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/UPY-completo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Lecture Notes in Unsupervised and Reinforcement Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome!
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 1 - Unsupervised Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L1-prep-normalization.html">
   Data Normalization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L1-prep-normalization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L2-prep-discretization.html">
   Discretization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L2-prep-discretization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L3-unsup-outlier-detection.html">
   Anomaly and Outlier Detection
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L3-prep-outlier-detection.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 2 - Dimensionality Reduction
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U2-M1-L1-similarity_metrics.html">
   (Dis)similarity metrics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html">
     <strong>
      Warning
     </strong>
     :
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-1-2-pt">
     Exercise 1 (2 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-2-3-pt">
     Exercise 2 (3 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-3-4-pts">
     Exercise 3 (4 pts)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-4-1-pt">
     Exercise 4 (1 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-5-ungraded">
     Exercise 5 (ungraded)
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="current reference internal" href="#">
   Dimensionality Reduction and PCA
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M2-L1-dim_red_PCA.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M2-L2-PCA_variants.html">
   Some linear and non-linear variants of PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M2-L3-Other_Linear_Methods_for_DR.html">
   Other Dimensionality Reduction Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M2-L4-Non_Linear_Methods_for_DR.html">
   Non-linear Dimensionality Reduction
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 3 - Clustering
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L1-Prototype_clustering.html">
   Prototype Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L2-Hierarchical_clustering.html">
   Hierarchical Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L3-Density_based_clustering.html">
   Density Based Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L4-Other_clustering_methods.html">
   Other Clustering Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M2-L1-Clustering_evaluation.html">
   Clustering Evaluation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 4 - Probabilistic Methods
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="U4-M1-L1-Latent_variable_models.html">
   Latent Variable Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U4-M1-L2-Independent_Component_Analysis.html">
   Independent Component Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U4-M2-L1-Gaussian_Mixture_Models.html">
   Gaussian Mixture Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="freedman-diaconis.html">
   Freedman-Diaconis Rule
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/U2-M2-L1-dim_red_PCA.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/gperaza/UL-lecture-notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/gperaza/UL-lecture-notes/issues/new?title=Issue%20on%20page%20%2FU2-M2-L1-dim_red_PCA.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principal-component-analysis-pca">
   Principal Component Analysis (PCA)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pca-through-variance-maximization">
     PCA through variance maximization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#minimizing-the-least-square-reconstruction-error">
     Minimizing the least-square reconstruction error
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#considerations">
     Considerations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-handwritten-digits">
     Example: Handwritten digits
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-whitening">
     Example: Whitening
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-visualization">
     Example: Visualization
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-biplot">
       The Biplot
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limitations-of-standard-pca">
     Limitations of standard PCA
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="dimensionality-reduction-and-pca">
<h1>Dimensionality Reduction and PCA<a class="headerlink" href="#dimensionality-reduction-and-pca" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Feature extraction refers to a set of techniques to build derivative features from the original features present in the data set. The new features can be linear or non-linear combinations of the original ones. The new features aim to capture the patterns on the data, while removing noise and redundancy, improving the performance of learning algorithms. Also, some algorithms work better with particular representations of a data set.</p>
<p>In the case where the number of extracted features is less than the number of original features, we are said to be performing dimensionality reduction. A reduced number of dimensions alleviates the <strong>curse of dimensionality</strong>, improving learning performance, as well as reducing the size of data set, improving computation time.</p>
</div>
<div class="section" id="principal-component-analysis-pca">
<h2>Principal Component Analysis (PCA)<a class="headerlink" href="#principal-component-analysis-pca" title="Permalink to this headline">¶</a></h2>
<p>The most popular method for feature extraction is Principal Component Analysis (PCA). In simple terms, PCA searches for a rotation (orthogonal linear transformation) of the axes in data space (features) in which the transformed features are uncorrelated, i.e., the rotated covariance matrix is diagonal. The diagonal entries of the covariance matrix are, then, the variance along each of the new axes. Finally, dimensionality reduction is performed by dropping the features with the lowest variance, under the assumption that those are the least informative, or contribute less to the reconstruction error.</p>
<script src="pca-rot.js" id="e6dca842-cfcf-494e-a114-10b591ed8136"></script><div class="section" id="pca-through-variance-maximization">
<h3>PCA through variance maximization<a class="headerlink" href="#pca-through-variance-maximization" title="Permalink to this headline">¶</a></h3>
<p>There are several ways to justify that specific transformation. Let’s to approach the problem directly, and find the set of orthogonal directions for which the variance is maximized. We will call this directions the <strong>loading vectors</strong>, the projections onto them the <strong>principal components</strong>, and we order them from larges variance to smallest. The first principal component is, thus, the direction along which the variance is larger.</p>
<p>To find the variance along an arbitrary direction <span class="math notranslate nohighlight">\(w\)</span>, we first take the projection of each centered observation along <span class="math notranslate nohighlight">\(w\)</span>, then take the variance. During this lecture, we will denote the centered data matrix by <span class="math notranslate nohighlight">\(X\)</span> to simplify notation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
Var(w^T x) =&amp; \frac{1}{n}\sum_i \left(w^T(x_i - \mu)\right)^{2}\\
=&amp; \frac{1}{n}\sum_i w^T \tilde{x}_i \tilde{x}_i^T w\\
=&amp; \frac{1}{n} w^T \sum_i\left(\tilde{x}_i \tilde{x}_i^T\right) w\\
=&amp; \frac{1}{n} w^T X_c^T X_c w\\
=&amp;  w^T \Sigma  w
\end{align}\end{split}\]</div>
<p>So, to find the first principal component, we need to solve</p>
<div class="math notranslate nohighlight">
\[
w^{*} = \underset{\mid w \mid=1}{\operatorname{argmax}}\ w^T\Sigma w
\]</div>
<p>Which is easier than it seems <span id="id1">[<a class="reference internal" href="#id55"><span>1</span></a>]</span>. Let’s assume <span class="math notranslate nohighlight">\(\Sigma\)</span> is already diagonal, with entries <span class="math notranslate nohighlight">\(\lambda_i\)</span> along the diagonal, and <span class="math notranslate nohighlight">\(\lambda_1 \geq \ldots \geq \lambda_d\)</span>. For any unit vector <span class="math notranslate nohighlight">\(w\)</span></p>
<div class="math notranslate nohighlight">
\[
w^T \Sigma w = \sum_{l,i,j} w_i x_{li} x_{lj} w_j
= \sum_l \lambda_{l} w_i^2 \leq \lambda_1\sum w_i^2 = \lambda_1 = e_1^T\Sigma e_1
\]</div>
<p>Then it’s easy to see that <span class="math notranslate nohighlight">\(w^* = e_i\)</span>, the first basis versor. In the general case of a non-diagonal matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>, we can always apply the eigen-decomposition</p>
<div class="math notranslate nohighlight">
\[
\Sigma = W \Lambda W^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is the orthogonal matrix with the eigenvectors of <span class="math notranslate nohighlight">\(\Sigma\)</span> as columns, and <span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix with . So, plugin the decomposition into the previous solution</p>
<div class="math notranslate nohighlight">
\[
(e_1^T W^T) \Sigma (W e_1)
\]</div>
<p>from which we obtain the general loading <span class="math notranslate nohighlight">\(w^* = W e_1\)</span>, the first column of <span class="math notranslate nohighlight">\(W\)</span>, i.e., the first eigenvector of <span class="math notranslate nohighlight">\(\Sigma\)</span>.</p>
<p>For the next loading vectors and components, we add the restriction that the new loading must be perpendicular to the previously vectors.</p>
<div class="math notranslate nohighlight">
\[
w_k = \underset{\mid w \mid=1, w \perp w_1,...,w_{k-1}}{\operatorname{argmax}}\ w^T\Sigma w
\]</div>
<p>Finding <span class="math notranslate nohighlight">\(w_{k}\)</span> is analogous to finding <span class="math notranslate nohighlight">\(w_1\)</span>, working in the reduced space after removing the previous <span class="math notranslate nohighlight">\(k-1\)</span>, dimensions. So the solution must be also an eigenvector of <span class="math notranslate nohighlight">\(\Sigma\)</span>. In fact, the whole set of eigenvectors of <span class="math notranslate nohighlight">\(\Sigma\)</span> define the directions of the principal components, and the eigenvalues are the variances along that very directions. As required, all the eigenvalues are positive, since the covariance matrix is positive semi-definite.</p>
<p>The principal components of a centered observation vector <span class="math notranslate nohighlight">\(x_c\)</span> are the coordinates of that vector in the space spanned by the eigenvectors, <span class="math notranslate nohighlight">\(w_i^T x_c\)</span>, with the projected vector given by <span class="math notranslate nohighlight">\(W^T x_{c}\)</span>, and the variance along that direction given by the corresponding eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>. The data matrix of the principal components is then the rotated centered data matrix,</p>
<div class="math notranslate nohighlight">
\[
X_{PC} = X_c W
\]</div>
<p>To summarize, from an optimization point of view, we seek to maximize <span class="math notranslate nohighlight">\(w^T\Sigma w\)</span> subject to the restriction <span class="math notranslate nohighlight">\(w^T w = 1\)</span> <span id="id2">[<a class="reference internal" href="#id57"><span>2</span></a>]</span>. Using Lagrange multipliers, we need to maximize <span class="math notranslate nohighlight">\(w^T\Sigma w - \lambda (w^T w - 1)\)</span>. Differentiating with respect to <span class="math notranslate nohighlight">\(w\)</span> and equating the derivative to 0, we obtain</p>
<div class="math notranslate nohighlight">
\[
\Sigma w - \lambda w = 0 \iff \Sigma w = \lambda w
\]</div>
<p>which is precisely the eigenproblem equation.</p>
<p>In the previous derivation we focused on PCA as a linear transformation that identifies directions of maximal variance. Next, we explore two derivations focusing on identifying useful sub-spaces to perform dimensionality reduction.</p>
</div>
<div class="section" id="minimizing-the-least-square-reconstruction-error">
<h3>Minimizing the least-square reconstruction error<a class="headerlink" href="#minimizing-the-least-square-reconstruction-error" title="Permalink to this headline">¶</a></h3>
<p>In the previous section, a natural interpretation of the procedure is that of fitting a multivariate Gaussian, defined by <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\Sigma\)</span> to the data. The idea here is to reinterpret the same process as of fitting a linear model to the data, where the fitted hyperplane is of dimension <span class="math notranslate nohighlight">\(q&lt;d\)</span>. We follow the discussion in <span id="id3">[<a class="reference internal" href="U3-M1-L2-Hierarchical_clustering.html#id50"><span>1</span></a>]</span>, section 14.5.</p>
<p>The equation of the fitting hyper-plane of dimension <span class="math notranslate nohighlight">\(q&lt;d\)</span> is</p>
<div class="math notranslate nohighlight">
\[
f(y) = \mu + W_{:q}y,
\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> is the q-dimensional vector with the reduced parametric coordinates of a point in the plane, <span class="math notranslate nohighlight">\(\mu\)</span> is the mean vector, a point in the plane, and <span class="math notranslate nohighlight">\(W_{:q}\)</span> is an orthogonal matrix with <span class="math notranslate nohighlight">\(q\)</span> unit vectors as columns. The <span class="math notranslate nohighlight">\(q\)</span> columns of <span class="math notranslate nohighlight">\(W_{:q}\)</span> are vectors parallel to the plane, so the product <span class="math notranslate nohighlight">\(W_{:q}y\)</span> is a linear combination of those vectors that explore the plane as we change the values of <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>To fit the model, we seek to minimize the <strong>reconstruction</strong> error, by optimizing for <span class="math notranslate nohighlight">\(\mu\)</span>, the set of coordinate vectors <span class="math notranslate nohighlight">\(\{y_i\}\)</span> and the matrix <span class="math notranslate nohighlight">\(W_{:q}\)</span></p>
<div class="math notranslate nohighlight">
\[
\underset{\mu,\{\lambda_i\},W_{:q}}{\min}
\sum_{i=1}^n \left | x_i - \mu - W_{:q} y_i \right |^2
\]</div>
<p>Derivating with respect to <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(y_{i}\)</span> allows us to optimize jointly for (exercise)</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\mu^{*} =&amp; \bar{x}\\
y_i^{*} =&amp; W_{:q}^T(x_i - \bar{x}).
\end{align}\end{split}\]</div>
<p>As you may have already guessed, the optimal matrix <span class="math notranslate nohighlight">\(W_{:q}\)</span> will turn out to be the matrix of eigenvectors of <span class="math notranslate nohighlight">\(\Sigma\)</span>. This makes the <span class="math notranslate nohighlight">\(y_i\)</span> the first <span class="math notranslate nohighlight">\(q\)</span> principal components of <span class="math notranslate nohighlight">\(x\)</span>. Now we find <span class="math notranslate nohighlight">\(W_{:q}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\underset{W_{q:}}{\min} \sum_i^n
\left |
\tilde{x}_i - W_{:q}W_{:q}^T\tilde{x}_i
\right |^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{x}_i = x_i - \bar{x}\)</span>. The <span class="math notranslate nohighlight">\(d\times d\)</span> matrix <span class="math notranslate nohighlight">\(W W^T\)</span> is a projection matrix. It first projects each point <span class="math notranslate nohighlight">\(\tilde{x}_i\)</span> into the hyper-plane by obtaining each component along each column of <span class="math notranslate nohighlight">\(W\)</span>. This a q-dimensional representation in the sub-space. Next, we move back into the original d-dimensional space by multiplying by <span class="math notranslate nohighlight">\(W\)</span>, effectively taking a linear combination of the unit vectors, with each projected component as the weights.</p>
<p>Exercise: Show the minimizing the reconstruction error is equivalent as maximizing the variance along the first q directions of <span class="math notranslate nohighlight">\(W_{:q}\)</span>. Use matrix algebra to transform into equivalent expressions.</p>
<p>The above exercise shows that the solution is the same as in the previous section, and the solution <span class="math notranslate nohighlight">\(W_{:q}\)</span> contains the first <span class="math notranslate nohighlight">\(q\)</span> eigenvectors of <span class="math notranslate nohighlight">\(\Sigma\)</span>, as before.</p>
<p>We can connect the solution with another matrix decomposition, namely the Singular Value Decomposition (SVD) of the data matrix <span class="math notranslate nohighlight">\(X\)</span>,</p>
<div class="math notranslate nohighlight">
\[
X = UDW^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(W\)</span> are <span class="math notranslate nohighlight">\(n\times d\)</span> and <span class="math notranslate nohighlight">\(d\times d\)</span> orthogonal matrices (if X is full rank) whose columns are called the left and right singular vectors respectively. <span class="math notranslate nohighlight">\(D\)</span> is <span class="math notranslate nohighlight">\(d\times d\)</span> diagonal matrix, whose entries are called the singular values, and are ordered so <span class="math notranslate nohighlight">\(d_1 \geq \ldots \geq d_d\)</span>. The covariance matrix is <span class="math notranslate nohighlight">\(\frac{1}{n}X^T X = \frac{1}{n}W D^T U^T U D W^T = W \frac{1}{n}D^T  D W^T = W \Lambda W^T\)</span>, so we identify the singular values as square root of the eigenvalues of <span class="math notranslate nohighlight">\(X^TX\)</span>, or <span class="math notranslate nohighlight">\(\sqrt{n}\)</span> times the standard deviations along the principal directions. The principal components matrix is given by <span class="math notranslate nohighlight">\(XW = UDW^T W = UD\)</span>, and the optimal <span class="math notranslate nohighlight">\(y_{i}\)</span> are given by the first <span class="math notranslate nohighlight">\(q\)</span> columns of <span class="math notranslate nohighlight">\(UD\)</span>, with each <span class="math notranslate nohighlight">\(y_i\)</span> being a row (observation).</p>
<p>So, how do we measure the quality of the reconstruction? We can calculate the fraction to the total variance retained by the reconstruction. The total variance along all the components, in either representation, can be found from the trace of the covariance matrix, <span class="math notranslate nohighlight">\(\sum \sigma^2_i = tr(\Sigma) = tr(\Lambda)\)</span>, thus the fraction of the variance retained after projection into the q-dimensional subspace spanned by <span class="math notranslate nohighlight">\(W_{:q}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\frac{\sum_{i=1}^q \lambda_i}{tr(\Sigma)}
\]</div>
<p>Finally, we can rewrite the fitted hyperplane equation as to make the expansion into PC more explicit,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
x_{i}^{rec} =&amp; \mu + W_{:q}y\\
=&amp; \bar{x} + \sum_{i=1}^q \left( \left (x_i - \bar{x} \right)^T w_i \right) w_i
\end{align}\end{split}\]</div>
</div>
<div class="section" id="considerations">
<h3>Considerations<a class="headerlink" href="#considerations" title="Permalink to this headline">¶</a></h3>
<p>It is a good idea to standardize the data matrix before applying PCA, since the particular choice of units may artificially inflate one feature variance with respect to others, thus biasing the first principal component along that direction. This is equivalent to diagonalize the correlation matrix. Note that in this case, the trace of <span class="math notranslate nohighlight">\(\Lambda\)</span> is not the fraction of total variance, but just the number of variables used in the reconstruction, and to obtain the fraction of total variance corresponding to each PC, we need to divide the corresponding eigenvalue by <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>Also, components associated with the smallest eigenvalues (variances), especially is the difference with previous eigenvalues is large, indicate possible linear relations in the data set. This small variance components can be regarded as random noise on top of a linear model.</p>
</div>
<div class="section" id="example-handwritten-digits">
<h3>Example: Handwritten digits<a class="headerlink" href="#example-handwritten-digits" title="Permalink to this headline">¶</a></h3>
<p>To illustrate PCA decomposition and dimensionality reduction we’ll use the MNIST dataset of handwritten digits. From <a class="reference external" href="http://deeplearningtutorial.net">deeplearningtutorial.net</a>:</p>
<blockquote>
<div><p>The MNIST dataset consists of handwritten digit images and it is divided in 60,000 examples for the training set and 10,000 examples for testing. In many papers as well as in this tutorial, the official training set of 60,000 is divided into an actual training set of 50,000 examples and 10,000 validation examples (for selecting hyper-parameters like learning rate and size of the model). All digit images have been size-normalized and centered in a fixed size image of 28 x 28 pixels. In the original dataset each pixel of the image is represented by a value between 0 and 255, where 0 is black, 255 is white and anything in between is a different shade of grey.</p>
</div></blockquote>
<p>Here are some examples of MNIST digits:</p>
<p><img alt="" src="_images/mnist.png" /></p>
<p>For convenience we’ll the pickled dataset from <a class="reference external" href="http://deeplearningtutorial.net">deeplearningtutorial.net</a>. The pickled file represents a tuple of 3 lists: the training set, the validation set and the testing set. Each of the three lists is a pair formed from a list of images and a list of class labels for each of the images. An image is represented as numpy 1-dimensional array of 784 (28 x 28) float values between 0 and 1 (0 stands for black, 1 for white). The labels are numbers between 0 and 9 indicating which digit the image represents.</p>
<p>First, import the modules we will use</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gzip</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
</pre></div>
</div>
<p>Now, lets import the data set</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The pickle database is already formated as to output training,</span>
<span class="c1"># validation and test sets.</span>
<span class="c1"># We first uncompress on the fly to avoid keeping the uncompressed</span>
<span class="c1"># database on disk</span>

<span class="n">pkl_file</span> <span class="o">=</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;Data/mnist.pkl.gz&#39;</span><span class="p">)</span>
<span class="n">train_set</span><span class="p">,</span> <span class="n">validation_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">pkl_file</span><span class="p">,</span>
                                                  <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;latin1&#39;</span><span class="p">)</span>
<span class="n">pkl_file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="c1"># Now we create a single data set from all three subsets</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">train_set</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validation_set</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">test_set</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">train_set</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">validation_set</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">test_set</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Each row of a data matrix is a digit,</span>
<span class="c1"># the following function will be useful to visualize them.</span>
<span class="k">def</span> <span class="nf">view_digit</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">example</span><span class="p">]</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">example</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example: </span><span class="si">%d</span><span class="s1">  Label: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;gray&#39;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">view_digit</span><span class="p">(</span><span class="mi">4000</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/f996989126cf3e4d6ba7e851918fd9fa467b777d.png" /></p>
<p>In this example, we will work exclusively with the digit 3 to visualize the PC.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X3</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<p>In the assignments, you will implemented PCA from scratch using only numpy. Here we will demonstrate how to use the PCA class of <code class="docutils literal notranslate"><span class="pre">sklearn.decomposition</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Lets keep only the first 49 principal components</span>
<span class="c1"># to make things quick.</span>
<span class="c1"># Initialize the class and obtain the components.</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">49</span><span class="p">)</span>
<span class="n">X_PCA</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X3</span><span class="p">)</span>
</pre></div>
</div>
<p>Remember, the eigenvalues of the diagonalized covariance matrix represent the amount of variance each component explains. The sum of the first 49 eigenvalues, give us the total variance explained in the reduced dimensions. First we present a plot of the eigenvalues, next a plot of the accumulated variance explained up to component <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Percentage of Variance Captured by 49 principal components</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fraction of variance explained by 49 principal components: &quot;</span><span class="p">,</span>
      <span class="nb">sum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>

<span class="c1"># Plot of the eigenvaluas of the covariance matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\lambda_i$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$i$&#39;</span><span class="p">);</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>Fraction of variance explained by 49 principal components:  0.8378539625555277
</pre></div>
</div>
<p><img alt="" src="_images/e56f7dc640fdc04c132fb2338e8b42b271958ac9.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot of the variance explained by the first n components</span>
<span class="n">var_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[:</span><span class="n">i</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))),</span>
         <span class="n">var_list</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$i$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\frac{\sum_1^i\lambda_i}{\sum_1^D\lambda_i}$&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/ef5efade30ec4f8fc7eb71eb29bfe41e7a0dc86d.png" /></p>
<p>We now visualize the first two principal components together with key observations located at key quantiles along each component.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Identify the 5%, 25%, 50%, 75%, 95% quantiles along both components</span>
<span class="n">pc1</span> <span class="o">=</span> <span class="n">X_PCA</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">pc2</span> <span class="o">=</span> <span class="n">X_PCA</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Exact quantiles</span>
<span class="n">qt1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">pc1</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
<span class="n">qt2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">pc2</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">])</span>

<span class="c1"># Plotting the first two principal components</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_PCA</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_PCA</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># Quantile grid</span>
<span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">qt1</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">qt2</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="c1"># Nearest points to quantiles</span>
<span class="k">def</span> <span class="nf">closest_node</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">nodes</span><span class="p">):</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
    <span class="n">dist_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">nodes</span> <span class="o">-</span> <span class="n">node</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dist_2</span><span class="p">)</span>
<span class="c1"># Create list of nearest points starting at top left corner</span>
<span class="n">examples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="n">qt2</span><span class="p">,</span> <span class="n">qt1</span><span class="p">):</span>
    <span class="n">closest</span> <span class="o">=</span> <span class="n">closest_node</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">pc1</span><span class="p">,</span> <span class="n">pc2</span><span class="p">)))</span>
    <span class="n">examples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">closest</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pc1</span><span class="p">[</span><span class="n">closest</span><span class="p">],</span> <span class="n">pc2</span><span class="p">[</span><span class="n">closest</span><span class="p">],</span> <span class="n">facecolors</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/6138e8bb6c675a0c44e84ccd2966f9e7fc05ce4a.png" /></p>
<p>We now visualize the observations circled in red in an array of the same shape as in the plot. Note how moving left to right amounts to tilt the digit to the right (PC1), while moving top to bottom changes the shape of the lower lobe.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">examples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">X3</span><span class="p">[</span><span class="n">examples</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;gray&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p><img alt="" src="_images/3582edcd38b82ad7b9b3dbfd39be440cd7de768e.png" /></p>
<p>To understand the effect of each component, we can think of each digit as a weighted sum of the unit vectors <span class="math notranslate nohighlight">\(w_i\)</span>, with the weights given by the principal components of each observation. Each <span class="math notranslate nohighlight">\(w_i\)</span> can thus be interpreted as an image, and the weighted sum of such images reconstructs each digit. Below we plot the first 49 vectors <span class="math notranslate nohighlight">\(w_i\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># A grid of the first 49 components</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">l</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">),</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)):</span>
    <span class="n">base</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;gray&#39;</span><span class="p">))</span>
    <span class="n">l</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<p><img alt="" src="_images/fdcd4ba7eee85a941fccaa295248bf83bc6277db.png" /></p>
<p>Focus in particular in the first two components and how they reflect the changes discussed before (tilt and shape).</p>
<p>To visualize the reconstruction, we begin with the mean and subsequently add the weighted components. Lets sum the first 3 components for a single example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">example</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">X3_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X3_mean</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;gray&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Mean&#39;</span><span class="p">)</span>

<span class="n">j</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.015</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39; </span><span class="si">{</span><span class="n">X_PCA</span><span class="p">[</span><span class="n">example</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s1">0.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">j</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;gray&#39;</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;$u_</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="s1">&#39;=&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">35</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X3</span><span class="p">[</span><span class="n">example</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;gray&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/36c2e628d12486d93ac6abb403e536efe6048aed.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">recons</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="n">rec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X3_mean</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">X_PCA</span><span class="p">[</span><span class="n">example</span><span class="p">,</span> <span class="p">:</span><span class="n">m</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
        <span class="n">rec</span> <span class="o">+=</span> <span class="n">l</span> <span class="o">*</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X3</span><span class="p">[</span><span class="n">example</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;gray&#39;</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">rec</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;gray&#39;</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Reconstructed from </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1"> components.&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>

<span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">recons</span><span class="p">,</span>
                     <span class="n">frames</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
<span class="n">anim</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;Figures/digit3_pca.gif&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s1">&#39;imagemagick&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/digit3_pca.gif" /></p>
</div>
<div class="section" id="example-whitening">
<h3>Example: Whitening<a class="headerlink" href="#example-whitening" title="Permalink to this headline">¶</a></h3>
<p>Previously, we have seen that we can individually scale features to zero mean and unit variance to remove the scaling effects from the dataset. Now we introduce a new transformation called <strong><strong>whitening</strong></strong> or <strong><strong>sphering</strong></strong>. After whitening, additionally to unit variance, the covariance among features is set to zero. To whiten an observation we define, for each data point <span class="math notranslate nohighlight">\(x_n\)</span> , a transformed value given by</p>
<div class="math notranslate nohighlight">
\[
y_n = L^{-1/2}W^T(x_n - \bar{x})
\]</div>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is the matrix with column eigenvectors from the PCA transform, and <span class="math notranslate nohighlight">\(\Lambda\)</span> is the diagonalized covariance matrix.</p>
<p>It is easy to verify that this is just a PCA transform (<span class="math notranslate nohighlight">\(W^T \tilde{x}_n\)</span>) followed by normal standardization, since multiplying by <span class="math notranslate nohighlight">\(\Lambda^{-1/2}\)</span> amounts to divide each columns by its standard deviation. Transforming all observations at once can be done by</p>
<div class="math notranslate nohighlight">
\[
Y = X_c W\Lambda^{-1/2} = \sqrt(n) UD D^{-1} = \sqrt(n) U
\]</div>
<p>where <span class="math notranslate nohighlight">\(D=\Lambda^{1/2}\)</span> is the singular value matrix from the SVD, and <span class="math notranslate nohighlight">\(U\)</span> is the matrix of left singular vectors. It is easy to verify that the covariance matrix of the <span class="math notranslate nohighlight">\(y_{i}\)</span> is the identity matrix, <span class="math notranslate nohighlight">\(\frac{1}{n}Y^T Y = U^T U = I\)</span>.</p>
<p>To understand the difference, we’ll use the classic Old Faithful dataset. The data set comprises 272 observations, each of which represents a single eruption and contains two variables corresponding to the duration in minutes of the eruption, and the time until the next eruption, also in minutes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">old_f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;Data/old-faithful.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="c1"># Original Data</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">old_f</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">old_f</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Eruption duration&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Waiting time&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>

<span class="c1"># Standardizing</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">old_f</span><span class="p">)</span>
<span class="n">old_f_n</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">old_f</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">old_f_n</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">old_f_n</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Eruption duration&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Waiting time&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Standardized&#39;</span><span class="p">)</span>
<span class="n">of_pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">of_pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">old_f_n</span><span class="p">)</span>
<span class="n">pc1</span> <span class="o">=</span> <span class="n">of_pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">pc2</span> <span class="o">=</span> <span class="n">of_pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">l1</span><span class="p">,</span> <span class="n">l2</span> <span class="o">=</span> <span class="n">of_pca</span><span class="o">.</span><span class="n">explained_variance_</span>
<span class="c1">#ax[1].plot([],[],&#39;-r&#39;)</span>

<span class="c1"># Whitened</span>
<span class="c1"># (Alternatively the compact SVD can be used, without computing PCA)</span>
<span class="n">of_pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">old_f_w</span> <span class="o">=</span> <span class="n">of_pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">old_f</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">of_pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>
<span class="n">old_f_w</span> <span class="o">=</span> <span class="n">old_f_w</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">L</span><span class="p">))</span>
<span class="c1"># Choose sign of PC so that plots resemble more</span>
<span class="c1"># Sign is an arbitrary choice in PCA</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">-</span><span class="n">old_f_w</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="n">old_f_w</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PC1 normed&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PC2 normed&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Whitened&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/3b41afab51a4d1af85540cfabe9cec002b001650.png" /></p>
<p>Trough PCA, we obtained a whitening matrix <span class="math notranslate nohighlight">\(W\Lambda^{-1/2}\)</span>. In reality, there exist infinitely many possible whitening matrices, since once the data is spherical any rotation wouldn’t change the covariance matrix. Another useful whitening matrix is the zero-phase component analysis whitening (ZCA) <span class="math notranslate nohighlight">\(W\Lambda^{-1/2}W^T = \Sigma^{-1/2}\)</span>, that rotates back the whitened data to the original basis. This is useful, for example, in image analysis, where the ZCA whitened images resemble the orginal images more than the PCA whitened ones.</p>
</div>
<div class="section" id="example-visualization">
<h3>Example: Visualization<a class="headerlink" href="#example-visualization" title="Permalink to this headline">¶</a></h3>
<p>On application of PCA, and dimensionality reduction in general, is that of visualization. We can project the data to its first two PC, which retain most of the variance, and plot them to obtain a visual 2D representation of the data set. We now plot the first two principal components of the MNIST data set for all digits. Visualization without dimensionality reduction in data sets with many dimensions is very hard. With PCA we hope to keep most of the structure of the data set in the first 2 or 3 dimensions, which allows us to visually inspect such structure in search for, for example, clusters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">train_set</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validation_set</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">test_set</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">train_set</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">validation_set</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">test_set</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X_PCA</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">Xd</span> <span class="o">=</span> <span class="n">X_PCA</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xd</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xd</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/c359337b45149c8f47fc11a2fe5003247233aff3.png" /></p>
<p>We can also visualize the Iris Data set.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Plot orginal features</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">8</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal length&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal width&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Petal length&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">8</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal width&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Petal length&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Petal width&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/2ca2575b94fc080aac18adf2233c7377d80b0fbc.png" /></p>
<p>In the next cell,we will apply your implementation of PCA to the iris data to reduce it to 2 dimensions and visualize the result in a 2D scatter plot. The PCA projection can be thought of as a rotation that selects the view that maximizes the spread of the data, which often corresponds to the “best” view.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Normalize previous to perform PCA.</span>
<span class="c1"># This way we perform PCA on the correlation matrix instead of the covariance matrix.</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_norm</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># PCA and project the data to 2D</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_norm</span><span class="p">)</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_norm</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Z</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Z</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;rainbow&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;1st principal component&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;2nd principal component&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Iris dataset plotted in 2D, using PCA for dimensionality reduction&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/fae92d1e73f6f34b98efef07492527b8af68a785.png" /></p>
<p>We can see that in the PCA space, the variance is maximized along PC1 (explains 0.73% of the variance) and PC2 (explains 22% of the variance). Together, they explain 95%.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Explained variance per principal component: </span><span class="si">{</span><span class="n">S</span><span class="o">/</span><span class="n">S</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>Explained variance per principal component: [0.72962445 0.22850762 0.03668922 0.00517871]
</pre></div>
</div>
<p>The importance of each feature is reflected by the magnitude of the corresponding values in the eigenvectors (higher magnitude — higher importance). Let’s find the most important features:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">U</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>[[0.52106591 0.37741762 0.71956635 0.26128628]
 [0.26934744 0.92329566 0.24438178 0.12350962]
 [0.5804131  0.02449161 0.14212637 0.80144925]
 [0.56485654 0.06694199 0.63427274 0.52359713]]
</pre></div>
</div>
<p>Remember the columns of U are the eigenvectors along the directions that maximize the variance. So, looking at the first principal component:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">U</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>[0.52106591 0.26934744 0.5804131  0.56485654]
[0.37741762 0.92329566 0.02449161 0.06694199]
</pre></div>
</div>
<p>we can conclude that feature 1, 3 and 4 are the most important for PC1. Similarly, we can state that feature 2 and then 1 are the most important for PC2.</p>
<div class="section" id="the-biplot">
<h4>The Biplot<a class="headerlink" href="#the-biplot" title="Permalink to this headline">¶</a></h4>
<p>From <span id="id4">[<a class="reference internal" href="#id57"><span>2</span></a>]</span>:</p>
<blockquote>
<div><p>One of the most informative graphical representations of a multivariate dataset is via a biplot, which is fundamentally connected to the SVD of a relevant data matrix, and therefore to PCA.</p>
</div></blockquote>
<p>Remember that an alternative way to approach PCA is through the decomposition:</p>
<div class="math notranslate nohighlight">
\[
X_c = U D W^T,
\]</div>
<p>where the matrix <span class="math notranslate nohighlight">\(W\)</span> contains the eigenvectors along the principal components as columns.</p>
<p>Under this decomposition, the matrix <span class="math notranslate nohighlight">\(U\)</span> and the matrix product <span class="math notranslate nohighlight">\(V D^T\)</span> contain information about the statistical correlation among features and observations. In a biplot, the first two columns of each <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(W D^T\)</span> are used to depict an approximated visual representation of such correlations. In particular, the rows of <span class="math notranslate nohighlight">\(U\)</span> are plotted as points representing observations, while the rows of <span class="math notranslate nohighlight">\(W D^T\)</span> are plotted as vectors representing features.</p>
<p>If the full set of eigenvectors is retained, the following points are true, in the biplot case, they remain approximately true, specially if the variance explained by the two first PC is large.</p>
<p>Points taken from <span id="id5">[<a class="reference internal" href="#id57"><span>2</span></a>]</span>:</p>
<ul class="simple">
<li><p>The cosine of the angle between any two vectors representing features is the coefficient of correlation between those features. This comes from the fact that the product <span class="math notranslate nohighlight">\(W D^T D W^T = n\Sigma\)</span> is proportional to the covariance matrix.</p></li>
<li><p>Similarly, the cosine of the angle between any vector representing a variable and the axis representing a given PC is the coefficient of correlation between those two variables.</p></li>
<li><p>The inner product between the markers for observation <span class="math notranslate nohighlight">\(i\)</span> and feature <span class="math notranslate nohighlight">\(j\)</span> gives the (centred) value of observation <span class="math notranslate nohighlight">\(i\)</span> on feature <span class="math notranslate nohighlight">\(j\)</span>. This is a direct result of the fact that <span class="math notranslate nohighlight">\(X_c = U D V^T\)</span>. The practical implication of this result is that orthogonally projecting the point representing observation <span class="math notranslate nohighlight">\(i\)</span> onto the vector representing feature <span class="math notranslate nohighlight">\(j\)</span> recovers the (centred) value <span class="math notranslate nohighlight">\(x_{ij} - \bar{x}_j\)</span>.</p></li>
<li><p>The Euclidean distance between the markers for observations <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> is proportional to the Mahalanobis distance between them, since <span class="math notranslate nohighlight">\(U\)</span> is proportional to the whitened data set.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">biplot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">VT</span><span class="o">.</span><span class="n">T</span>

    <span class="n">xs</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">H</span> <span class="o">=</span> <span class="n">V</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">scalex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">H</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">scaley</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">U</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">H</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">scalex</span><span class="p">,</span> <span class="n">scaley</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">H</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="s2">&quot;Var&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">ha</span> <span class="o">=</span> <span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span> <span class="o">=</span> <span class="s1">&#39;center&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">ha</span> <span class="o">=</span> <span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span> <span class="o">=</span> <span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">biplot</span><span class="p">(</span><span class="n">X_norm</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/a62aa47280732be68818c79fbd8ce97d66f9da18.png" /></p>
</div>
</div>
<div class="section" id="limitations-of-standard-pca">
<h3>Limitations of standard PCA<a class="headerlink" href="#limitations-of-standard-pca" title="Permalink to this headline">¶</a></h3>
<p>Examples adapted from <span id="id6">[<a class="reference internal" href="#id58"><span>4</span></a>]</span>:</p>
<p>Suppose we have a data set that tracks positions along a circumference, with some noise added. The variable of interest is the angle, but PCA, being linear, cannot recover that for us, though it will provide PC.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate ferris wheel artifical data</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]);</span>
</pre></div>
</div>
<p><img alt="" src="_images/ba1a6ede5154125ac8e642362f8db4ecf951164b.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply PCA</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">mean_</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">scale_</span>
<span class="n">X_norm</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#  Run PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_norm</span><span class="p">)</span>
<span class="n">pc1</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">pc2</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">pc1</span><span class="p">,</span><span class="n">pc2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;bo&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">mec</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">],</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">U</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span>
             <span class="n">head_width</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/27456ea16a4ac98296e869e954be7ec77da5680d.png" /></p>
<p>Some times, the directions of interest are not orthogonal. In this cases, ICA is a better choice (more on ICA later).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate artificial data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">x1</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span>  <span class="o">-</span><span class="mf">.3</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">x2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span>  <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x2</span><span class="p">,</span><span class="n">y2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span><span class="n">X2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/808a0698db3e5c723a98f9adc6452458485334ea.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply PCA</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">mean_</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">scale_</span>
<span class="n">X_norm</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#  Run PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_norm</span><span class="p">)</span>
<span class="n">pc1</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">pc2</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">pc1</span><span class="p">,</span><span class="n">pc2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;bo&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">U</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span>
             <span class="n">head_width</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>


<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/4b900f80f116762955c6185bed0a9023d64ff716.png" /></p>
<p>From <span id="id7">[<a class="reference internal" href="#id58"><span>4</span></a>]</span>:</p>
<blockquote>
<div><p>The solution to this paradox lies in the goal we selected for the analysis. The goal of the analysis is to decorrelate the data, or said in other terms, the goal is to remove second-order dependencies in the data. In the data sets of the example, higher order dependencies exist between the variables. Therefore, removing second-order dependencies is insufficient at revealing all structure in the data.</p>
<p>When are second order dependencies sufficient for revealing all dependencies in a data set? This statistical condition is met when the first and second order statistics are sufficient statistics of the data. This occurs, for instance, when a data set is Gaussian distributed.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="id8"><dl class="citation">
<dt class="label" id="id55"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Brett Bernstein. Lecture 13: principal component analysis. URL: <a class="reference external" href="https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/13-PCA-Notes_sol.pdf">https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/13-PCA-Notes_sol.pdf</a> (visited on 2021-04-21).</p>
</dd>
<dt class="label" id="id57"><span class="brackets">2</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>,<a href="#id5">3</a>)</span></dt>
<dd><p>Ian T Jolliffe and Jorge Cadima. Principal component analysis: a review and recent developments. <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em>, 374(2065):20150202, 2016.</p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. <em>The elements of statistical learning: data mining, inference, and prediction</em>. Springer Science &amp; Business Media, 2009.</p>
</dd>
<dt class="label" id="id58"><span class="brackets">4</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Jonathon Shlens. A tutorial on principal component analysis. <em>arXiv preprint arXiv:1404.1100</em>, 2014.</p>
</dd>
</dl>
</p>
<div class="toctree-wrapper compound">
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="assignments-dummy/U2-M1-L1-similarity_metrics.html" title="previous page"><strong>Warning</strong>:</a>
    <a class='right-next' id="next-link" href="assignments-dummy/U2-M2-L1-dim_red_PCA.html" title="next page">Principal Component Analysis</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Gonzalo G. Peraza Mues<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>