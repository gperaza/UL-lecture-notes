Data Preprocessing: Normalization
=================================

In this notebook we'll explore data preprocessing techniques through the
California housing dataset available from sklearn. We'll reproduce
results found on the sklearn documentation, though we will be using our
own functions to understand how each method works. For production its a
good idea to use the sklearn methods as they provide a convenient API
for ML pipelines. Links to skleran relevant docs can be found at the end
of the notebook.

We begin with our standard imports:

.. code:: python

   import numpy as np
   import matplotlib as mpl
   import matplotlib.pyplot as plt
   from matplotlib import cm

   # Load the example dataset
   from sklearn.datasets import fetch_california_housing

We define our dataset and create some auxiliary functions for plotting.
We will only use two features, median income and number of house holds.
The plotting functions are available from the
`file:utils.py <utils.py>`__ file.

.. code:: python

   dataset = fetch_california_housing()
   X_full = dataset.data

   # Take only 2 features to make visualization easier
   # Feature of 0 has a long tail distribution.
   # Feature 5 has a few but very large outliers.

   X = X_full[:, [0, 5]]

.. code:: python

   from utils import make_plot

   # Original Data
   make_plot('Unscaled data', X)

Our base plot shows data in its original scale. We will review the
effect of different scalings by comparing them to the plot above.

Introduction
------------

Data normalization is a standard preprocessing step that helps equalize
the relative importance of features before applying a learning algorithm
(enwiki:1001781300?). By applying normalization the effect of each
feature particular scale is removed, all quantities become
dimensionless.

For example, many classifiers calculate the distance between two points
by the Euclidean distance. If one of the features has a broad range of
values, the distance will be governed by this particular feature.
Therefore, the range of all features should be normalized so that each
feature contributes approximately proportionately to the final distance.

From (sarle_compaineural-rescale?). If one input has a range of 0 to 1,
while another input has a range of 0 to 1,000,000, then the contribution
of the first input to the distance will be swamped by the second input.
So it is essential to rescale the inputs so that their variability
reflects their importance, or at least is not in inverse relation to
their importance. For lack of better prior information, it is common to
standardize each input to the same range or the same standard deviation.
If you know that some inputs are more important than others, it may help
to scale the inputs such that the more important ones have larger
variances and/or ranges.

Standardizing input variables can have far more important effects on
initialization of the weights than simply avoiding saturation. Assume we
have an MLP with one hidden layer applied to a classification problem
and are therefore interested in the hyperplanes defined by each hidden
unit. Each hyperplane is the locus of points where the net-input to the
hidden unit is zero and is thus the classification boundary generated by
that hidden unit considered in isolation. The connection weights from
the inputs to a hidden unit determine the orientation of the hyperplane.
The bias determines the distance of the hyperplane from the origin. If
the bias terms are all small random numbers, then all the hyperplanes
will pass close to the origin. Hence, if the data are not centered at
the origin, the hyperplane may fail to pass through the data cloud. If
all the inputs have a small coefficient of variation, it is quite
possible that all the initial hyperplanes will miss the data entirely.
With such a poor initialization, local minima are very likely to occur.
It is therefore important to center the inputs to get good random
initializations. In particular, scaling the inputs to [-1,1] will work
better than [0,1], although any scaling that sets to zero the mean or
median or other measure of central tendency is likely to be as good, and
robust estimators of location and scale (Iglewicz, 1983) will be even
better for input variables with extreme outliers.

Thus it is easy to see that you will get better initializations if the
data are centered near zero and if most of the data are distributed over
an interval of roughly [-1,1] or [-2,2]. If you are firmly opposed to
the idea of standardizing the input variables, you can compensate by
transforming the initial weights, but this is much more complicated than
standardizing the input variables.

Another reason why feature scaling is applied is that gradient descent
converges much faster with feature scaling than without it.

Various other pairs of location and scale estimators can be used besides
the mean and standard deviation, or midrange and range. Robust estimates
of location and scale are desirable if the inputs contain outliers.

Min-max normalization
---------------------

To re-scale a range between an arbitrary set of values :math:`[a, b]`

.. math::


   x' = a + \frac{\left(x - x_{min}\right)\left(b - a\right)}{x_{max} - x_{min}}

The general formula for a min-max of :math:`[0, 1]` is given as:

.. math::


   x' = \frac{x - x_{min}}{x_{max} - x_{min}}

.. code:: python

   def norm_min_max(X, a=0, b=1):
       """Applies min-max normalization to a data matrix where each feature (column) is rescaled to the interval [a,b]"""

       ### BEGIN SOLUTION
       x_min = np.min(X, axis=0)
       x_max = np.max(X, axis=0)

       X_norm = a + (b - a)*(X - x_min) / (x_max - x_min)
       ### END SOLUTION

       return X_norm

.. code:: python

   make_plot('Min-Max scaling [0,1]', norm_min_max(X))
   ### BEGIN HIDDEN TESTS
   X_test = np.array([[1,2,3],[4,5,6],[7,8,9]])
   assert (norm_min_max(X_test) == np.array([[0. , 0. , 0. ], [0.5, 0.5, 0.5], [1. , 1. , 1. ]])).all()
   ### END HIDDEN TESTS

Mean normalization
------------------

Center the data, such that the scaled mean is at zero. The scaled range
is 1.

.. math::


   x' = \frac{x - \bar{x}}{x_{max} - x_{min}}

.. code:: python

   def norm_mean(X):
       """Applies mean normalization to a data matrix"""

       ### BEGIN SOLUTION
       x_min = np.min(X, axis=0)
       x_max = np.max(X, axis=0)
       x_mean = np.mean(X, axis=0)

       X_norm = (X - x_mean) / (x_max - x_min)
       ### END SOLUTION

       return X_norm

.. code:: python

   make_plot('Mean-norm scaling', norm_mean(X))
   ### BEGIN HIDDEN TESTS
   X_test = np.array([[1,2,3],[4,5,6],[7,8,9]])
   assert (norm_mean(X_test) ==
           np.array([[-0.5, -0.5, -0.5],
                     [ 0. ,  0. ,  0. ],
                     [ 0.5,  0.5,  0.5]])).all()
   ### END HIDDEN TESTS

Standardization
---------------

Feature standardization makes the values of each feature in the data
have zero-mean (when subtracting the mean in the numerator) and
unit-variance. The general method of calculation is to determine the
distribution mean and standard deviation for each feature. Next we
subtract the mean from each feature. Then we divide the values (mean is
already subtracted) of each feature by its standard deviation.

.. math::


   x' = \frac{x - \bar{x}}{\sigma_x}

.. code:: python

   def norm_standardize(X):
       """Applies standardization to a data matrix so each feature has mean 0 and sd 1."""

       ### BEGIN SOLUTION
       x_mean = np.mean(X, axis=0)
       x_std = np.std(X, axis=0)

       X_norm = (X - x_mean) / x_std
       ### END SOLUTION

       return X_norm

.. code:: python

   make_plot('Standardization', norm_standardize(X))
   ### BEGIN HIDDEN TESTS
   X_test = np.array([[1,2,3],[4,5,6],[7,8,9]])
   ans = 1.2247448713915892
   assert (norm_standardize(X_test) == np.array([[-ans, -ans, -ans],
                                                 [  0.,   0.,   0.],
                                                 [ ans,  ans,  ans]])).all()
   ### END HIDDEN TESTS

Scaling rows to unit length
---------------------------

Another option that is widely used in machine-learning is to scale the
components of a feature vector such that the complete vector has length
one. This usually means dividing each component by the Euclidean length
of the vector:

.. math::


   x' = \frac{x}{\left\| x \right\|}

In some applications it can be more practical to use the L1 norm of the
feature vector. Note that this transformation acts on ROWS, not columns
of the data matrix. It is useful when using similarity measures based on
the vector dot product, such as when working with Natural Language
Processing models.

.. code:: python

   def norm_unit_length(X, norm=2):
       """Scales each feature vector of a data matrix to unit length."""

       ### BEGIN SOLUTION
       x_norms = np.linalg.norm(X, axis=1, ord=norm)
       X_norm = X/x_norms[None,:].T
       ### END SOLUTION

       return X_norm

.. code:: python

   make_plot('Unit length row scaling', norm_unit_length(X))
   ### BEGIN HIDDEN TESTS
   X_test = np.array([[1,2,3],[4,5,6],[7,8,9]])
   for i in range(X_test.shape[0]):
       assert 0.999999 <= np.linalg.norm(norm_unit_length(X_test)[i,:]) <= 1
   ### END HIDDEN TESTS

Max-abs normalization
---------------------

Scales features so that the maximum absolute value of each feature is
scaled to unit size. The motivation to use this scaling include
robustness to very small standard deviations of features and preserving
zero entries in sparse data. It does not shift/center the data, and thus
does not destroy any sparsity.

.. math::


   x' = \frac{x}{max(|x|)}

.. code:: python

   def norm_max_abs(X):
       """Applies max-abs normalization to a data matrix where each feature (column) is rescaled to within [-1,1]. Preserves sparsity."""

       ### BEGIN SOLUTION
       x_max = np.max(np.abs(X), axis=0)
       X_norm = X/x_max
       ### END SOLUTION

       return X_norm

.. code:: python

   make_plot('Max-Abs scaling', norm_max_abs(X))
   ### BEGIN HIDDEN TESTS
   X_test = np.array([[1,2,3],[4,5,6],[7,8,9]])
   X_test_normed = np.array([[0.14285714, 0.25      , 0.33333333],
                             [0.57142857, 0.625     , 0.66666667],
                             [1.        , 1.        , 1.        ]])
   assert (norm_max_abs(X_test) <= X_test_normed + 0.00001).all()
   assert (norm_max_abs(X_test) >= X_test_normed - 0.00001).all()
   ### END HIDDEN TESTS

Scaling data with outliers
--------------------------

If your data contains many outliers, scaling using the mean and variance
of the data is likely to not work very well. A more robust scaling
removes the median and scales the data according to the quantile range.

.. math::


   x' = \frac{x - q_2}{q_3 - q_1}

.. code:: python

   def norm_robust(X):
       """Applies normalization of a data matrix based on the median and the IQR"""

       ### BEGIN SOLUTION
       x_q1 = np.percentile(X, 25, axis=0)
       x_q3 = np.percentile(X, 75, axis=0)
       x_median = np.median(X, axis=0)

       X_norm = (X - x_median) / (x_q3 - x_q1)
       ### END SOLUTION

       return X_norm

.. code:: python

   make_plot('Robust scaling', norm_robust(X))
   ### BEGIN HIDDEN TESTS
   X_test = np.array([[1,2,3],[4,5,6],[7,8,9]])
   X_test_normed = np.array([[-1., -1., -1.],
                             [ 0.,  0.,  0.],
                             [ 1.,  1.,  1.]])
   assert (norm_robust(X_test) == X_test_normed).all()
   ### END HIDDEN TESTS

References
----------

`bibliography:references.bib <bibliography:references.bib>`__
`bibliographystyle:unsrt <bibliographystyle:unsrt>`__

.. _include-examples-using-sklearn-methods-for-comparison.:

TODO Include examples using sklearn methods for comparison.
-----------------------------------------------------------
