{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "**IMPORTANT: DO NOT COPY OR SPLIT CELLS.** If you do, you'll mess the autograder. If need more cells to work or test things out, create a new cell. You may add as many new cells as you need.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and group below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COURSE = \"Unsupervised Learning 2021\"\n",
    "GROUP = \"D8A\"\n",
    "NAME = \"\" # Match your GitHub Classroom ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this exercise, you will use principal component analysis to find a low-dimensional representation of face images. This exercise has been adapted from the course Machine Learning from Andrew Ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "In this exercise, you will use principal component analysis (PCA) to perform dimensionality reduction. You will first experiment with an example 2D dataset to get intuition on how PCA works, and then use it on a bigger dataset of 5000 face image dataset.\n",
    "\n",
    "### Example Dataset\n",
    "\n",
    "To help you understand how PCA works, you will first start with a 2D dataset which has one direction of large variation and one of smaller variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into the variable X \n",
    "X = np.loadtxt('Data/data1.csv', delimiter=',')\n",
    "\n",
    "#  Visualize the example dataset\n",
    "plt.plot(X[:, 0], X[:, 1], 'bo', ms=10, mec='k', mew=1)\n",
    "plt.axis([0.5, 6.5, 2, 8])\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing PCA ( 2pts)\n",
    "\n",
    "In this part of the exercise, you will implement PCA. PCA consists of two computational steps: \n",
    "\n",
    "1. Compute the covariance matrix of the data.\n",
    "2. Use SVD (in python we use numpy's implementation `np.linalg.svd`) to compute the eigenvectors $U_1$, $U_2$, $\\dots$, $U_n$. These will correspond to the principal components of variation in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15cacbfa7e5375b47a5e33b10dc78ec3",
     "grade": false,
     "grade_id": "ex-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "    \"\"\"\n",
    "    Run principal component analysis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset to be used for computing PCA.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    U : array_like\n",
    "        The eigenvectors, representing the computed principal components\n",
    "        of X. U has dimensions (n x n) where each column is a single \n",
    "        principal component.\n",
    "    \n",
    "    S : array_like\n",
    "        A vector of size n, contaning the singular values for each\n",
    "        principal component.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return U, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the function `pca`, the following cell will run PCA on the normalized example dataset and plot the corresponding principal components found similar to the figure below. \n",
    "\n",
    "![](Figures/pca_components.png)\n",
    "\n",
    "The following cell will also output the top principal component (eigenvector) found, and you should expect to see an output of about `[-0.707 -0.707]`. (It is possible that `numpy` may instead output the negative of this, since $U_1$ and $-U_1$ are equally valid choices for the first principal component.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d734778f77f5c9cc3a24d243535b14e5",
     "grade": true,
     "grade_id": "ex-1-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  Before running PCA, it is important to first normalize X\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "mu = scaler.mean_\n",
    "sigma = scaler.scale_\n",
    "X_norm = scaler.transform(X)\n",
    "\n",
    "#  Run PCA\n",
    "U, S = pca(X_norm)\n",
    "\n",
    "#  Draw the eigenvectors centered at mean of data. These lines show the\n",
    "#  directions of maximum variations in the dataset.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[:, 0], X[:, 1], 'bo', ms=10, mec='k', mew=0.25)\n",
    "\n",
    "for i in range(2):\n",
    "    ax.arrow(mu[0], mu[1], 1.5 * S[i]*U[0, i], 1.5 * S[i]*U[1, i],\n",
    "             head_width=0.25, head_length=0.2, fc='k', ec='k', lw=2, zorder=1000)\n",
    "\n",
    "ax.axis([0.5, 6.5, 2, 8])\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "print('Top eigenvector: U[:, 0] = [{:.6f} {:.6f}]'.format(U[0, 0], U[1, 0]))\n",
    "print(' (you should expect to see [-0.707107 -0.707107])')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction with PCA (2 pts)\n",
    "\n",
    "After computing the principal components, you can use them to reduce the feature dimension of your dataset by projecting each example onto a lower dimensional space, $x^{(i)} \\rightarrow z^{(i)}$ (e.g., projecting the data from 2D to 1D). In this part of the exercise, you will use the eigenvectors returned by PCA and\n",
    "project the example dataset into a 1-dimensional space. In practice, if you were using a learning algorithm such as linear regression or perhaps neural networks, you could now use the projected data instead of the original data. By using the projected data, you can train your model faster as there are less dimensions in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75b0287e05a799b1470f2eafb90c497c",
     "grade": false,
     "grade_id": "ex-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def projectData(X, U, K):\n",
    "    \"\"\"\n",
    "    Computes the reduced data representation when projecting only \n",
    "    on to the top K eigenvectors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n). The dataset is assumed to be \n",
    "        normalized.\n",
    "    \n",
    "    U : array_like\n",
    "        The computed eigenvectors using PCA. This is a matrix of \n",
    "        shape (n x n). Each column in the matrix represents a single\n",
    "        eigenvector (or a single principal component).\n",
    "    \n",
    "    K : int\n",
    "        Number of dimensions to project onto. Must be smaller than n.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Z : array_like\n",
    "        The projects of the dataset onto the top K eigenvectors. \n",
    "        This will be a matrix of shape (m x k).\n",
    "\n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly.\n",
    "    Z = np.zeros((X.shape[0], K))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will project the first example onto the first dimension and you should see a value of about 1.481 (or possibly -1.481, if you got $-U_1$ instead of $U_1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4063f2e1c37575eb085a9a1eb0129b54",
     "grade": true,
     "grade_id": "ex-2-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  Project the data onto K = 1 dimension\n",
    "K = 1\n",
    "Z = projectData(X_norm, U, K)\n",
    "print('Projection of the first example: {:.6f}'.format(Z[0, 0]))\n",
    "print('(this value should be about    : 1.496313)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructing an approximation of the data (2 pts)\n",
    "\n",
    "After projecting the data onto the lower dimensional space, you can approximately recover the data by projecting them back onto the original high dimensional space. Your task is to complete the function `recoverData` to project each example in `Z` back onto the original space and return the recovered approximation in `Xrec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bf0348f294ded78a4ff39617d019a50",
     "grade": false,
     "grade_id": "ex-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def recoverData(Z, U, K):\n",
    "    \"\"\"\n",
    "    Recovers an approximation of the original data when using the \n",
    "    projected data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Z : array_like\n",
    "        The reduced data after applying PCA. This is a matrix\n",
    "        of shape (m x K).\n",
    "    \n",
    "    U : array_like\n",
    "        The eigenvectors (principal components) computed by PCA.\n",
    "        This is a matrix of shape (n x n) where each column represents\n",
    "        a single eigenvector.\n",
    "    \n",
    "    K : int\n",
    "        The number of principal components retained\n",
    "        (should be less than n).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_rec : array_like\n",
    "        The recovered data after transformation back to the original \n",
    "        dataset space. This is a matrix of shape (m x n), where m is \n",
    "        the number of examples and n is the dimensions (number of\n",
    "        features) of original datatset.\n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly.\n",
    "    X_rec = np.zeros((Z.shape[0], U.shape[0]))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return X_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the code in `recoverData`, the following cell will recover an approximation of the first example and you should see a value of about `[-1.058 -1.058]`. The code will then plot the data in this reduced dimension space. This will show you what the data looks like when using only the corresponding eigenvectors to reconstruct it. An example of what you should get for PCA projection is shown in this figure: \n",
    "\n",
    "![](Figures/pca_reconstruction.png)\n",
    "\n",
    "In the figure above, the original data points are indicated with the blue circles, while the projected data points are indicated with the red circles. The projection effectively only retains the information in the direction given by $U_1$. The dotted lines show the distance from the data points in original space to the projected space. Those dotted lines represent the error measure due to PCA projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cd2756186c6e7ac54614b8bba60332a",
     "grade": true,
     "grade_id": "ex-3-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_rec  = recoverData(Z, U, K)\n",
    "print('Approximation of the first example: [{:.6f} {:.6f}]'.format(X_rec[0, 0], X_rec[0, 1]))\n",
    "print('       (this value should be about  [-1.058053 -1.058053])')\n",
    "\n",
    "#  Plot the normalized dataset (returned from featureNormalize)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(X_norm[:, 0], X_norm[:, 1], 'bo', ms=8, mec='b', mew=0.5)\n",
    "ax.set_aspect('equal')\n",
    "plt.axis([-3, 2.75, -3, 2.75])\n",
    "\n",
    "# Draw lines connecting the projected points to the original points\n",
    "ax.plot(X_rec[:, 0], X_rec[:, 1], 'ro', mec='r', mew=2, mfc='none')\n",
    "for xnorm, xrec in zip(X_norm, X_rec):\n",
    "    ax.plot([xnorm[0], xrec[0]], [xnorm[1], xrec[1]], '--k', lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Image Dataset\n",
    "\n",
    "In this part of the exercise, you will run PCA on face images to see how it can be used in practice for dimension reduction. The dataset `ex7faces.mat` contains a dataset `X` of face images, each $32 \\times 32$ in grayscale. This dataset was based on a [cropped version](http://conradsanderson.id.au/lfwcrop/) of the [labeled faces in the wild](http://vis-www.cs.umass.edu/lfw/) dataset. Each row of `X` corresponds to one face image (a row vector of length 1024). \n",
    "\n",
    "In order to visualize the faces, we define a helper function in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayData(X, example_width=None, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Displays 2D data in a nice grid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input data of size (m x n) where m is the number of examples and n is the number of\n",
    "        features.\n",
    "\n",
    "    example_width : int, optional\n",
    "        THe width of each 2-D image in pixels. If not provided, the image is assumed to be square,\n",
    "        and the width is the floor of the square root of total number of pixels.\n",
    "\n",
    "    figsize : tuple, optional\n",
    "        A 2-element tuple indicating the width and height of figure in inches.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    example_width = example_width or int(np.round(np.sqrt(n)))\n",
    "    example_height = int(n / example_width)\n",
    "\n",
    "    # Compute number of items to display\n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m / display_rows))\n",
    "\n",
    "    fig, ax_array = plt.subplots(display_rows, display_cols, figsize=figsize)\n",
    "    fig.subplots_adjust(wspace=0.025, hspace=0.025)\n",
    "\n",
    "    ax_array = ax_array.ravel()\n",
    "    \n",
    "    for i, ax in enumerate(ax_array):\n",
    "        ax.imshow(X[i].reshape(example_height, example_width, order='F'), cmap='gray')\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will load and visualize the first 100 of these face images similar to what is shown in this figure:\n",
    "\n",
    "![Faces](Figures/faces.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load Face dataset\n",
    "X = np.loadtxt('Data/faces.gz', delimiter=',')\n",
    "\n",
    "#  Display the first 100 faces in the dataset\n",
    "displayData(X[:100, :], figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on Faces\n",
    "\n",
    "To run PCA on the face dataset, we first normalize the dataset by subtracting the mean of each feature from the data matrix `X`.  After running PCA, you will obtain the principal components of the dataset. Notice that each principal component in `U` (each column) is a vector of length $n$ (where for the face dataset, $n = 1024$). It turns out that we can visualize these principal components by reshaping each of them into a $32 \\times 32$ matrix that corresponds to the pixels in the original dataset. \n",
    "\n",
    "The following cell will first normalize the dataset for you and then run your PCA code. Then, the first 36 principal components (conveniently called eigenfaces) that describe the largest variations are displayed. If you want, you can also change the code to display more principal components to see how they capture more and more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  normalize X by subtracting the mean value from each feature\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "mu = scaler.mean_\n",
    "sigma = scaler.scale_\n",
    "X_norm = scaler.transform(X)\n",
    "\n",
    "#  Run PCA\n",
    "U, S = pca(X_norm)\n",
    "\n",
    "#  Visualize the top 36 eigenvectors found\n",
    "displayData(U[:, :36].T, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction\n",
    "\n",
    "Now that you have computed the principal components for the face dataset, you can use it to reduce the dimension of the face dataset. This allows you to use your learning algorithm with a smaller input size (e.g., 100 dimensions) instead of the original 1024 dimensions. This can help speed up your learning algorithm.\n",
    "\n",
    "The next cell will project the face dataset onto only the first 100 principal components. Concretely, each face image is now described by a vector $z^{(i)} \\in \\mathbb{R}^{100}$. To understand what is lost in the dimension reduction, you can recover the data using only the projected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Project images to the eigen space using the top k eigenvectors \n",
    "#  If you are applying a machine learning algorithm \n",
    "K = 100\n",
    "Z = projectData(X_norm, U, K)\n",
    "\n",
    "print('The projected data Z has a shape of: ', Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, an approximate recovery of the data is performed and the original and projected face images\n",
    "are displayed similar to what is shown here:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Figures/faces_original.png\" width=\"300\"></td>\n",
    "        <td><img src=\"Figures/faces_reconstructed.png\" width=\"300\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "From the reconstruction, you can observe that the general structure and appearance of the face are kept while the fine details are lost. This is a remarkable reduction (more than 10x) in the dataset size that can help speed up your learning algorithm significantly. For example, if you were training a neural network to perform person recognition (given a face image, predict the identity of the person), you can use the dimension reduced input of only a 100 dimensions instead of the original pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Project images to the eigen space using the top K eigen vectors and \n",
    "#  visualize only using those K dimensions\n",
    "#  Compare to the original input, which is also displayed\n",
    "K = 100\n",
    "X_rec  = recoverData(Z, U, K)\n",
    "\n",
    "# Display normalized data\n",
    "displayData(X_norm[:100, :], figsize=(6, 6))\n",
    "plt.gcf().suptitle('Original faces')\n",
    "\n",
    "# Display reconstructed data from only k eigenfaces\n",
    "displayData(X_rec[:100, :], figsize=(6, 6))\n",
    "plt.gcf().suptitle('Recovered faces');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Exercises and Proofs\n",
    "\n",
    "To further your understanding of PCA, solve the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (2 pts)\n",
    "\n",
    "In class, we discussed that PCA solves the optimization problem\n",
    "$$\n",
    "\\underset{\\mu,\\{\\lambda_i\\},W_{:q}}{\\min}\n",
    "\\sum_{i=1}^n \\left | x_i - \\mu - W_{:q} y_i \\right |^2\n",
    "$$\n",
    "\n",
    "Prove that \n",
    "\n",
    "\\begin{align}\n",
    "\\mu^{*} =& \\bar{x}\\\\\n",
    "y_i^{*} =& W_{:q}^T(x_i - \\bar{x}).\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed0992b4ced52e660a936b3a4b286cb6",
     "grade": true,
     "grade_id": "prob-1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (2 pts)\n",
    "\n",
    "With the previous optimal values $\\mu^*$ and $y_i^*$, we still need to find\n",
    "\n",
    "$$\n",
    "\\underset{W_{q:}}{\\min} \\sum_i^n\n",
    "\\left |\n",
    "\\tilde{x}_i - W_{:q}W_{:q}^T\\tilde{x}_i\n",
    "\\right |^2\n",
    "$$\n",
    "\n",
    "where $\\tilde{x}_i = x_i - \\bar{x}$. The $d\\times d$ matrix $W W^T$ is a projection matrix. It first projects each point $\\tilde{x}_i$ into the hyper-plane by obtaining each component along each column of $W$.\n",
    "This a q-dimensional representation in the sub-space. Next, we move back into the original d-dimensional space by multiplying by $W$, effectively taking a linear combination of the unit vectors, with each projected component as the weights.\n",
    "\n",
    "Show the minimizing the reconstruction error is equivalent as maximizing the variance along the first q directions of $W_{:q}$. Use matrix algebra to transform into an equivalent expression discussed in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d91a265e23ce6bea5faa9c5356f1fb19",
     "grade": true,
     "grade_id": "prob-2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
