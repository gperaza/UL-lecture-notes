
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Freedman-Diaconis Rule &#8212; Lecture Notes in Unsupervised and Reinforcement Learning</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Gaussian Mixture Models" href="U4-M2-L1-Gaussian_Mixture_Models.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/UPY-completo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Lecture Notes in Unsupervised and Reinforcement Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome!
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 1 - Unsupervised Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L1-prep-normalization.html">
   Data Normalization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L1-prep-normalization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L2-prep-discretization.html">
   Discretization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L2-prep-discretization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L3-unsup-outlier-detection.html">
   Anomaly and Outlier Detection
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L3-prep-outlier-detection.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 2 - Dimensionality Reduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U2-M1-L1-similarity_metrics.html">
   (Dis)similarity metrics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html">
     <strong>
      Warning
     </strong>
     :
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-1-2-pt">
     Exercise 1 (2 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-2-3-pt">
     Exercise 2 (3 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-3-4-pts">
     Exercise 3 (4 pts)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-4-1-pt">
     Exercise 4 (1 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-5-ungraded">
     Exercise 5 (ungraded)
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U2-M2-L1-dim_red_PCA.html">
   Dimensionality Reduction and PCA
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M2-L1-dim_red_PCA.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M2-L2-PCA_variants.html">
   Some linear and non-linear variants of PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M2-L3-Other_Linear_Methods_for_DR.html">
   Other Dimensionality Reduction Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M2-L4-Non_Linear_Methods_for_DR.html">
   Non-linear Dimensionality Reduction
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 3 - Clustering
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L1-Prototype_clustering.html">
   Prototype Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L2-Hierarchical_clustering.html">
   Hierarchical Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L3-Density_based_clustering.html">
   Density Based Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L4-Other_clustering_methods.html">
   Other Clustering Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M2-L1-Clustering_evaluation.html">
   Clustering Evaluation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 4 - Probabilistic Methods
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="U4-M1-L1-Latent_variable_models.html">
   Latent Variable Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U4-M1-L2-Independent_Component_Analysis.html">
   Independent Component Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U4-M2-L1-Gaussian_Mixture_Models.html">
   Gaussian Mixture Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Freedman-Diaconis Rule
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/freedman-diaconis.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/gperaza/UL-lecture-notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/gperaza/UL-lecture-notes/issues/new?title=Issue%20on%20page%20%2Ffreedman-diaconis.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="freedman-diaconis-rule">
<h1>Freedman-Diaconis Rule<a class="headerlink" href="#freedman-diaconis-rule" title="Permalink to this headline">¶</a></h1>
<p>There is a rule of thumb for determining the optimal number of bins in a histogram. The Freedman-Diaconis rule states that the optimal bin width can be estimated as</p>
<div class="math notranslate nohighlight">
\[
h = \frac{2\ IQR}{n^{1/3}}
\]</div>
<p>The asymptotic (large <span class="math notranslate nohighlight">\(n\)</span>) optimal bin width was derived by Scott <span id="id1">[<a class="reference internal" href="#id9"><span>1</span></a>,<span></span>]</span> yet, its value depends on the derivative of the theoretical distribution, often not known. Freedman and Diaconis claimed &#64;freedman1981histogram the bin width can be robustly estimated by the formula above, which works well most of the time, under the requirements that the true distribution has squared integrable and continuous first and second derivatives.</p>
<p>It is worth it to provide a rough derivation of the FD rule, as it is an nice exercise in the art of approximation.</p>
<p>To find the optimal bin width, we minimize the Mean Integrated Squared Error (MISE),</p>
<div class="math notranslate nohighlight">
\[
MISE = E\left[ \int \left\{ H(x) - f(x)  \right\}^{2}\right],
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(x)\)</span> is the underlying probability density function and <span class="math notranslate nohighlight">\(H(x)\)</span> is the histogram function, which gives the height bin <span class="math notranslate nohighlight">\(j\)</span> containing <span class="math notranslate nohighlight">\(x\)</span>,</p>
<div class="math notranslate nohighlight">
\[
H(x) = \frac{N_{j}(x)}{N h}
\]</div>
<p>The MISE gives the expected area between the estimated density (histogram) and the true density (<span class="math notranslate nohighlight">\(f(x)\)</span>).</p>
<p>Within each bin, the count number <span class="math notranslate nohighlight">\(N_j\)</span> is a binomial random variable with parameters <span class="math notranslate nohighlight">\(p=hf_j\)</span> and <span class="math notranslate nohighlight">\(n=N\)</span> (# of trials), where <span class="math notranslate nohighlight">\(p\)</span> is the probability of an observation lying in bin <span class="math notranslate nohighlight">\(j\)</span> and</p>
<div class="math notranslate nohighlight">
\[
f_j = \frac{1}{h}\int_{\text{bin }j}f(u) du.
\]</div>
<p>Thus we obtain</p>
<div class="math notranslate nohighlight">
\[
E\left[ H(x) \right] = \frac{E\left[ N_{j}  \right]}{Nh} = f_j(x)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
Var\left(H(x)\right) = \frac{Var\left( N_{j}  \right)}{N^2 h^2}
= \frac{f_j\left( 1 - h f_j \right)}{N h}
\]</div>
<p>so</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
E\left[ \right\{ H(x) - f(x)\left\}^{2}  \right]
=&amp; E\left[ H^2(x) \right] - 2f(x) E\left[H(x)\right] + f^2(x) \\
=&amp; Var(H(x)) + E[H(x)]^2 - 2f(x) E\left[H(x)\right] + f^2(x) \\
=&amp; \frac{f_j\left( 1 - h f_j \right)}{N h} + f_j^2 - 2 f f_j + f^2\\
=&amp; \frac{f_j}{Nh} - \frac{f_j^2}{N} + (f_j - f)^2
\end{align}\end{split}\]</div>
<p>and integrating over the whole interval, with <span class="math notranslate nohighlight">\(\int p_j = h\sum p_j =
h = h\int f_j \rightarrow \int f_j = 1\)</span></p>
<div class="math notranslate nohighlight">
\[
MISE = \frac{1}{Nh} - \frac{1}{N}\int f_j^2(x)dx + \int (f_j(x) - f(x))^2 dx.
\]</div>
<p>The first term above refers to the sampling error made, and grows as <span class="math notranslate nohighlight">\(h\rightarrow 0\)</span> since bin counts stop reflecting the true density, some even being left empty. The last term refers to the bias from discretization, and goes to zero with <span class="math notranslate nohighlight">\(h\)</span>, as we show below. The mid term is proportional to <span class="math notranslate nohighlight">\(\sim \frac{1}{N}\)</span>, and vanishes in the asymptotic limit <span class="math notranslate nohighlight">\(N\rightarrow \infty\)</span>, which is the limit that interests us.</p>
<p>To deal with third term in the MISE, we expand <span class="math notranslate nohighlight">\(f(u)\)</span> around <span class="math notranslate nohighlight">\(x\)</span>, keeping the linear terms, which amounts to discarding contributions of order at most <span class="math notranslate nohighlight">\(O(h^{2})\)</span></p>
<div class="math notranslate nohighlight">
\[
f(u) \approx f(x) + f'(x)(u-x)
\]</div>
<p>The expression for <span class="math notranslate nohighlight">\(f_j(x)\)</span> can now be approximated</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
f_j \approx&amp; \frac{1}{h}\int_{\text{bin }j}\left( f(x) + f'(x)(u-x)  \right) du\\
=&amp; \frac{1}{h} \left( hf(x) + \frac{f'(x)}{2}\left( u-x \right)^{2} \Big|_{x_j}^{x_{j+1}} \right)\\
=&amp; f(x) + \frac{f'(x)}{2h}\left[\left( x_{j+1} -x \right)^{2} - \left( x_{j} -x \right)^{2}\right]\\
=&amp; f(x) + \frac{f'(x)}{2h}\left[\left( (x_{j+1} - x_j) - (x - x_j) \right)^{2} - \left( x - x_j \right)^{2}\right]\\
=&amp; f(x) + \frac{f'(x)}{2h}\left[\left( h - (x - x_j) \right)^{2} - \left( x - x_j \right)^{2}\right]\\
=&amp; f(x) + \frac{f'(x)}{2h}\left[ h^2 - 2h(x-x_j) \right]\\
=&amp; f(x) + \frac{hf'(x)}{2} - (x-x_j)f'(x),
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{j}\)</span> is the lower limit of bin <span class="math notranslate nohighlight">\(j\)</span>. We can identify the bias of <span class="math notranslate nohighlight">\(E[H(x)]\)</span> as <span class="math notranslate nohighlight">\(\frac{hf'(x)}{2} - (x-x_j)f'(x)\)</span>.</p>
<p>Plugging this expansion into the MISE, remebering the second term vanishes,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
MISE \approx&amp; \frac{1}{Nh} + \int \left(\frac{hf'(x)}{2} - (x-x_j)f'(x)\right)^2 dx\\
=&amp; \frac{1}{Nh} + \frac{h^2}{4}\int f'^2(x)dx + \int (x-x_j)^2f'^2(x) dx - h\int (x-x_j)f'^2(x) dx
\end{align}\end{split}\]</div>
<p>We can split the second a third integrals into the sum of integrals for each bin <span class="math notranslate nohighlight">\(j\)</span>, for which <span class="math notranslate nohighlight">\(x_j\)</span> are constants. With a change of variable, <span class="math notranslate nohighlight">\(y=x-x_j\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\int (x-x_j)^2f'^2(x) dx = \sum_j \int_0^h  y^2f'^2(y + x_j) dy\\
\int (x-x_j)f'^2(x) dx = \sum_j \int_0^h y f'^2(y + x_j) dy.
\end{align}\end{split}\]</div>
<p>Expanding yet again <span class="math notranslate nohighlight">\(f'(y + x_{j}) \approx f'(x_{j}) + O(h)\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_j \int_0^h  y^2(f'^2(x_j) +  O(h))dy = \sum_j \frac{h^{3}}{3} f'^2(x_j) + O(h^4) \\
\sum_j \int_0^h y (f'^2(x_j) +  O(h))dy = \sum_j \frac{h^{2}}{2} f'^2(x_j) +  O(h^{3}).
\end{align}\end{split}\]</div>
<p>Dropping the higher order terms, we can approximate the sums by integrals, by identifying <span class="math notranslate nohighlight">\(h = \Delta x\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_j \frac{h^{3}}{3} f'^2(x_j) \approx \frac{h^{2}}{3} \int  f'^2(x)dx  \\
\sum_j \frac{h^{2}}{2} f'^2(x_j) \approx \frac{h}{2}\int f'^2(x)dx.
\end{align}\end{split}\]</div>
<p>Plugging back into the MISE equation</p>
<div class="math notranslate nohighlight">
\[\begin{align}
MISE \approx&amp; \frac{1}{Nh} + \left(\frac{h^2}{4} + \frac{h^2}{3} - \frac{h^2}{2}\right)\int f'^2(x)dx
= \frac{1}{Nh} + \frac{h^2}{12}\int f'^2(x)dx.
\end{align}\]</div>
<p>And optmizing with respect to <span class="math notranslate nohighlight">\(h\)</span>, one obtains the optimal bin width <span class="math notranslate nohighlight">\(h^{*}\)</span></p>
<div class="math notranslate nohighlight">
\[
-\frac{1}{N (h^{*})^{2}} + \frac{h^{*}}{6} \int f'^2(x)dx = 0.
\]</div>
<p>Which yields</p>
<div class="math notranslate nohighlight">
\[
h^* = \left(\frac{6}{N \int f'^2(x)dx}\right)^\frac{1}{3}
\]</div>
<p>This optimal bin width depends on the density <span class="math notranslate nohighlight">\(f(x)\)</span>. Assuming a normal distribution gives</p>
<div class="math notranslate nohighlight">
\[
h^* = \left(\frac{24\sqrt{\pi}}{N}\right)^{\frac{1}{3}}s
\]</div>
<p>Another choice is to approximate the integral more robustly using the IQR, leading to the FD rule.</p>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="id2"><dl class="citation">
<dt class="label" id="id9"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>David W Scott. On optimal and data-based histograms. <em>Biometrika</em>, 66(3):605–610, 1979.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="U4-M2-L1-Gaussian_Mixture_Models.html" title="previous page">Gaussian Mixture Models</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Gonzalo G. Peraza Mues<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>