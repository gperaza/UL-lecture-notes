
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>(Dis)similarity metrics &#8212; Lecture Notes in Unsupervised and Reinforcement Learning</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Warning:" href="assignments-dummy/U2-M1-L1-similarity_metrics.html" />
    <link rel="prev" title="Assignment: Anomaly and Outlier Detection" href="assignments-dummy/U1-M1-L3-prep-outlier-detection.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/UPY-completo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Lecture Notes in Unsupervised and Reinforcement Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome!
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 1 - Unsupervised Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L1-prep-normalization.html">
   Data Normalization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L1-prep-normalization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L2-prep-discretization.html">
   Discretization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L2-prep-discretization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L3-unsup-outlier-detection.html">
   Anomaly and Outlier Detection
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L3-prep-outlier-detection.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 2 - Dimensionality Reduction
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active collapsible-parent">
  <a class="current reference internal" href="#">
   (Dis)similarity metrics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html">
     <strong>
      Warning
     </strong>
     :
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-1-2-pt">
     Exercise 1 (2 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-2-3-pt">
     Exercise 2 (3 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-3-4-pts">
     Exercise 3 (4 pts)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-4-1-pt">
     Exercise 4 (1 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-5-ungraded">
     Exercise 5 (ungraded)
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M1-L3-dim_red_PCA.html">
   Dimensionality Reduction and PCA
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="freedman-diaconis.html">
   Freedman-Diaconis Rule
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/U2-M1-L1-similarity_metrics.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/gperaza/UL-lecture-notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/gperaza/UL-lecture-notes/issues/new?title=Issue%20on%20page%20%2FU2-M1-L1-similarity_metrics.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-a-proximity-measure">
   What is a proximity measure?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dissimilarity-measures">
     Dissimilarity measures
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-measures">
     Similarity measures
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transforming-between-similarity-and-dissimilarity">
     Transforming between similarity and dissimilarity
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-dissimilarity-similarity-matrix">
   The dissimilarity (similarity) matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dissimilarity-measures-for-numerical-data">
   Dissimilarity measures for numerical data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weighted-l-p-norm-minkowski-distance">
     Weighted
     <span class="math notranslate nohighlight">
      \(l_{p}\)
     </span>
     -norm, Minkowski Distance
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#l-1-manhattan-distance">
       <span class="math notranslate nohighlight">
        \(l_1\)
       </span>
       , Manhattan distance
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#l-2-euclidean-distance">
       <span class="math notranslate nohighlight">
        \(l_2\)
       </span>
       , Euclidean distance
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#l-infty-chebyshev-distance">
       <span class="math notranslate nohighlight">
        \(l_\infty\)
       </span>
       , Chebyshev distance
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#canberra-distance">
     Canberra distance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mahalanobis-distance">
     Mahalanobis distance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#similarity-measures-for-numerical-data">
   Similarity measures for numerical data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cosine-similarity">
     Cosine similarity
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#l-2-normalized-euclidean-distance">
       <span class="math notranslate nohighlight">
        \(l_2\)
       </span>
       -normalized Euclidean distance
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#soft-cosine">
       Soft cosine
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pearson-s-correlation-coefficient">
     Pearson’s correlation coefficient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spearman-correlation">
     Spearman correlation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tanimoto-similarity">
     Tanimoto similarity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#based-on-weighted-l-2-norm">
     Based on weighted
     <span class="math notranslate nohighlight">
      \(l_2\)
     </span>
     norm
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#categorical-data-test-data">
   Categorical data, test data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#similarity-measures-for-categorical-data">
   Similarity measures for categorical data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-matching-coefficient-smc">
     Simple matching coefficient (SMC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jaccard-or-tanimoto-coefficient-jc">
     Jaccard or Tanimoto coefficient (JC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sorensen-dice-coefficient">
     Sørensen–Dice coefficient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ochiai-coefficient">
     Ochiai coefficient
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dissimilarity-measures-for-categorical-data">
   Dissimilarity measures for categorical data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hamming-distance">
     Hamming distance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dynamic-measures">
   Dynamic measures
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edit-distance">
     Edit Distance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dynamic-time-warping-dtw">
     Dynamic Time Warping (DTW)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#longest-common-subsequence-lcss">
     Longest Common Subsequence (LCSS)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mixed-features">
   Mixed features
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fuzzy-measures">
   Fuzzy measures
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#missing-data">
   Missing data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proximity-between-a-point-and-a-set">
   Proximity between a point and a set
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#point-representatives">
     Point representatives
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyper-plane-representatives">
     Hyper-plane representatives
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyper-sphere-representatives">
     Hyper-sphere representatives
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proximity-among-sets">
   Proximity among sets
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proximity-of-distributions">
   Proximity of distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kl-divergence">
     KL-divergence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jensen-shannon-distance">
     Jensen-Shannon distance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proximity-measures-on-graps">
   Proximity measures on graps
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-between-nodes-in-a-graph">
     Similarity between nodes in a graph
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#structural-distance">
       Structural distance
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#random-walk-based-similarity-pagerank">
       Random Walk-based similarity (PageRank)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#similarity-between-graphs">
     Similarity between graphs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="dis-similarity-metrics">
<h1>(Dis)similarity metrics<a class="headerlink" href="#dis-similarity-metrics" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Lacking response and class information, most unsupervised algorithms search for patterns based on how alike or different are observations with respect to each other. One consequence of this is that the outcome of these algorithms will depend on what we mean by two observations being alike or similar. In this lesson, we’ll review several measures of proximity, quantifying either similarity or dissimilarity between pair of observations, or sets of observations. Most of the present material was adapted from references <span id="id1">[<a class="reference internal" href="#id53"><span>1</span></a>,<a class="reference internal" href="#id63"><span>2</span></a>]</span>.</p>
</div>
<div class="section" id="what-is-a-proximity-measure">
<h2>What is a proximity measure?<a class="headerlink" href="#what-is-a-proximity-measure" title="Permalink to this headline">¶</a></h2>
<p>There are actually very few requirements for a valid proximity measure. The most important property is that the magnitude of the measure must reflect in some sense whether two observations are close or far apart. When quantifying similarity, the measure must be larger the more each pair of observations resemble each other, while when quantifying dissimilarity, the inverse would be true. A dissimilarity measure thus acts much as a distance between observations (though in strict a distance must satisfy additional constraints, such as the triangle inequality).</p>
<div class="section" id="dissimilarity-measures">
<h3>Dissimilarity measures<a class="headerlink" href="#dissimilarity-measures" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(d(\vec{x},\vec{y})\)</span> be the dissimilarity between two observations <span class="math notranslate nohighlight">\(\vec{x}\)</span> and <span class="math notranslate nohighlight">\(\vec{y}\)</span>. Then, for <span class="math notranslate nohighlight">\(d(\vec{x},\vec{y})\)</span> to be a valid dissimilarity measure the following properties must hold:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(d(\vec{x},\vec{y})\)</span> is a function <span class="math notranslate nohighlight">\(d:X\times X \rightarrow \mathbb{R}\)</span> such that <span class="math notranslate nohighlight">\(-\infty &lt; d_0 \leq d(\vec{x},\vec{y}) &lt; \infty\)</span>, where <span class="math notranslate nohighlight">\(d_0\)</span> is the minimum possible value for <span class="math notranslate nohighlight">\(d(\vec{x},\vec{y})\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(d(\vec{x},\vec{x}) = d_0\)</span></p></li>
<li><p>It is a symmetric function, <span class="math notranslate nohighlight">\(d(\vec{x},\vec{y}) = d(\vec{y},\vec{x})\)</span>. Actually, this last property does not hold for some functions used in practice, with some consequences, the most obvious being that the dissimilarity matrix is not symmetric.</p></li>
</ul>
<p>The measure <span class="math notranslate nohighlight">\(d(\vec{x},\vec{y})\)</span> is also a <strong>metric</strong> is the following properties also hold:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(d(\vec{x},\vec{y}) = d_0\)</span> if and only if <span class="math notranslate nohighlight">\(\vec{x} = \vec{y}\)</span></p></li>
<li><p>The triangle inequality: <span class="math notranslate nohighlight">\(d(\vec{x},\vec{z}) \leq d(\vec{x},\vec{y}) + d(\vec{y},\vec{z})\)</span>. The most famous metric is the euclidean distance, or <span class="math notranslate nohighlight">\(L_2\)</span>-norm.</p></li>
</ul>
</div>
<div class="section" id="similarity-measures">
<h3>Similarity measures<a class="headerlink" href="#similarity-measures" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(s(\vec{x},\vec{y})\)</span> be the dissimilarity between two observations <span class="math notranslate nohighlight">\(\vec{x}\)</span> and <span class="math notranslate nohighlight">\(\vec{y}\)</span>. Then, for <span class="math notranslate nohighlight">\(s(\vec{x},\vec{y})\)</span> to be a valid dissimilarity measure the following properties must hold:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s(\vec{x},\vec{y})\)</span> is a function <span class="math notranslate nohighlight">\(s:X\times X \rightarrow \mathbb{R}\)</span> such that <span class="math notranslate nohighlight">\(-\infty &lt; s(\vec{x},\vec{y}) \leq s_0 &lt; \infty\)</span>, where <span class="math notranslate nohighlight">\(s_0\)</span> is the maximum possible value for <span class="math notranslate nohighlight">\(s(\vec{x},\vec{y})\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(s(\vec{x},\vec{x}) = s_0\)</span></p></li>
<li><p>It is a symmetric function, <span class="math notranslate nohighlight">\(s(\vec{x},\vec{y}) = s(\vec{y},\vec{x})\)</span>.</p></li>
</ul>
<p>The measure <span class="math notranslate nohighlight">\(s(\vec{x},\vec{y})\)</span> is also a <strong>metric</strong> is the following properties also hold:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s(\vec{x},\vec{y}) = s_0\)</span> if and only if <span class="math notranslate nohighlight">\(\vec{x} = \vec{y}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(s(\vec{x},\vec{y})s(\vec{y},\vec{z}) \leq \left [ s(\vec{x},\vec{y}) + s(\vec{y},\vec{z})  \right ]s(\vec{x},\vec{z})\)</span>.</p></li>
</ul>
</div>
<div class="section" id="transforming-between-similarity-and-dissimilarity">
<h3>Transforming between similarity and dissimilarity<a class="headerlink" href="#transforming-between-similarity-and-dissimilarity" title="Permalink to this headline">¶</a></h3>
<p>It is relatively simple to transform from a similarity metric to a dissimilarity one, and vice-versa, or to build new metrics by transformation. It can be shown that the following transformations result in a valid metric:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s = \frac{a}{d}\)</span>, with <span class="math notranslate nohighlight">\(a&gt;0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(s = d_{max} - d\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d' = -\ln(d_{max} + k - d)\)</span> for <span class="math notranslate nohighlight">\(d(\vec{x},\vec{y})&gt;0\)</span> and <span class="math notranslate nohighlight">\(k&gt;0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(d' = \frac{kd}{1+d}\)</span> for <span class="math notranslate nohighlight">\(d(\vec{x},\vec{y})&gt;0\)</span> and <span class="math notranslate nohighlight">\(k&gt;0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(s' = \frac{1}{1-s}\)</span> for <span class="math notranslate nohighlight">\(s &lt; 1\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(s = \frac{1}{1+d}\)</span> is a common transformation</p></li>
</ul>
</div>
</div>
<div class="section" id="the-dissimilarity-similarity-matrix">
<h2>The dissimilarity (similarity) matrix<a class="headerlink" href="#the-dissimilarity-similarity-matrix" title="Permalink to this headline">¶</a></h2>
<p>We have previously defined the data matrix <span class="math notranslate nohighlight">\(X\)</span> as an <span class="math notranslate nohighlight">\(N\times D\)</span> matrix, where <span class="math notranslate nohighlight">\(N\)</span> is the number of observations and <span class="math notranslate nohighlight">\(D\)</span> is the number of features or dimensions. The (dis)similarity matrix (or distance matrix) (<span class="math notranslate nohighlight">\(D\)</span>) <span class="math notranslate nohighlight">\(S\)</span> is a <span class="math notranslate nohighlight">\(N\times N\)</span> matrix where each entry (<span class="math notranslate nohighlight">\(D_{ij}\)</span>) <span class="math notranslate nohighlight">\(S_{ij}\)</span> is the (dis)similarity between observations <span class="math notranslate nohighlight">\(\vec{x}_i\)</span> and <span class="math notranslate nohighlight">\(vec{x}_j\)</span>, (<span class="math notranslate nohighlight">\(d(\vec{x}_i,\vec{x}_j)\)</span>) <span class="math notranslate nohighlight">\(s(\vec{x}_i,\vec{x}_j)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
D = \begin{pmatrix}
d(x_1,x_1) &amp; d(x_1,x_2) &amp; \dots &amp; d(x_1,x_n)\\
d(x_2,x_1) &amp; d(x_2,x_2) &amp; \dots &amp; d(x_2,x_n)\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
d(x_n,x_1) &amp; d(x_n,x_2) &amp; \dots &amp; d(x_n,x_n)
\end{pmatrix}
\end{split}\]</div>
<p>We can define a general purpose function that accepts a proximity measure, and returns a (dis)similarity matrix:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>

<span class="k">def</span> <span class="nf">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="s1">&#39;euclidian&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">measure</span> <span class="o">==</span> <span class="s1">&#39;euclidian&#39;</span><span class="p">:</span>
        <span class="s2">&quot; This is fast for euclidian distance.&quot;</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
            <span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="s2">&quot; Revert to slower loop for unknown function.&quot;</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
            <span class="n">D</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">measure</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">D</span>
</pre></div>
</div>
<p>We can test our function with some sample numerical data, let’s use the Iris data set again. Remember, the data set consists of 150 observations with 4 features.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal width (cm)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal length (cm)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">3</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Petal width (cm)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Petal length (cm)&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/f63bddd1aa463c2df77a8712676f3ad4e15c61fe.png" /></p>
<p>And find the distance matrix, to avoid printing the whole matrix, let’s visualize the distance matrix using a heat map while defining a helper function for future use. To further explore the effect of each measure, we also project the distance matrix into a the “best” 2D representation using a technique called multidimensional scaling (MDS). MDS searches the best projection that preserves the original distances. We will study MDS in detail further into the course.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">MDS</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">mds</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="c1"># MDS projection</span>
        <span class="n">mds</span> <span class="o">=</span> <span class="n">MDS</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dissimilarity</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">fdata</span> <span class="o">=</span> <span class="n">mds</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
        <span class="n">ax3</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fdata</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">fdata</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Heat map</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>

    <span class="c1"># Hitogram</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">D</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/2c1c083c8777fef628e34f098291b3bff600b01b.png" /></p>
<ul class="simple">
<li><p>TODO: Make visualization interactive, such that selecting a pair in the heatmap, it colors such pair in the scatterplot, and displays the metric. Alternatively, select a pair of points in the scatter and display the metric.</p></li>
</ul>
</div>
<div class="section" id="dissimilarity-measures-for-numerical-data">
<h2>Dissimilarity measures for numerical data<a class="headerlink" href="#dissimilarity-measures-for-numerical-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="weighted-l-p-norm-minkowski-distance">
<h3>Weighted <span class="math notranslate nohighlight">\(l_{p}\)</span>-norm, Minkowski Distance<a class="headerlink" href="#weighted-l-p-norm-minkowski-distance" title="Permalink to this headline">¶</a></h3>
<p>The weighted <span class="math notranslate nohighlight">\(l_p\)</span>-norm is the most common dissimilarity measure. It is defines as</p>
<div class="math notranslate nohighlight">
\[
d_p(\vec{x},\vec{y}) = \left ( \sum_{i=1}^D w_i\left | x_i - y_i \right |^p \right )^{\frac{1}{p}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(y_i\)</span> are the ith feature of observation <span class="math notranslate nohighlight">\(\vec{x}\)</span> and <span class="math notranslate nohighlight">\(\vec{y}\)</span>, and <span class="math notranslate nohighlight">\(w_i&gt;0\)</span> is the weight coefficient. For <span class="math notranslate nohighlight">\(w_i = 1\)</span> we get the unweighted <span class="math notranslate nohighlight">\(l_p\)</span>-norm. The value of <span class="math notranslate nohighlight">\(p\)</span> controls the relative importance of large to small absolute differences <span class="math notranslate nohighlight">\(\left | x_i - y_i \right|\)</span>. The relative importance of large differences grows with the value of <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>The Mikowski distance is not scale invariant, so its usual to standardize the data before using this metric. It satisfies the triangle inequality, so it is a metric.</p>
<p>The most common <span class="math notranslate nohighlight">\(p\)</span> values are <span class="math notranslate nohighlight">\(p=1, 2, \infty\)</span>, discussed below. The <span class="math notranslate nohighlight">\(l_1\)</span> and <span class="math notranslate nohighlight">\(l_\infty\)</span> norms may be viewed as overestimation and underestimation of the <span class="math notranslate nohighlight">\(l_2\)</span> norm, since <span class="math notranslate nohighlight">\(d_{\infty}\leq d_2 \leq d_1\)</span>.</p>
<p>We now implement the general Minkowski distance within our original function. A similar implementation to that of the Euclidean distance would still be possible, but adjusting for an arbitrary exponent <span class="math notranslate nohighlight">\(p\)</span>. Here we instead define the distance function to further explore its properties.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">minkowski_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1">#x = np.asarray(x)</span>
    <span class="c1">#y = np.asarray(y)</span>

    <span class="k">if</span> <span class="s1">&#39;p&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Missing required parameter p&quot;</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
<p>It is interesting to study how the distance changes with <span class="math notranslate nohighlight">\(p\)</span>. Next, we consider 10 different distance from vectors of dimension 5 sampled from the <span class="math notranslate nohighlight">\(U(0,1)\)</span> distribution, an plot the Minkowsky distance vs <span class="math notranslate nohighlight">\(p\)</span></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">p_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">d_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">minkowski_d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">p_list</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">p_list</span><span class="p">,</span> <span class="n">d_list</span><span class="p">,</span> <span class="s1">&#39;o--&#39;</span><span class="p">)</span>

</pre></div>
</div>
<p><img alt="" src="_images/6b8c6b328c4c6fea6d82b82e2f62d64fcd1594ef.png" /></p>
<p>In general the distance decreases with <span class="math notranslate nohighlight">\(p\)</span> as we now show. We need to prove that <span class="math notranslate nohighlight">\(|\vec{x}|_{p} \geq |\vec{x}|_{p+1}\)</span>. For <span class="math notranslate nohighlight">\(\vec{x} = 0\)</span> the equality holds. For <span class="math notranslate nohighlight">\(\vec{x} \neq 0\)</span>, let <span class="math notranslate nohighlight">\(y_i = |x_i|/|\vec{x}|_{p+1}\leq 1\)</span>. Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
y_i^{p+1} &amp;\leq y_i^{p}\\
\sum_i y_i^{p+1} &amp;\leq \sum_i y_i^p\\
\sum_i y_i^{p+1} &amp;\leq \sum_i y_i^p\\
1 &amp;\leq \sum_i y_i^p
\end{align}\end{split}\]</div>
<p>From the last inequality</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_i |x_i|^p &amp;\geq |\vec{x}|_{p+1}^{p}\\
(\sum_i |x_i|^p)^{1/p} &amp;\geq (|\vec{x}|_{p+1}^{p})^{1/p}\\
|\vec{x}|_{p} &amp;\geq |\vec{x}|_{p+1}
\end{align}\end{split}\]</div>
<p>thus, proving the asseveration.</p>
<p><img alt="The following figure shows unit circles (the set of all points that are at the unit distance from the centre) with various values of . Source: https://en.wikipedia.org/wiki/Minkowski_distance" src="_images/2D_unit_balls.svg" /></p>
<div class="section" id="l-1-manhattan-distance">
<h4><span class="math notranslate nohighlight">\(l_1\)</span>, Manhattan distance<a class="headerlink" href="#l-1-manhattan-distance" title="Permalink to this headline">¶</a></h4>
<p>The <span class="math notranslate nohighlight">\(l_1\)</span> normal, also called the Manhattan distance, or the city-block distance, is defined as</p>
<div class="math notranslate nohighlight">
\[
d_1(\vec{x},\vec{y}) =  \sum_{i=1}^D \left | x_i - y_i \right |
\]</div>
<p>It’s called the city block distance, since it sums the absolute difference of each component, akin to summing the orthogonal distances transversed when moving on a squared road network. It is preferred over the <span class="math notranslate nohighlight">\(l_2\)</span> norm when outliers are present. A good standardization for the Manhattan distance is to divide each its feature by its range.</p>
<p><img alt="Taxicab geometry versus Euclidean distance: In taxicab geometry, the red, yellow, and blue paths all have the same shortest path length of 12. In Euclidean geometry, the green line has length and is the unique shortest path. Source: https://en.wikipedia.org/wiki/Taxicab_geometry#/media/File:Manhattan_distance.svg" src="_images/Manhattan_distance.svg" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">minkowski_d</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/01521b2040a5cccf701b562acd135ac8da18ca67.png" /></p>
<p>One possible weighted transformation for the Manhattan distance, which greatly exaggerates large distances, is</p>
<div class="math notranslate nohighlight">
\[
d_G(\vec{x},\vec{y}) = -log_{10}\left( 1 - \frac{1}{D}\sum_{i=1}^D \frac{|x_i - y_i|}{b_i - a_i}  \right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(b_i\)</span> and <span class="math notranslate nohighlight">\(a_i\)</span> are the maximum and minimum values among the ith features of N vectors of X, respectively.</p>
<p>Since <span class="math notranslate nohighlight">\(d_G\)</span> depends on the whole set X, through <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, we need to pass those vectors explicitly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">d_g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">d_g</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/591169c9a8c739509e2592bff2f063ccf0f772c3.png" /></p>
</div>
<div class="section" id="l-2-euclidean-distance">
<h4><span class="math notranslate nohighlight">\(l_2\)</span>, Euclidean distance<a class="headerlink" href="#l-2-euclidean-distance" title="Permalink to this headline">¶</a></h4>
<div class="math notranslate nohighlight">
\[
d_2(\vec{x},\vec{y}) = \sqrt{\left ( \sum_{i=1}^D \left (x_i - y_i \right )^2 \right )}
\]</div>
<p>We have already implemented the <span class="math notranslate nohighlight">\(l_2\)</span> norm as the euclidean distance before. We can verify the output of the Minkowsky distance with <span class="math notranslate nohighlight">\(p=2\)</span> is the same.</p>
<p>This is probably the most common distance metric, and many learning algorithms are design to work with Euclidean distance, such a KNN and K-means.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">minkowski_d</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/2c1c083c8777fef628e34f098291b3bff600b01b.png" /></p>
</div>
<div class="section" id="l-infty-chebyshev-distance">
<h4><span class="math notranslate nohighlight">\(l_\infty\)</span>, Chebyshev distance<a class="headerlink" href="#l-infty-chebyshev-distance" title="Permalink to this headline">¶</a></h4>
<p>In the limit <span class="math notranslate nohighlight">\(p \rightarrow \infty\)</span> the Minkowski distance becomes the Chebyshev distance, i.e., the maximum difference, component-wise.</p>
<div class="math notranslate nohighlight">
\[
d_\infty(\vec{x},\vec{y}) = \underset{1\leq i \leq D}{\operatorname{max}} \left | x_i - y_i  \right |
\]</div>
<div class="figure align-default" id="id72">
<img alt="_images/chess.png" src="_images/chess.png" />
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">The Chebyshev distance between two spaces on a chess board gives the minimum number of moves a king requires to move between them. This is because a king can move diagonally, so that the jumps to cover the smaller distance parallel to a rank or column is effectively absorbed into the jumps covering the larger. Above are the Chebyshev distances of each square from the square f6. Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Chebyshev_distance">https://en.wikipedia.org/wiki/Chebyshev_distance</a></span><a class="headerlink" href="#id72" title="Permalink to this image">¶</a></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">chebyshev_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">chebyshev_d</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/8dee37640746e6705547cde6166387377ff50ce8.png" /></p>
</div>
</div>
<div class="section" id="canberra-distance">
<h3>Canberra distance<a class="headerlink" href="#canberra-distance" title="Permalink to this headline">¶</a></h3>
<p>Text from <span id="id2">[<a class="reference internal" href="#id54"><span>3</span></a>]</span>.</p>
<p>The Canberra distance is a metric function often used for data scattered around an origin. It was introduced in 1966 (Lance &amp; Williams 1966) and is today mainly used in the form of 1967 (Lance &amp; Williams 1967).</p>
<p>The Canberra metric is similar to the Manhattan distance (which itself is a special form of the Minkowski distance). The distinction is that the absolute difference between the variables of the two objects is divided by the sum of the absolute variable values prior to summing. The generalised equation is given in the form:</p>
<div class="math notranslate nohighlight">
\[
d_C(\vec{x},\vec{y}) = \sum_{i=1}^D \frac{|x_i - y_i|}{|x_i| + |y_i|}
\]</div>
<p>This metric has the property that the result becomes unity when the variables are of opposite sign. It is useful in the special case where signs represent differences in kind rather than in degree. Anyhow, it is mainly used for values &gt; 0. This metric is easily biased for measures around the origin and very sensitive for values close to 0, where it is more sensitive to proportional than to absolute differences. This feature becomes more apparent in higher dimensional space, respectively an increasing number of variables. It is in turn less influenced than the Manhattan distance by variables with high values. As a very sensitive measurement it is applicable to identify deviations from normal readings.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">canberra_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">canberra_d</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/7e3732b5499d6618e3ab78d66a1f3355a0b73597.png" /></p>
<p>A metric that shares some properties wit the Canberra distance is</p>
<div class="math notranslate nohighlight">
\[
d_Q(\vec{x},\vec{y}) = \sqrt{\frac{1}{D}\sum_{i=1}^D \left(\frac{x_i-y_i}{x_i+y_i}\right)^2}
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">d_q</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(((</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">d_q</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/877514e9e578f1c376fe7b550b16f2896e80a00d.png" /></p>
<p>Yet, another similar metric is the Bray-Curtis distance. The Bray-Curtis distance is in the range [0, 1] if all coordinates are positive, and is undefined if the inputs are of length zero.</p>
<div class="math notranslate nohighlight">
\[
d_{BC}(\vec{x},\vec{y}) = \frac{\sum_{i=1}^D |x_i-y_i|}{\sum_{i=1}^D |x_i+y_i|}
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bc_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">))</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">bc_d</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/573ec2969beb06a34d3f932166a328aefa4a14e6.png" /></p>
</div>
<div class="section" id="mahalanobis-distance">
<h3>Mahalanobis distance<a class="headerlink" href="#mahalanobis-distance" title="Permalink to this headline">¶</a></h3>
<p>A generalization of the <span class="math notranslate nohighlight">\(l_2\)</span> norm is</p>
<div class="math notranslate nohighlight">
\[
d(\vec{x},\vec{y}) = \sqrt{(\vec{x} - \vec{y})^T B (\vec{x} - \vec{y})}
\]</div>
<p>where <span class="math notranslate nohighlight">\(B\)</span> is a symmetric, positive definite matrix.</p>
<p>For the special case where <span class="math notranslate nohighlight">\(B\)</span> is the inverse of the covariance matrix, we obtain the <strong>Mahalanobis distance</strong></p>
<div class="math notranslate nohighlight">
\[
d_M(\vec{x},\vec{y}) = \sqrt{(\vec{x} - \vec{y})^T \Sigma^{-1} (\vec{x} - \vec{y})}
\]</div>
<p>The Mahalanobis distance projects the standard euclidean distance onto the principal axes, then scales each component by the variance along those axes. Think of it as a generalization of the uni-variate standardization <span class="math notranslate nohighlight">\((x - y)\sigma\)</span>.</p>
<p>Consider the diagonalization of the <span class="math notranslate nohighlight">\(D\times D\)</span> covariance matrix <span class="math notranslate nohighlight">\(\Sigma = (X-\mu)^T (X-\mu)/(N-1)\)</span>, <span class="math notranslate nohighlight">\(\Sigma = U \Lambda U^T\)</span>. Here <span class="math notranslate nohighlight">\(U\)</span> is the matrix of column eigenvector, where each column a eigenvector pointing in the directions of the principal components, and <span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix with the variances along the principal axes in the diagonal.</p>
<p>A single centered observation is rotated by <span class="math notranslate nohighlight">\(\vec{x}' = U^T \vec{x}\)</span>. So to project the difference vector <span class="math notranslate nohighlight">\(\vec{\delta}\)</span> onto the principal components we do <span class="math notranslate nohighlight">\(\vec{\delta}' = U^T (\vec{x} - \vec{y})\)</span>.</p>
<p>Now, in the new space, features are uncorrelated, so we can standardize each component dividing by the standard deviation along each axis, <span class="math notranslate nohighlight">\(\delta'_i/s_i\)</span>. Calculating the squared euclidean distance with the scaled components is equivalent to the following matrix operation</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
d^2 = \vec{\delta}'^T
\begin{pmatrix}
1/s_1^2 &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; 1/s_2^2 &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; 1/s_D^2
\end{pmatrix}
\vec{\delta}'
= \vec{\delta}'^T \Lambda^{-1} \vec{\delta}'
\end{align}\end{split}\]</div>
<p>With the rotations made explicit</p>
<div class="math notranslate nohighlight">
\[
d^2 = (\vec{x} - \vec{y})^T U \Lambda^{-1} U^T (\vec{x} - \vec{y})
= (\vec{x} - \vec{y})^T \Sigma^{-1} (\vec{x} - \vec{y})
\]</div>
<p>which makes the equivalence explicit.</p>
<p>You will implement the Mahalanobis distance in the assignment, as a function called <code class="docutils literal notranslate"><span class="pre">mahalanobis_d</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mahalanobis_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="s2">&quot; A required argument is the covariance matrix S. &quot;</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;S&#39;</span><span class="p">]</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">d</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">S</span><span class="p">)</span> <span class="o">@</span> <span class="n">d</span><span class="p">)</span>

<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">mahalanobis_d</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/fdb19a4e634220c4c246e64a08efd32a53d3268d.png" /></p>
</div>
</div>
<div class="section" id="similarity-measures-for-numerical-data">
<h2>Similarity measures for numerical data<a class="headerlink" href="#similarity-measures-for-numerical-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="cosine-similarity">
<h3>Cosine similarity<a class="headerlink" href="#cosine-similarity" title="Permalink to this headline">¶</a></h3>
<p>Some text from <span id="id3">[<a class="reference internal" href="#id55"><span>4</span></a>]</span>.</p>
<p>Cosine similarity is a similarity metric based on the inner product of two vectors. It takes its name from the geometrical interpretation of the inner product, which gives the cosine of the angle between two vectors as</p>
<div class="math notranslate nohighlight">
\[
s_{\cos}(\vec{x},\vec{y}) = \frac{\vec{x}^T\vec{y}}{|\vec{x}||\vec{y}|}
\]</div>
<p>It is useful when the relative proportions among coordinates is important, while ignoring their absolute magnitude. This measure is invariant under rotations, but not to linear transformations, since general transformations do not preserve angles. It is also the same as the inner product of the same vectors normalized to both have length 1.</p>
<p>The cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1], and it’s most commonly used in high-dimensional positive spaces.</p>
<p>For example, in information retrieval and text mining, each term is notionally assigned a different dimension and a document is characterised by a vector where the value in each dimension corresponds to the number of times the term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter. For text matching, the attribute vectors A and B are usually the term frequency vectors of the documents. Cosine similarity can be seen as a method of normalizing document length during comparison.</p>
<p>The technique is also used to measure cohesion within clusters in the field of data mining.</p>
<p>One advantage of cosine similarity is its low-complexity, especially for sparse vectors: only the non-zero dimensions need to be considered.</p>
<p>A related distance measure is the cosine distance, defined for positive spaces</p>
<div class="math notranslate nohighlight">
\[
d_{\cos}(\vec{x},\vec{y}) = 1 - s_{\cos}(\vec{x},\vec{y})
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cosine_s</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">x_n</span> <span class="o">*</span> <span class="n">y_n</span><span class="p">)</span>
    <span class="c1"># Clip values which outside domain due to rounding errors.</span>
    <span class="n">cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cs</span>

<span class="k">def</span> <span class="nf">cosine_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cosine_s</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">cosine_d</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/124fb20d09215096163a4496132267847041f102.png" /></p>
<p>It is important to note, however, that this is not a proper distance metric as it does not have the triangle inequality property. To repair the triangle inequality property while maintaining the same ordering, it is necessary to convert to angular distance.</p>
<p>When the vector elements may be positive or negative:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  &amp;\text{angular distance} =
  \frac{\cos^{-1}(\text{cosine similarity})}{\pi}\\
  &amp;\text{angular similarity} = 1 - \text{angular distance}
\end{align}\end{split}\]</div>
<p>Or, if the vector elements are always positive:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  &amp;\text{angular distance} =
  2\frac{\cos^{-1}(\text{cosine similarity})}{\pi}\\
  &amp;\text{angular similarity} = 1 - \text{angular distance}
\end{align}\end{split}\]</div>
<p>The advantage of the angular similarity coefficient is that, when used as a difference coefficient (by subtracting it from 1) the resulting function is a proper distance metric, which is not the case for the first meaning. However, for most uses this is not an important property. For any use where only the relative ordering of similarity or distance within a set of vectors is important, then which function is used is immaterial as the resulting order will be unaffected by the choice.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">angular_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">arccos</span><span class="p">(</span><span class="n">cosine_s</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">angular_d</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/5056aff084cceda093e9c95dfcfb0a11f821a77c.png" /></p>
<p>When working with text documents, often a damping function is applied to word frequencies, such as <code class="docutils literal notranslate"><span class="pre">sqrt</span></code> or <code class="docutils literal notranslate"><span class="pre">log</span></code>, to dampen the effect of high frequency counts. Also, to take into account that some words are more common than others, frequencies are weighted by the inverse document frequency, <span class="math notranslate nohighlight">\(\log(n/n_{i})\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of documents in the collection, and <span class="math notranslate nohighlight">\(n_{i}\)</span> is the number of documents containing the ith word. For more information, refer to Section 3.3 of <span id="id4">[<a class="reference internal" href="#id63"><span>2</span></a>]</span>.</p>
<div class="section" id="l-2-normalized-euclidean-distance">
<h4><span class="math notranslate nohighlight">\(l_2\)</span>-normalized Euclidean distance<a class="headerlink" href="#l-2-normalized-euclidean-distance" title="Permalink to this headline">¶</a></h4>
<p>Another effective proxy for Cosine Distance can be obtained by <span class="math notranslate nohighlight">\(l_2\)</span> normalisation of the vectors, followed by the application of normal Euclidean distance. Using this technique each term in each vector is first divided by the magnitude of the vector, yielding a vector of unit length, which lies on the unit circle. Then, it is clear, the Euclidean distance over the end-points of any two vectors is a proper metric which gives the same ordering as the Cosine distance for any comparison of vectors (imagine drawing lines between vector points on the unit circle), and furthermore avoids the potentially expensive trigonometric operations required to yield a proper metric. Once the normalization has occurred, the vector space can be used with the full range of techniques available to any Euclidean space, notably standard dimensionality reduction techniques. This normalized form distance is notably used within many Deep Learning algorithms.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">d2_normlzd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">d2_normlzd</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/d804c1c7bb028fdf45f7cebdbb27ea9f29c7ce87.png" /></p>
</div>
<div class="section" id="soft-cosine">
<h4>Soft cosine<a class="headerlink" href="#soft-cosine" title="Permalink to this headline">¶</a></h4>
<p>Finally, cosine similarity assumes features are completely independent. If we believe there exist an intrinsic similarity among some features, e.g. some words as “game” and “play”, we may introduce such similarity through a series of weights relating feature <span class="math notranslate nohighlight">\(i\)</span> to feature <span class="math notranslate nohighlight">\(j\)</span>, <span class="math notranslate nohighlight">\(s_{ij}\)</span>, and calculate the “soft cosine similarity” as</p>
<div class="math notranslate nohighlight">
\[
\operatorname {soft\_cosine} _{1}(\vec{x},\vec{y}) =
\frac {\sum \nolimits_{i,j}^{D} s_{ij} x_{i} y_{j}}
{\sqrt {\sum \nolimits_{i,j}^{D} s_{ij} x_{i} x_{j}}
 \sqrt {\sum \nolimits_{i,j}^{D} s_{ij} y_{i} y_{j}}},
\]</div>
</div>
</div>
<div class="section" id="pearson-s-correlation-coefficient">
<h3>Pearson’s correlation coefficient<a class="headerlink" href="#pearson-s-correlation-coefficient" title="Permalink to this headline">¶</a></h3>
<p>The correlation coefficient is equivalent to a centered cosine similarity.</p>
<div class="math notranslate nohighlight">
\[
\rho(\vec{x},\vec{y}) = \frac{COV(\vec{x},\vec{y})}{\sigma_{x}\sigma{y}}
= \frac{(\vec{x}-\mu_{x})^T(\vec{y}-\mu_{y})}
       {|(\vec{x}-\mu_{x})||(\vec{y}-\mu_{y})|}
\]</div>
<p>with <span class="math notranslate nohighlight">\(-1 \leq \rho \leq 1\)</span>. A related dissimilarity measure being</p>
<div class="math notranslate nohighlight">
\[
d_{\rho}(\vec{x}m\vec{y}) = \frac{1 - \rho}{2}
\]</div>
<p>with <span class="math notranslate nohighlight">\(0 \leq d_{\rho} \leq 1\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pearson_s</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">cosine_s</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pearson_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pearson_s</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">pearson_d</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/877e1cab484ff1883f3bfa74a4c2e45cb11b7992.png" /></p>
<p>A nice property of correlation measures is that they account for biases of one vector with respect to another. For example, <span class="math notranslate nohighlight">\(\rho((5,4,5,4),(4,3,4,3)) = 1\)</span>, so they are sensible to similar linear patters in the data.</p>
</div>
<div class="section" id="spearman-correlation">
<h3>Spearman correlation<a class="headerlink" href="#spearman-correlation" title="Permalink to this headline">¶</a></h3>
<p>Spearman correlation is similar to Pearson correlation, but only uses rank information (positions within a list of values), rather than the actual values. This makes Spearman correlation less sensitive to outliers in the data, and allows its use with ordinal data.</p>
<p>From <span id="id5">[<a class="reference internal" href="#id56"><span>5</span></a>]</span>. The Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson’s correlation assesses linear relationships, Spearman’s correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or −1 occurs when each of the variables is a perfect monotone function of the other.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">spearman_s</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">xr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">yr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pearson_s</span><span class="p">(</span><span class="n">xr</span><span class="p">,</span><span class="n">yr</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">spearman_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">spearman_s</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">spearman_d</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/0e0a17c7549246605107bd06a3ef6ee7a9e3dbd4.png" /></p>
</div>
<div class="section" id="tanimoto-similarity">
<h3>Tanimoto similarity<a class="headerlink" href="#tanimoto-similarity" title="Permalink to this headline">¶</a></h3>
<p>More typically used for categorical data, the Jaccard, or Tanimoto, coefficient can be also used with real valued vectors. It is defined as</p>
<div class="math notranslate nohighlight">
\[
s_T(\vec{x},\vec{y}) = \frac{\vec{x}^T\vec{y}}
                            {|\vec{x}|^2 + |\vec{y}|^2 - \vec{x}^T\vec{y}}
= \frac{1}{1 + \frac{(\vec{x}-\vec{y})^T(\vec{x}-\vec{y})}{\vec{x}^T\vec{y}}}
\]</div>
<p>The Tanimoto measure is inversely proportional to the squared Euclidean distance divided by the inner product. Since the inner product can thought as a measure of correlation, the more correlated the vectors are, the larger the value of <span class="math notranslate nohighlight">\(s_T\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tanimoto_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">@</span> <span class="n">y</span> <span class="o">-</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">tanimoto_d</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/1a251c4e4809b97960da25f00ce4cbea57fee059.png" /></p>
</div>
<div class="section" id="based-on-weighted-l-2-norm">
<h3>Based on weighted <span class="math notranslate nohighlight">\(l_2\)</span> norm<a class="headerlink" href="#based-on-weighted-l-2-norm" title="Permalink to this headline">¶</a></h3>
<p>Another measure that is sometimes used is, which also shares some properties with the Canberra distance, is the Euclidean distance weighted by the sum of the length of both vectors</p>
<div class="math notranslate nohighlight">
\[
s_c(\vec{x},\vec{y}) = 1 - \frac{d_2(\vec{x},\vec{y})}{|\vec{x}|+|\vec{y}|}
\]</div>
<p><span class="math notranslate nohighlight">\(s_c\)</span> takes its maximum value, 1, when <span class="math notranslate nohighlight">\(\vec{x}=\vec{y}\)</span> and its minimum, 0, when <span class="math notranslate nohighlight">\(\vec{x} = -\vec{y}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">w_d2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span>
                                <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">w_d2</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/bb98ff4ba4fcf6b6246d552527a5c36640b1a728.png" /></p>
</div>
</div>
<div class="section" id="categorical-data-test-data">
<h2>Categorical data, test data<a class="headerlink" href="#categorical-data-test-data" title="Permalink to this headline">¶</a></h2>
<p>Before introducing proximity measures for categorical data, we need an appropriate example data set with categorical variables. We will use the <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer">breast cancer data set</a> from <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets.php">UCI Machine Learning repository</a>.</p>
<p>From the official description:</p>
<p>This is one of three domains provided by the Oncology Institute that has repeatedly appeared in the machine learning literature. (See also lymphography and primary-tumor.) This data set includes 201 instances of one class and 85 instances of another class. The instances are described by 9 attributes, some of which are linear and some are nominal.</p>
<p>The attributes of the data set are, with their respective possible classes:</p>
<ol class="simple">
<li><p>class: no-recurrence-events, recurrence-events</p></li>
<li><p>age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.</p></li>
<li><p>menopause: lt40, ge40, premeno.</p></li>
<li><p>tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.</p></li>
<li><p>inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.</p></li>
<li><p>node-caps: yes, no.</p></li>
<li><p>deg-malig: 1, 2, 3.</p></li>
<li><p>breast: left, right.</p></li>
<li><p>breast-quad: left-up, left-low, right-up, right-low, central.</p></li>
<li><p>irradiat: yes, no.</p></li>
</ol>
<p>We now import the <a href="Data/breast-cancer.csv">breast-cancer.csv</a> file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">cancer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Data/breast-cancer.csv&#39;</span><span class="p">)</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">cancer</span><span class="p">[</span><span class="n">cancer</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span>
<span class="n">cancer</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>class</p></th>
<th class="head"><p>age</p></th>
<th class="head"><p>menopause</p></th>
<th class="head"><p>tumor-size</p></th>
<th class="head"><p>inv-nodes</p></th>
<th class="head"><p>node-caps</p></th>
<th class="head"><p>deg-malig</p></th>
<th class="head"><p>breast</p></th>
<th class="head"><p>breast-quad</p></th>
<th class="head"><p>irradiat</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>no-recurrence-events</p></td>
<td><p>30-39</p></td>
<td><p>premeno</p></td>
<td><p>30-34</p></td>
<td><p>0-2</p></td>
<td><p>no</p></td>
<td><p>3</p></td>
<td><p>left</p></td>
<td><p>left~low~</p></td>
<td><p>no</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>no-recurrence-events</p></td>
<td><p>40-49</p></td>
<td><p>premeno</p></td>
<td><p>20-24</p></td>
<td><p>0-2</p></td>
<td><p>no</p></td>
<td><p>2</p></td>
<td><p>right</p></td>
<td><p>right~up~</p></td>
<td><p>no</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>no-recurrence-events</p></td>
<td><p>40-49</p></td>
<td><p>premeno</p></td>
<td><p>20-24</p></td>
<td><p>0-2</p></td>
<td><p>no</p></td>
<td><p>2</p></td>
<td><p>left</p></td>
<td><p>left~low~</p></td>
<td><p>no</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>no-recurrence-events</p></td>
<td><p>60-69</p></td>
<td><p>ge40</p></td>
<td><p>15-19</p></td>
<td><p>0-2</p></td>
<td><p>no</p></td>
<td><p>2</p></td>
<td><p>right</p></td>
<td><p>left~up~</p></td>
<td><p>no</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>no-recurrence-events</p></td>
<td><p>40-49</p></td>
<td><p>premeno</p></td>
<td><p>0-4</p></td>
<td><p>0-2</p></td>
<td><p>no</p></td>
<td><p>2</p></td>
<td><p>right</p></td>
<td><p>right~low~</p></td>
<td><p>no</p></td>
</tr>
</tbody>
</table>
<p>We can explore the relationship between pairs of variables using a scatter-plot. Due to large number of features, here we show only a single pair. To interactively explore different combination pairs, please run the code locally.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cols</span> <span class="o">=</span> <span class="n">cancer</span><span class="o">.</span><span class="n">columns</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">j</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">x1</span> <span class="o">=</span> <span class="n">cancer</span><span class="p">[</span><span class="n">cols</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">codes</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">cancer</span><span class="p">[</span><span class="n">cols</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">codes</span>
<span class="c1"># Add jitter</span>
<span class="n">x1</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
<span class="n">y1</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">y1</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">cols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">cols</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">cancer</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">codes</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/28f63ff11664fd309f294f9e7bb628cc1cd11d03.png" /></p>
<p>We will define some of our metrics to work with one-hot encoded data. So we ask pandas to do that for us.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_oh</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">cancer</span><span class="p">)</span>
<span class="n">cancer_oh</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>class~no~-recurrence-events</p></th>
<th class="head"><p>class~recurrence~-events</p></th>
<th class="head"><p>age~20~-29</p></th>
<th class="head"><p>age~30~-39</p></th>
<th class="head"><p>age~40~-49</p></th>
<th class="head"><p>age~50~-59</p></th>
<th class="head"><p>age~60~-69</p></th>
<th class="head"><p>age~70~-79</p></th>
<th class="head"><p>menopause~ge40~</p></th>
<th class="head"><p>menopause~lt40~</p></th>
<th class="head"><p>menopause~premeno~</p></th>
<th class="head"><p>tumor-size~0~-4</p></th>
<th class="head"><p>tumor-size~10~-14</p></th>
<th class="head"><p>tumor-size~15~-19</p></th>
<th class="head"><p>tumor-size~20~-24</p></th>
<th class="head"><p>tumor-size~25~-29</p></th>
<th class="head"><p>tumor-size~30~-34</p></th>
<th class="head"><p>tumor-size~35~-39</p></th>
<th class="head"><p>tumor-size~40~-44</p></th>
<th class="head"><p>tumor-size~45~-49</p></th>
<th class="head"><p>tumor-size~5~-9</p></th>
<th class="head"><p>tumor-size~50~-54</p></th>
<th class="head"><p>inv-nodes~0~-2</p></th>
<th class="head"><p>inv-nodes~12~-14</p></th>
<th class="head"><p>inv-nodes~15~-17</p></th>
<th class="head"><p>inv-nodes~24~-26</p></th>
<th class="head"><p>inv-nodes~3~-5</p></th>
<th class="head"><p>inv-nodes~6~-8</p></th>
<th class="head"><p>inv-nodes~9~-11</p></th>
<th class="head"><p>node-caps_?</p></th>
<th class="head"><p>node-caps~no~</p></th>
<th class="head"><p>node-caps~yes~</p></th>
<th class="head"><p>deg-malig~1~</p></th>
<th class="head"><p>deg-malig~2~</p></th>
<th class="head"><p>deg-malig~3~</p></th>
<th class="head"><p>breast~left~</p></th>
<th class="head"><p>breast~right~</p></th>
<th class="head"><p>breast-quad_?</p></th>
<th class="head"><p>breast-quad~central~</p></th>
<th class="head"><p>breast-quad~leftlow~</p></th>
<th class="head"><p>breast-quad~leftup~</p></th>
<th class="head"><p>breast-quad~rightlow~</p></th>
<th class="head"><p>breast-quad~rightup~</p></th>
<th class="head"><p>irradiat~no~</p></th>
<th class="head"><p>irradiat~yes~</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>We now have 45 binary features, but beware, the ones referring to same original feature are not independent.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Xc</span> <span class="o">=</span> <span class="n">cancer_oh</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;class_no-recurrence-events&#39;</span><span class="p">,</span>
                             <span class="s1">&#39;class_recurrence-events&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span>
<span class="n">yc</span> <span class="o">=</span> <span class="n">cancer_oh</span><span class="p">[</span><span class="s1">&#39;class_recurrence-events&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
<p>It is important to take care when using the proximity measures, since depending on the representation (original vs one-hot) the way of calculating each measure may change. We will point out such differences when appropriate.</p>
</div>
<div class="section" id="similarity-measures-for-categorical-data">
<h2>Similarity measures for categorical data<a class="headerlink" href="#similarity-measures-for-categorical-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="simple-matching-coefficient-smc">
<h3>Simple matching coefficient (SMC)<a class="headerlink" href="#simple-matching-coefficient-smc" title="Permalink to this headline">¶</a></h3>
<p>The Simple Matching Coefficient just counts how many entries match for both vectors. It is defined as</p>
<div class="math notranslate nohighlight">
\[
\text{SMC} = \frac {\text{number of matching attributes}}{\text{number of attributes}}
\]</div>
<p>When dealing with categorical vectors in the original representation, the above equation must be applied directly.</p>
<p>When dealing with one-hot encoded vectors, counting the matches is equivalent to counting the number of matching ones, ignoring the zeroes, which in turn is equivalent to the dot product of both vectors, over the number of original dimensions</p>
<div class="math notranslate nohighlight">
\[
SMC_{oh}(\vec{x},\vec{y}) = \frac{\vec{x}^T\vec{y}}{D_{orignal}}
= \frac{\vec{x}^T\vec{y}}{|x|_1 + |y|_1 - \vec{x}^T\vec{y}}
\]</div>
<p>This last definition is equivalent to the Jaccard index for binary vectors.</p>
<p>We need to distinguish between one-hot encoded binary vectors, and natural binary vectors, for which the original representation consists of only two classes encoded as 1 and 0. If the attributes are symmetrical, (1s and 0s are equivalently important, i.e., carry equivalent information), the SMC must count matches of both ones and zeroes, and the above equation does not apply, and we must fall back to the first definition. As an example, consider gender encoded as male:0 and female:1. It is clear that both 1s and 0s are equally important.</p>
<p>For asymmetric natural binary vectors, for which the 0s do not carry the same amount of information as 1s, the Jaccard index (see below) is a more appropriate metric. For example (from <span id="id6">[<a class="reference internal" href="#id58"><span>6</span></a>]</span>), in market basket analysis, the basket of two consumers who we wish to compare might only contain a small fraction of all the available products in the store, so the SMC will usually return very high values of similarities even when the baskets bear very little resemblance, thus making the Jaccard index a more appropriate measure of similarity in that context. For example, consider a supermarket with 1000 products and two customers. The basket of the first customer contains salt and pepper and the basket of the second contains salt and sugar. In this scenario, the similarity between the two baskets as measured by the Jaccard index would be 1/3, but the similarity becomes 0.998 using the SMC.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">smc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">one_hot</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;one_hot&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">one_hot</span><span class="p">:</span>
        <span class="n">matches</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">matches</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">d_smc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">smc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">Xc</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">d_smc</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">yc</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/f1ffc63445d982b896f6ef92ed36ec4184927d70.png" /></p>
<p>In a similar manner to how the Mahalanobis distance takes into account properties of the whole data set, we can weight the SMC for categorical data so that matches on unusual values contribute more to the similarity than matches on usual non-informative values. One way to this is to weight the similarity of a single feature by the <strong>inverse occurrence frequency</strong> of the feature in the whole data set:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
SMC_i =
\begin{cases}
\frac{1}{f_i(x_i)^2} &amp; \text{if $x_i=y_i$}\\
0 &amp; \text{otherwise}
\end{cases}
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f_i\)</span> is the relative frequency of class <span class="math notranslate nohighlight">\(x_i\)</span> in feature <span class="math notranslate nohighlight">\(i\)</span>.</p>
</div>
<div class="section" id="jaccard-or-tanimoto-coefficient-jc">
<h3>Jaccard or Tanimoto coefficient (JC)<a class="headerlink" href="#jaccard-or-tanimoto-coefficient-jc" title="Permalink to this headline">¶</a></h3>
<p>The Jaccardor Tanimoto coefficient was designed as a similarly coefficient among sets, the original definition being</p>
<div class="math notranslate nohighlight">
\[
s_J(A,B) = \frac{|A \cap B |}{|A \cup B|}
= \frac{|A \cap B |}{|A| + |B| - |A \cap B |}
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(0 \leq s_J\ leq 1\)</span>, and if both sets are empty, we define <span class="math notranslate nohighlight">\(s_J=1\)</span>.</p>
<p>The index is also used with binary categorical feature vectors, for which it takes the form</p>
<div class="math notranslate nohighlight">
\[
s_J = \frac{\vec{x}^T\vec{y}}{|x|^2 + |y|^2 - \vec{x}^T\vec{y}}
\]</div>
<p>When working with natural binary vectors, the Jaccard index is more appropriate than the SMC for asymmetric features, for example, in market basket analysis, where ones indicate the presence of an item in a set, and zeros signal the absence of an item in a set. When working with symmetric features the SMC is more appropriate, as discussed in the preceding section. The Jaccard index is also the most popular similarity measure for comparing chemical structures represented by means of fingerprints <span id="id7">[<a class="reference internal" href="#id59"><span>7</span></a>]</span>.</p>
<p>In the case of one-hot encoded binary vectors, the Jaccard index is equivalent to the SMC in the original representation, and was implement above.</p>
<p>For integer categorical vectors, where each coordinates contains, for example, the degree or strength of a feature (0 meaning total absence), the Jaccard index takes the form</p>
<div class="math notranslate nohighlight">
\[
s_J = \frac{n_{matches}}{n_x + n_y - n_{x,y&gt;0}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(n_x\)</span> and <span class="math notranslate nohighlight">\(n_y\)</span> are the number of non-zero entries of <span class="math notranslate nohighlight">\(\vec{x}\)</span> and <span class="math notranslate nohighlight">\(\vec{y}\)</span>, <span class="math notranslate nohighlight">\(n_{matches}\)</span> are the number of non-zero matches, and <span class="math notranslate nohighlight">\(n_{x,y &gt; 0}\)</span> is the number entries in x and y that are simultaneously larger than zero.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">s_j</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">binary</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Binarize vectors</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">yb</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">nx</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">ny</span> <span class="o">=</span> <span class="n">yb</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">nxy</span> <span class="o">=</span> <span class="n">xb</span> <span class="o">@</span> <span class="n">yb</span>
        <span class="n">n_matches</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">nxy</span>
        <span class="k">return</span> <span class="n">n_matches</span> <span class="o">/</span> <span class="p">(</span><span class="n">nx</span> <span class="o">+</span> <span class="n">ny</span> <span class="o">-</span> <span class="n">nxy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="sorensen-dice-coefficient">
<h3>Sørensen–Dice coefficient<a class="headerlink" href="#sorensen-dice-coefficient" title="Permalink to this headline">¶</a></h3>
<p>The SD coefficient is defined, among two sets <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, as:</p>
<div class="math notranslate nohighlight">
\[
SD = \frac{2|A \cap B|}{|A| + |B|}
\]</div>
<p>For binary data it takes the form</p>
<div class="math notranslate nohighlight">
\[
SD = \frac{2\vec{x}^T\vec{y}}{|\vec{x}|^2+|\vec{y}|^2}
\]</div>
<p>The SD is not a metric, but it is equivalent to the Jaccard index, since one is a monotone function of the other, <span class="math notranslate nohighlight">\(J = S/(2-S)\)</span>.</p>
<p>A generalization of both Jaccard and SD is the Tversky index <span id="id8">[<a class="reference internal" href="#id67"><span>8</span></a>]</span>.</p>
</div>
<div class="section" id="ochiai-coefficient">
<h3>Ochiai coefficient<a class="headerlink" href="#ochiai-coefficient" title="Permalink to this headline">¶</a></h3>
<p>In biology, a categorical equivalent to the cosine similarity is the Ochiai coefficient which can be represented as <span id="id9">[<a class="reference internal" href="#id55"><span>4</span></a>]</span>:</p>
<div class="math notranslate nohighlight">
\[
K= \frac {|A \cap B|}{\sqrt {|A|\times |B|}}
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are sets, and <span class="math notranslate nohighlight">\(|A|\)</span> is the number of elements in <span class="math notranslate nohighlight">\(A\)</span>, not the norm of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>If sets are represented as bit vectors, the Otsuka-Ochiai coefficient can be seen to be the same as the cosine similarity.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">Xc</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">cosine_d</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">yc</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/1fea390cb7effdbf16ded6d34338b635b5e9d3e4.png" /></p>
</div>
</div>
<div class="section" id="dissimilarity-measures-for-categorical-data">
<h2>Dissimilarity measures for categorical data<a class="headerlink" href="#dissimilarity-measures-for-categorical-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="hamming-distance">
<h3>Hamming distance<a class="headerlink" href="#hamming-distance" title="Permalink to this headline">¶</a></h3>
<p>Defined as the number of places where two vectors differ. If vectors are in their original representation, the implementation follows the definition</p>
<div class="math notranslate nohighlight">
\[
d_H = n_{x \neq y}
\]</div>
<p>If vectors are natural binary vectors, then the hamming distance is equivalent to the Manhattan distance and the squared Euclidean distance, also equal to the sum of a XOR element wise operation</p>
<div class="math notranslate nohighlight">
\[
d_H = d_1 = d_2^2
\]</div>
<p>When dealing with one-hot encoded vectors the number of mismatches can be found from the number of matches minus the original dimension, or as the element wise XOR sum over 2</p>
<div class="math notranslate nohighlight">
\[
d_{H,oh} = \vec{x}^T \vec{y} - (|\vec{x}| + |\vec{y}| - \vec{x}^T\vec{y})
= 2\vec{x}^T \vec{y} - |\vec{x}| - |\vec{y}|
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">d_hamming</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">one_hot</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;one_hot&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">one_hot</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="mi">2</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">proximity_matrix</span><span class="p">(</span><span class="n">Xc</span><span class="p">,</span> <span class="n">measure</span><span class="o">=</span><span class="n">d_hamming</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plot_prox</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">mds</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">yc</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/b61af4f3b55dbdf23a4df46af84946c3cd430858.png" /></p>
</div>
</div>
<div class="section" id="dynamic-measures">
<h2>Dynamic measures<a class="headerlink" href="#dynamic-measures" title="Permalink to this headline">¶</a></h2>
<div class="section" id="edit-distance">
<h3>Edit Distance<a class="headerlink" href="#edit-distance" title="Permalink to this headline">¶</a></h3>
<p>The Hamming distance is a particular case of the edit distance between two vectors. The edit distance is typically used for strings, but can also be implemented for categorical vectors. Is defined as the number of operations required to transform one vector or string into another. In the most general case, the edit distance can be calculated for vectors of different length, if either the operation of addition or deletion is defined.</p>
<p>Different types of edit distance allow different sets of string operations <span id="id10">[<a class="reference internal" href="#id60"><span>9</span></a>]</span>. For instance:</p>
<ul class="simple">
<li><p>The Levenshtein distance allows deletion, insertion and substitution.</p></li>
<li><p>The Longest common subsequence (LCS) distance allows only insertion and deletion, not substitution.</p></li>
<li><p>The Hamming distance allows only substitution, hence, it only applies to strings of the same length.</p></li>
<li><p>The Damerau–Levenshtein distance allows insertion, deletion, substitution, and the transposition of two adjacent characters.</p></li>
<li><p>The Jaro distance allows only transposition.</p></li>
</ul>
<p>In Levenshtein’s original definition, each of these operations has unit cost, so the Levenshtein distance is equal to the minimum number of operations required to transform a to b. A more general definition associates non-negative weight functions with the operations.</p>
<p>For example, the Levenshtein distance between “kitten” and “sitting” is 3. A minimal edit script that transforms the former into the latter is:</p>
<ol class="simple">
<li><p>kitten → sitten (substitute “s” for “k”)</p></li>
<li><p>sitten → sittin (substitute “i” for “e”)</p></li>
<li><p>sittin → sitting (insert “g” at the end)</p></li>
</ol>
<p>Edit distance finds applications in computational biology and natural language processing.</p>
<p>A common algorithm that implements the Levenshtein Distance is the Wagner–Fischer algorithm <span id="id11">[<a class="reference internal" href="#id61"><span>10</span></a>]</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">leveshtein_d</span><span class="p">(</span><span class="s1">&#39;sitting&#39;</span><span class="p">,</span> <span class="s1">&#39;kitten&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>3.0
</pre></div>
</div>
</div>
<div class="section" id="dynamic-time-warping-dtw">
<h3>Dynamic Time Warping (DTW)<a class="headerlink" href="#dynamic-time-warping-dtw" title="Permalink to this headline">¶</a></h3>
<p>More typically used for time-series data, DTW allows for portions of sequence vectors to stretch or contract to allow for a better matching among pairs of vectors. In speech recognition, this property enables matching of patterns at different speaking speed.</p>
<p>This stretching is accomplished by allowing many-to-one mapping of vector coordinates. This mapping is equivalent to repeating values in the wrapped sections, and then do a one-to-one mapping as usual, for which any proximity measure can be used, e.g., the <span class="math notranslate nohighlight">\(l_p\)</span> norm.</p>
<p>An optimal wrapping can be found using a dynamic programming approach. For the Manhattan distance, a possible recursive implementation is as follows</p>
<div class="math notranslate nohighlight">
\[
d_1(\vec{x}_{D=i}, \vec{y}_{D=i}) = |x_i - y_i| + d_1(\vec{x}_{D=i-1}, \vec{y}_{D=i-1})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\vec{x}_{D=i}\)</span> is the i-dimensional vector up to the ith coordinate. The idea of DTW it that the indices of the rhs need not be reduced by one, effectively repeating one or more values,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
DTW(\vec{x}_{D=i},\vec{y}_{D=j}) = d(x_i, y_j) + min
\begin{cases}
DTW(\vec{x}_{D=i},\vec{y}_{D=j-1}) &amp; \text{repeat $x_i$}\\
DTW(\vec{x}_{D=i-1},\vec{y}_{D=j}) &amp; \text{repeat $y_j$}\\
DTW(\vec{x}_{D=i-1},\vec{y}_{D=j-1}) &amp; \text{repeat neither}
\end{cases}
\end{align}\end{split}\]</div>
<p>We can fill the <span class="math notranslate nohighlight">\(|x|\times |y|\)</span> matrix recursively, to find all possible distances, then extract the term corresponding to the full vectors.</p>
<p>An implementation using the Manhattan distance is:</p>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>2.0
</pre></div>
</div>
<p>A usual constraint used with DTW is that of locality. This constraint imposes a minimum level <span class="math notranslate nohighlight">\(w\)</span> of positional alignment between matched elements. The DTW will be computed only if <span class="math notranslate nohighlight">\(|i-j|&lt;\leq w\)</span>. See <span id="id12">[<a class="reference internal" href="#id63"><span>2</span></a>]</span> section 3.4.1.3.</p>
</div>
<div class="section" id="longest-common-subsequence-lcss">
<h3>Longest Common Subsequence (LCSS)<a class="headerlink" href="#longest-common-subsequence-lcss" title="Permalink to this headline">¶</a></h3>
<p>We define the LCSS of two sequence vectors as the longest subsequence shared by both vectors. A subsequence is a sub set of possibly discontinuous coordinates in the same order as in the original vector. The LCSS is a similarity function.</p>
<p>For example <span id="id13">[<a class="reference internal" href="#id64"><span>11</span></a>]</span>, consider the sequences (ABCD) and (ACBAD). They have 5 length-2 common subsequences: (AB), (AC), (AD), (BD), and (CD); 2 length-3 common subsequences: (ABD) and (ACD); and no longer common subsequences. So (ABD) and (ACD) are their longest common subsequences.</p>
<p>Similarly as in the previous section, a the LCSS of two string can be build recursively from the LCSS of their prefixes. With the operator ^ signifying string concatenation,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
{\displaystyle {\mathit {LCS}}(X_{i},Y_{j})={\begin{cases}\emptyset &amp;{\mbox{if }}i=0{\mbox{ or }}j=0\\{\mathit {LCS}}(X_{i-1},Y_{j-1}){\hat {}}x_{i}&amp;{\mbox{if }}i,j&gt;0{\mbox{ and }}x_{i}=y_{j}\\\operatorname {\max } \{{\mathit {LCS}}(X_{i},Y_{j-1}),{\mathit {LCS}}(X_{i-1},Y_{j})\}&amp;{\mbox{if }}i,j&gt;0{\mbox{ and }}x_{i}\neq y_{j}.\end{cases}}}
\end{split}\]</div>
<p>To find the LCS of <span class="math notranslate nohighlight">\(X_{i}\)</span> and <span class="math notranslate nohighlight">\(Y_j\)</span>, compare <span class="math notranslate nohighlight">\(x_{i}\)</span> and <span class="math notranslate nohighlight">\(y_{j}\)</span>. If they are equal, then the sequence <span class="math notranslate nohighlight">\({\mathit {LCS}}(X_{i-1},Y_{j-1})\)</span> is extended by that element, <span class="math notranslate nohighlight">\(x_{i}\)</span>. If they are not equal, then the longer of the two sequences, <span class="math notranslate nohighlight">\({\mathit {LCS}}(X_{i},Y_{j-1})\)</span>, and <span class="math notranslate nohighlight">\({\mathit {LCS}}(X_{i-1},Y_{j})\)</span>, is retained. (If they are the same length, but not identical, then both are retained.) Not that even if either <span class="math notranslate nohighlight">\(x_{i}\)</span> or <span class="math notranslate nohighlight">\(y_{i}\)</span> have matched before, and they match again, it does not hurt to move the match to the new trailing elements. This is akin to either skip the previous match, or skip the current match. A worked out example can be found on <span id="id14">[<a class="reference internal" href="#id64"><span>11</span></a>]</span>.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2.0</p></td>
<td><p>((A C) (G A) (G C))</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="mixed-features">
<h2>Mixed features<a class="headerlink" href="#mixed-features" title="Permalink to this headline">¶</a></h2>
<p>If our vectors have both real and categorical attributed, we two different options at our disposal, either transform the vectors to vectors of a single type, or define a measure that treats each set of features differently and then sum the pair of measures with appropriate weights.</p>
<p>The simplest approach it to label encode categorical attributes as integers, and apply a proximity measure for real valued vectors. It is a good idea to standardize or normalize each feature, as to allow a fair comparison between features of different scales.</p>
<p>An alternative to above approach is to discretize all real valued features to obtain a categorical vector, then use some measure for categorical feature vectors.</p>
<p>A more elaborate approach is to partition both vectors into a categorical and real subset of features and deal with each of them independently. Let <span class="math notranslate nohighlight">\(\vec{x} = (\vec{x}_R, \vec{x}_{C})^T\)</span> and <span class="math notranslate nohighlight">\(\vec{y} = (\vec{y}_R, \vec{y}_{C})^T\)</span>, the similarity (distance) between <span class="math notranslate nohighlight">\(\vec{x}\)</span> and <span class="math notranslate nohighlight">\(\vec{y}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
s(\vec{x},\vec{y}) = \lambda s_R(\vec{x}_R,\vec{y}_R) + (1-\lambda)s_C(\vec{x},\vec{y})
\]</div>
<p>where <span class="math notranslate nohighlight">\(s_R\)</span> and <span class="math notranslate nohighlight">\(s_{C}\)</span> are real and categorical similarity measures, respectively, and <span class="math notranslate nohighlight">\(\lambda\)</span> regulates the relative importance of the categorical and numerical attributes. Missing domain knowledge, a good initial choice is to set lambda to the fraction of real features of the data set.</p>
<p>Optionally, instead of standardizing each feature prior to finding the proximity values, we can instead standardize each similarity measure (categorical, real) by its standard deviation, i.e., this is the deviation of the similarities (distances), not the deviations of each feature</p>
<div class="math notranslate nohighlight">
\[
s(\vec{x},\vec{y}) = \frac{\lambda}{\sigma_R} s_R(\vec{x}_R,\vec{y}_R) + \frac{(1-\lambda)}{\sigma_C} s_C(\vec{x},\vec{y}).
\]</div>
<p>An option that uses the one minus the range-normalized Manhattan distance and the Jaccard coefficient as similarity measures is the Gower similarity <span id="id15">[<a class="reference internal" href="#id62"><span>12</span></a>]</span>.</p>
<div class="math notranslate nohighlight">
\[
s_{Gower} = \frac{\sum_{i=1}^{D} s_i}{\sum_{i=1}^{D} w_i}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
w_i =
\begin{cases}
0 &amp; \text{if either $x_i$ or $y_i$ are undefined}\\
0 &amp; \text{if the ith feature is binary and  $x_i = y_i = 0$}\\
1 &amp; \text{otherwise}
\end{cases}
\end{align}\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
s_i =
\begin{cases}
1 &amp; \text{if the ith feature is binary and  $x_i = y_i = 1$}\\
1 &amp; \text{if the ith feature is categorical and  $x_i = y_i$}\\
1 - \frac{|x_i - y_i|}{\text{range}_i} &amp; \text{if the ith feature is real}\\
0 &amp; \text{otherwise}
\end{cases}
\end{align}\end{split}\]</div>
<p>TODO: Implement general function for mixed variables.</p>
<p>TODO: Implement Gower similarity (verify definition in original paper, the weights seems different from the generic weights <span class="math notranslate nohighlight">\(\lambda\)</span>).</p>
</div>
<div class="section" id="fuzzy-measures">
<h2>Fuzzy measures<a class="headerlink" href="#fuzzy-measures" title="Permalink to this headline">¶</a></h2>
<p>Here we consider the case of probability vectors, or vectors with coordinates in the <span class="math notranslate nohighlight">\([0,1]\)</span> range that encode the degree or belief of some characteristic or property. This is a generalization of binary logic, which works with binary vectors, called fuzzy logic, where we deal with uncertain measurements.</p>
<p>In binary logic, two variables x and y are equivalent if the following expression evaluates to 1 (true)</p>
<div class="math notranslate nohighlight">
\[
((NOT\ x)\ AND\ (NOT\ y))\ OR\ (x\ AND\ y)
\]</div>
<p>The AND and the OR operator can be implemented with the <code class="docutils literal notranslate"><span class="pre">min</span></code> and <code class="docutils literal notranslate"><span class="pre">max</span></code> functions, respectively. The not operator may be implemented as <code class="docutils literal notranslate"><span class="pre">1-x</span></code>. This implementations can be extended to fuzzy variables, thus the fuzzy similarity between variables may be defined as</p>
<div class="math notranslate nohighlight">
\[
s_{fzzy} = \max(\min(1-x_i, 1-y_i), \min(x_i, y_i))
\]</div>
<p>For vectors, we can define</p>
<div class="math notranslate nohighlight">
\[
s_{fzzy}^q = \left( \sum_{i=1}^D s_{fzzy}(x_i,y_i)^q  \right)^{\frac{1}{q}}
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">s_fzzy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
    <span class="n">c2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">)</span><span class="o">**</span><span class="n">q</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">q</span><span class="p">)</span>
</pre></div>
</div>
<p>Lets consider the scalar case,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">p_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">x_g</span><span class="p">,</span> <span class="n">y_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">p_list</span><span class="p">,</span> <span class="n">p_list</span><span class="p">)</span>

<span class="n">s_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">s_fzzy</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">xi</span><span class="p">,</span><span class="n">yi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x_g</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y_g</span><span class="o">.</span><span class="n">ravel</span><span class="p">())]</span>
<span class="n">s_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">s_list</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">x_g</span><span class="p">,</span> <span class="n">y_g</span><span class="p">,</span> <span class="n">s_arr</span><span class="p">,</span> <span class="n">shading</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">c</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/0417070ef017c2394b3c399428eff795db989d70.png" /></p>
<p>The similarity of a scalar (vector) with itself is not always 1. It takes into account the associated uncertainty of the true identity of the variable, i.e., the probability that two variables are actually equivalent.</p>
</div>
<div class="section" id="missing-data">
<h2>Missing data<a class="headerlink" href="#missing-data" title="Permalink to this headline">¶</a></h2>
<p>When dealing with missing we have the following options:</p>
<ol>
<li><p>Discard all observations with missing values. Not feasible is a large proportions of observations have missing values.</p></li>
<li><p>Impute missing values.</p></li>
<li><p>Calculate the proximity using only available pair of coordinates, and scale its value to take into account the reduction in magnitude due to missing dimensions. Let <span class="math notranslate nohighlight">\(b\)</span> be the number of unavailable pairs,</p>
<div class="math notranslate nohighlight">
\[
      p(\vec{x},\vec{y}) = \frac{D}{D - b} p(\vec{x}_{i \notin b},\vec{y}_{i \notin b})
      \]</div>
<p>A good measure is the Manhattan distance, since it ensures that the range of the reduced measure is the same as that of the complete measure.</p>
</li>
<li><p>Find the average scalar proximity for all features among the whole data set. Calculate vector proximities as the sum of scalar proximities, if a scalar value is not available for a vector pair, use the average proximity for the dimension.</p>
<div class="math notranslate nohighlight">
\[
      p(\vec{x},\vec{y}) = \sum_{i=1}^{D} \psi(x_i, y_i)
      \]</div>
<p>where <span class="math notranslate nohighlight">\(\psi(x_i, y_i)\)</span> is the scalar proximity if both <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(y_i\)</span> are available, and the average proximity of the ith feature if not.</p>
</li>
</ol>
</div>
<div class="section" id="proximity-between-a-point-and-a-set">
<h2>Proximity between a point and a set<a class="headerlink" href="#proximity-between-a-point-and-a-set" title="Permalink to this headline">¶</a></h2>
<p>In many clustering algorithms, to evaluate whether a point should belong to a cluster, we need to find its distance to such cluster. If the cluster <span class="math notranslate nohighlight">\(C\)</span> is represented by a set of points, then we are asked to find the distance between a point <span class="math notranslate nohighlight">\(\vec{x}\)</span> and a set (cluster) <span class="math notranslate nohighlight">\(\{\vec{y} \in C\}\)</span>.</p>
<p>When we consider the complete set <span class="math notranslate nohighlight">\(C\)</span>, we can define the point-set distance from pairwise distances <span class="math notranslate nohighlight">\(d(\vec{x},\vec{y})\)</span> (or similarities) between <span class="math notranslate nohighlight">\(\vec{x}\)</span> and all elements of <span class="math notranslate nohighlight">\(C\)</span>. For example:</p>
<ul>
<li><p><strong>Max-proximity</strong>:</p>
<div class="math notranslate nohighlight">
\[
     d(\vec{x}, C) = \underset{y \in C}{\operatorname{max}} d(\vec{x},\vec{y})
     \]</div>
</li>
<li><p><strong>Min-proximity</strong>:</p>
<div class="math notranslate nohighlight">
\[
     d(\vec{x}, C) = \underset{y \in C}{\operatorname{min}} d(\vec{x},\vec{y})
     \]</div>
</li>
<li><p><strong>Average-proximity</strong>:</p>
<div class="math notranslate nohighlight">
\[
     d(\vec{x}, C) = \frac{1}{|C|}\sum_{y \in C} d(\vec{x},\vec{y})
     \]</div>
</li>
</ul>
<p>Another, perhaps more efficient option, is to represent each cluster by a prototype point or geometry, and to evaluate the distance to such prototype.</p>
<div class="section" id="point-representatives">
<h3>Point representatives<a class="headerlink" href="#point-representatives" title="Permalink to this headline">¶</a></h3>
<p>For compact clusters, a point may be chosen as a representative. This points can be chosen or calculated in several ways, for example:</p>
<ul>
<li><p><strong>Mean vector</strong>, appropriate for real valued features</p>
<div class="math notranslate nohighlight">
\[
     \vec{\mu} = \frac{1}{|C|}\sum_{y\in C}\vec{y}
     \]</div>
</li>
<li><p><strong>Mean center</strong>. If categorical features are used, and we want the center to be a valid vector of categories, we may choose a point <span class="math notranslate nohighlight">\(\vec{m}_c\)</span> of the cluster as the prototype, by finding the point that satisfies</p>
<div class="math notranslate nohighlight">
\[
     \sum_{y \in C} d(\vec{m_c},\vec{y}) \leq \sum_{y \in C} d(\vec{z},\vec{y}),\quad \forall \vec{z} \in C
     \]</div>
</li>
<li><p><strong>Median center</strong>, appropriate if the distance is not a metric. The median center <span class="math notranslate nohighlight">\(\vec{m}_{med}\)</span> is the point of the cluster that satisfies</p>
<div class="math notranslate nohighlight">
\[
     \text{med}(d(\vec{m_{med}},\vec{y})|\vec{y}\in C)
     \leq \text{med}(d(\vec{z},\vec{y})|\vec{y}\in C),
     \quad \forall \vec{z} \in C
     \]</div>
</li>
</ul>
</div>
<div class="section" id="hyper-plane-representatives">
<h3>Hyper-plane representatives<a class="headerlink" href="#hyper-plane-representatives" title="Permalink to this headline">¶</a></h3>
<p>If the cluster is linear in shape, as in several computer vision applications, we can replace it by a fitting hyperplane, with equation</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^{D} a_jx_j + a_0 = \vec{a}^T\vec{x} + a_0 = 0
\]</div>
<p>The distance from a point <span class="math notranslate nohighlight">\(\vec{x}\)</span> to a hyperplane <span class="math notranslate nohighlight">\(H\)</span> is</p>
<div class="math notranslate nohighlight">
\[
d(\vec{x},H) = \underset{z \in H}{\operatorname{min}} \ d(\vec{x},\vec{z})
\]</div>
<p>which, in the case of Euclidean distance, becomes</p>
<div class="math notranslate nohighlight">
\[
d(\vec{x},H) = \frac{|\vec{a}^T\vec{x} + a_0|}{|\vec{a}|_2}
\]</div>
</div>
<div class="section" id="hyper-sphere-representatives">
<h3>Hyper-sphere representatives<a class="headerlink" href="#hyper-sphere-representatives" title="Permalink to this headline">¶</a></h3>
<p>If the cluster are not compact, but resemble a blob, instead of a point representative, we may choose to use a hyper-sphere <span class="math notranslate nohighlight">\(Q\)</span> with center at <span class="math notranslate nohighlight">\(\vec{c}\)</span> and of radius <span class="math notranslate nohighlight">\(r\)</span>, with general equation</p>
<div class="math notranslate nohighlight">
\[
(\vec{x} - \vec{c})^{T}(\vec{x} - \vec{c})=r^2
\]</div>
<p>with the distance given by</p>
<div class="math notranslate nohighlight">
\[
d(\vec{x},Q) = \underset{z \in Q}{\operatorname{min}} \ d(\vec{x},\vec{z})
\]</div>
<p>where the Euclidean distance is commonly used.</p>
</div>
</div>
<div class="section" id="proximity-among-sets">
<h2>Proximity among sets<a class="headerlink" href="#proximity-among-sets" title="Permalink to this headline">¶</a></h2>
<p>Sometimes we need to quantify the dis(similarity) between sets of observations (for example, in hierarchical clustering). We can extend the definitions of proximity between observations to proximity between sets by considering pairwise measures between the set elements. Then, the proximity is a function <span class="math notranslate nohighlight">\(U\times U \rightarrow \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(U\)</span> is the set of all subsets <span class="math notranslate nohighlight">\(D_i \subset X\)</span>. Note that often the proximity between sets is not a metric, not even a measure, even if the pair-wise proximity function is a metric.</p>
<p>For example, for two sets <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<ul>
<li><p><strong>Max-proximity</strong>. If p is a DM, not a measure. If p is a SM, a measure, but not a metric.</p>
<div class="math notranslate nohighlight">
\[
     p(X, Y) = \underset{x \in X, y \in Y}{\operatorname{max}} p(\vec{x},\vec{y})
     \]</div>
</li>
<li><p><strong>Min-proximity</strong>: If p is a SM, not a measure. If p is a DM, a measure, but not a metric.</p>
<div class="math notranslate nohighlight">
\[
     p(X, Y) = \underset{x \in X, y \in Y}{\operatorname{min}} p(\vec{x},\vec{y})
     \]</div>
</li>
<li><p><strong>Average-proximity</strong>: Never a measure.</p>
<div class="math notranslate nohighlight">
\[
     p(X, Y) =\frac{1}{|X||Y|} \sum_{x \in X}\sum_{y \in Y} p(\vec{x},\vec{y})
     \]</div>
</li>
<li><p><strong>Mean proximity</strong>: A measure if p is a measure. Proximity between point representatives.</p>
<div class="math notranslate nohighlight">
\[
     p(X, Y) = p(\vec{m}_x,\vec{m}_y)
     \]</div>
</li>
<li><p><strong>Ward proximity</strong>:</p>
<div class="math notranslate nohighlight">
\[
     p(X,Y) = \sqrt{\frac{|X||Y|}{|X|+|Y|}}p(\vec{m}_x,\vec{m}_y)
     \]</div>
</li>
</ul>
</div>
<div class="section" id="proximity-of-distributions">
<h2>Proximity of distributions<a class="headerlink" href="#proximity-of-distributions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="kl-divergence">
<h3>KL-divergence<a class="headerlink" href="#kl-divergence" title="Permalink to this headline">¶</a></h3>
<p>From <span id="id16">[<a class="reference internal" href="#id68"><span>13</span></a>]</span>.</p>
<p>The Kullback-Leiber divergence is a measure of similarity between distributions. For two distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>, the KL divergence from <span class="math notranslate nohighlight">\(Q\)</span> to <span class="math notranslate nohighlight">\(P\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
D_{\text{KL}}(P\parallel Q)=\sum _{x\in {\mathcal {X}}}P(x)\log \left({\frac {P(x)}{Q(x)}}\right).
\]</div>
<p>In other words, it is the expectation of the logarithmic difference between the probabilities <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>, where the expectation is taken using the probabilities <span class="math notranslate nohighlight">\(P\)</span>. For continuous distributions, it takes the form</p>
<div class="math notranslate nohighlight">
\[
D_{\text{KL}}(P \parallel Q) = \int_{-\infty}^{\infty } p(x) \log \left( \frac {p(x)}{q(x)} \right)dx
\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span> denote the probability densities of <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>Often, the distribution <span class="math notranslate nohighlight">\(P\)</span> is the data, and the distribution <span class="math notranslate nohighlight">\(Q\)</span> is the model. Then, the Kullback–Leibler divergence is then interpreted as the average difference of the number of bits required for encoding samples of <span class="math notranslate nohighlight">\(P\)</span> using a code optimized for <span class="math notranslate nohighlight">\(Q\)</span> rather than one optimized for <span class="math notranslate nohighlight">\(P\)</span>. In the context of machine learning, <span class="math notranslate nohighlight">\(D_{\text{KL}}(P\parallel Q)\)</span> is often called the information gain achieved if <span class="math notranslate nohighlight">\(P\)</span> would be used instead of <span class="math notranslate nohighlight">\(Q\)</span> which is currently used. In other words, it is the amount of information lost when <span class="math notranslate nohighlight">\(Q\)</span> is used to approximate <span class="math notranslate nohighlight">\(P\)</span>. In order to find a distribution <span class="math notranslate nohighlight">\(Q\)</span> that is closest to <span class="math notranslate nohighlight">\(P\)</span>, we can minimize KL divergence and compute an information projection (see t-SNE later on).</p>
<p>The KL divergence is not symmetric (<span class="math notranslate nohighlight">\(D_{\text{KL}}(P\parallel Q) \neq D_{\text{KL}}(Q\parallel P)\)</span>), nor satisfies the triangle inequality, and is, thus, not a metric.</p>
<p>The following properties hold, among others <span id="id17">[<a class="reference internal" href="#id68"><span>13</span></a>]</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D_{\text{KL}}(P\parallel Q)\geq 0\)</span> (follows from the Gibb’s inequality) (Prove this.)</p></li>
<li><p><span class="math notranslate nohighlight">\(D_{\text{KL}}(P\parallel P) = 0\)</span></p></li>
</ul>
<p>Mutual information is just the KL divergence of the joint distribution to the product of the marginal distributions, and thus, measures independence in some sense.</p>
</div>
<div class="section" id="jensen-shannon-distance">
<h3>Jensen-Shannon distance<a class="headerlink" href="#jensen-shannon-distance" title="Permalink to this headline">¶</a></h3>
<p>From <span id="id18">[<a class="reference internal" href="#id69"><span>14</span></a>]</span>.</p>
<p>One can derive a proper metric from the KL divergence, this being the square root of the Jensen-Shannon distance, defined as</p>
<div class="math notranslate nohighlight">
\[
{{\rm {JSD}}}(P\parallel Q)={\frac  {1}{2}}D(P\parallel M)+{\frac  {1}{2}}D(Q\parallel M)
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
M={\frac  {1}{2}}(P+Q)
\]</div>
<p>The Jensen–Shannon divergence is a method of measuring the similarity between two probability distributions. If using base logarithm, it is bounded by <span class="math notranslate nohighlight">\(0\leq {{\rm {JSD}}}(P\parallel Q)\leq 1\)</span>. For log base e, or ln, the upper bound is ln(2).</p>
<p>The Jensen–Shannon divergence has been applied in bioinformatics and genome comparison, in protein surface comparison, in the social sciences, in the quantitative study of history, fire experiments and in machine learning (GANs). See <span id="id19">[<a class="reference internal" href="#id69"><span>14</span></a>]</span> for references.</p>
</div>
</div>
<div class="section" id="proximity-measures-on-graps">
<h2>Proximity measures on graps<a class="headerlink" href="#proximity-measures-on-graps" title="Permalink to this headline">¶</a></h2>
<div class="section" id="similarity-between-nodes-in-a-graph">
<h3>Similarity between nodes in a graph<a class="headerlink" href="#similarity-between-nodes-in-a-graph" title="Permalink to this headline">¶</a></h3>
<p>Consider a graph <span class="math notranslate nohighlight">\(G=(N,E)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is a set of nodes, and <span class="math notranslate nohighlight">\(E\)</span> is a set of edges. Two nodes in <span class="math notranslate nohighlight">\(G\)</span> are considered close is they are “connected”. There are several ways to measure this “connectedness”.</p>
<div class="section" id="structural-distance">
<h4>Structural distance<a class="headerlink" href="#structural-distance" title="Permalink to this headline">¶</a></h4>
<p>For unweighted graphs, the <strong>geodesic distance</strong> between two nodes is the number of edges of the shortest path connecting them. For weighted graphs, the sum of the weights of the edges belonging to the shortest path are added to find the distance.</p>
<p>The shortest path between a source node <span class="math notranslate nohighlight">\(s\)</span> and a node <span class="math notranslate nohighlight">\(j\)</span> can be found using the Dijkstra’s algorithm <span id="id20">[<a class="reference internal" href="#id65"><span>15</span></a>]</span>. The essence of this algorithm is the following:</p>
<ol>
<li><p>Initialize all distance <span class="math notranslate nohighlight">\(SP(i,j) = \infty\)</span>.</p></li>
<li><p>Start at node <span class="math notranslate nohighlight">\(s\)</span> and set <span class="math notranslate nohighlight">\(SP(s,s) = 0\)</span>, do not consider <span class="math notranslate nohighlight">\(s\)</span> examined so far.</p></li>
<li><p>For all un-examined nodes:</p>
<ol>
<li><p>Choose the one with minimum <span class="math notranslate nohighlight">\(SP(s,i)\)</span>.</p></li>
<li><p>For each neighbor <span class="math notranslate nohighlight">\(j\)</span> of node <span class="math notranslate nohighlight">\(i\)</span>:</p>
<div class="math notranslate nohighlight">
\[
          SP(s,j) = min(SP(s,j), SP(s,i) + w_{ij})
          \]</div>
<p>Until all nodes are examined.</p>
</li>
<li><p>Set node <span class="math notranslate nohighlight">\(i\)</span> as examined.</p></li>
</ol>
</li>
</ol>
<p>Note that step 3.2 does not sets neighbors as examined, <span class="math notranslate nohighlight">\(SP(s,i)\)</span> can be updated more than once, until examined directly, keeping the minimum path so far at each step.</p>
</div>
<div class="section" id="random-walk-based-similarity-pagerank">
<h4>Random Walk-based similarity (PageRank)<a class="headerlink" href="#random-walk-based-similarity-pagerank" title="Permalink to this headline">¶</a></h4>
<p>Since shortest path algorithms only considers single path between nodes, as a proximity function, it ignores the fact that nodes connected by many paths should be considered closer in some applications.</p>
<p>One way to take into account the multiplicity of paths is to use a restarting random walk. Start at the source node <span class="math notranslate nohighlight">\(s\)</span>, and move through the graph choosing an edge with probability proportional to decreasing function of its weight. Consider also the probability to restart the random walk at <span class="math notranslate nohighlight">\(s\)</span>. Nodes more connected to <span class="math notranslate nohighlight">\(s\)</span> will be visited more often. A possible implementation of this idea is the PageRank algorithm <span id="id21">[<a class="reference internal" href="#id66"><span>16</span></a>]</span>.</p>
</div>
</div>
<div class="section" id="similarity-between-graphs">
<h3>Similarity between graphs<a class="headerlink" href="#similarity-between-graphs" title="Permalink to this headline">¶</a></h3>
<p>This is hard problem (It is NP-hard to match complete graphs.) The following ideas have been proposed:</p>
<ul class="simple">
<li><p>Maximum common subgraph distance. If two graphs share a large subgraph, they are considered more similar.</p></li>
<li><p>Substructure-based similarity: Count frequent substructures between two graphs.</p></li>
<li><p>Graph-edit distance: Number of edits required to transform a graph into another.</p></li>
<li><p>Graph kernels: Kernel functions defined to measure similarity between graphs, such as the shortest path kernel and the random-walk kernel.</p></li>
</ul>
<p>For more information, see <span id="id22">[<a class="reference internal" href="#id63"><span>2</span></a>]</span>, Chapter 17.</p>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="id23"><dl class="citation">
<dt class="label" id="id53"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Sergios Theodoridis and Konstantinos Koutroumbas. <em>Pattern Recognition</em>. Elsevier, 2009. URL: <a class="reference external" href="https://doi.org/10.1016/b978-1-59749-272-0.x0001-2">https://doi.org/10.1016/b978-1-59749-272-0.x0001-2</a>, <a class="reference external" href="https://doi.org/10.1016/b978-1-59749-272-0.x0001-2">doi:10.1016/b978-1-59749-272-0.x0001-2</a>.</p>
</dd>
<dt class="label" id="id63"><span class="brackets">2</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id4">2</a>,<a href="#id12">3</a>,<a href="#id22">4</a>)</span></dt>
<dd><p>Charu C Aggarwal. <em>Data mining: the textbook</em>. Springer, 2015.</p>
</dd>
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Jan Schulz. Canberra distance. 2007. [Online; accessed 12-April-2021]. URL: <a class="reference external" href="http://www.code10.info/index.php?option=com_content&amp;view=article&amp;id=49:article_canberra-distance&amp;catid=38:cat_coding_algorithms_data-similarity&amp;Itemid=57">http://www.code10.info/index.php?option=com_content&amp;view=article&amp;id=49:article_canberra-distance&amp;catid=38:cat_coding_algorithms_data-similarity&amp;Itemid=57</a>.</p>
</dd>
<dt class="label" id="id55"><span class="brackets">4</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id9">2</a>)</span></dt>
<dd><p>Wikipedia contributors. Cosine similarity — Wikipedia, the free encyclopedia. 2021. [Online; accessed 13-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Cosine_similarity&amp;oldid=1017252556">https://en.wikipedia.org/w/index.php?title=Cosine_similarity&amp;oldid=1017252556</a>.</p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Wikipedia contributors. Spearman's rank correlation coefficient — Wikipedia, the free encyclopedia. 2021. [Online; accessed 15-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Spearman%27s_rank_correlation_coefficient&amp;oldid=1013762125">https://en.wikipedia.org/w/index.php?title=Spearman%27s_rank_correlation_coefficient&amp;oldid=1013762125</a>.</p>
</dd>
<dt class="label" id="id58"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Wikipedia contributors. Simple matching coefficient — Wikipedia, the free encyclopedia. 2021. [Online; accessed 16-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Simple_matching_coefficient&amp;oldid=1007400537">https://en.wikipedia.org/w/index.php?title=Simple_matching_coefficient&amp;oldid=1007400537</a>.</p>
</dd>
<dt class="label" id="id59"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>Wikipedia contributors. Chemical similarity — Wikipedia, the free encyclopedia. 2021. [Online; accessed 18-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Chemical_similarity&amp;oldid=1016793498">https://en.wikipedia.org/w/index.php?title=Chemical_similarity&amp;oldid=1016793498</a>.</p>
</dd>
<dt class="label" id="id67"><span class="brackets"><a class="fn-backref" href="#id8">8</a></span></dt>
<dd><p>Wikipedia contributors. Tversky index — Wikipedia, the free encyclopedia. 2021. [Online; accessed 21-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Tversky_index&amp;oldid=1007400550">https://en.wikipedia.org/w/index.php?title=Tversky_index&amp;oldid=1007400550</a>.</p>
</dd>
<dt class="label" id="id60"><span class="brackets"><a class="fn-backref" href="#id10">9</a></span></dt>
<dd><p>Wikipedia contributors. Edit distance — Wikipedia, the free encyclopedia. 2021. [Online; accessed 18-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Edit_distance&amp;oldid=1013329630">https://en.wikipedia.org/w/index.php?title=Edit_distance&amp;oldid=1013329630</a>.</p>
</dd>
<dt class="label" id="id61"><span class="brackets"><a class="fn-backref" href="#id11">10</a></span></dt>
<dd><p>Wikipedia contributors. Wagner-fischer algorithm — Wikipedia, the free encyclopedia. 2021. [Online; accessed 18-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/wiki/Wagner-Fischer_algorithm">https://en.wikipedia.org/wiki/Wagner-Fischer_algorithm</a>.</p>
</dd>
<dt class="label" id="id64"><span class="brackets">11</span><span class="fn-backref">(<a href="#id13">1</a>,<a href="#id14">2</a>)</span></dt>
<dd><p>Wikipedia contributors. Longest common subsequence problem — Wikipedia, the free encyclopedia. 2021. [Online; accessed 20-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Longest_common_subsequence_problem&amp;oldid=1018929068">https://en.wikipedia.org/w/index.php?title=Longest_common_subsequence_problem&amp;oldid=1018929068</a>.</p>
</dd>
<dt class="label" id="id62"><span class="brackets"><a class="fn-backref" href="#id15">12</a></span></dt>
<dd><p>John C Gower. A general coefficient of similarity and some of its properties. <em>Biometrics</em>, pages 857–871, 1971.</p>
</dd>
<dt class="label" id="id68"><span class="brackets">13</span><span class="fn-backref">(<a href="#id16">1</a>,<a href="#id17">2</a>)</span></dt>
<dd><p>Wikipedia contributors. Kullback-leibler divergence — Wikipedia, the free encyclopedia. 2021. [Online; accessed 21-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Kullback%E2%80%93Leibler_divergence&amp;oldid=1013473911">https://en.wikipedia.org/w/index.php?title=Kullback%E2%80%93Leibler_divergence&amp;oldid=1013473911</a>.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">14</span><span class="fn-backref">(<a href="#id18">1</a>,<a href="#id19">2</a>)</span></dt>
<dd><p>Wikipedia contributors. Jensen-shannon divergence — Wikipedia, the free encyclopedia. 2021. [Online; accessed 21-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Jensen%E2%80%93Shannon_divergence&amp;oldid=1018225207">https://en.wikipedia.org/w/index.php?title=Jensen%E2%80%93Shannon_divergence&amp;oldid=1018225207</a>.</p>
</dd>
<dt class="label" id="id65"><span class="brackets"><a class="fn-backref" href="#id20">15</a></span></dt>
<dd><p>Wikipedia contributors. Dijkstra's algorithm — Wikipedia, the free encyclopedia. 2021. [Online; accessed 21-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Dijkstra%27s_algorithm&amp;oldid=1014900072">https://en.wikipedia.org/w/index.php?title=Dijkstra%27s_algorithm&amp;oldid=1014900072</a>.</p>
</dd>
<dt class="label" id="id66"><span class="brackets"><a class="fn-backref" href="#id21">16</a></span></dt>
<dd><p>Wikipedia contributors. Pagerank — Wikipedia, the free encyclopedia. 2021. [Online; accessed 21-April-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=PageRank&amp;oldid=1018352503">https://en.wikipedia.org/w/index.php?title=PageRank&amp;oldid=1018352503</a>.</p>
</dd>
</dl>
</p>
<ul class="simple">
<li><p>TODO: Explore effect of centering and scaling on the metrics.</p></li>
<li><p>TODO: Look for example applications of key metrics.</p></li>
</ul>
<div class="toctree-wrapper compound">
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="assignments-dummy/U1-M1-L3-prep-outlier-detection.html" title="previous page">Assignment: Anomaly and Outlier Detection</a>
    <a class='right-next' id="next-link" href="assignments-dummy/U2-M1-L1-similarity_metrics.html" title="next page"><strong>Warning</strong>:</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Gonzalo G. Peraza Mues<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>