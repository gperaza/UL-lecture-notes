
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Some linear and non-linear variants of PCA &#8212; Lecture Notes in Unsupervised and Reinforcement Learning</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Other Linear Methods for DR" href="U2-M2-L3-Other_Linear_Methods_for_DR.html" />
    <link rel="prev" title="Principal Component Analysis" href="assignments-dummy/U2-M2-L1-dim_red_PCA.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/UPY-completo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Lecture Notes in Unsupervised and Reinforcement Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome!
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 1 - Unsupervised Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L1-prep-normalization.html">
   Data Normalization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L1-prep-normalization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L2-prep-discretization.html">
   Discretization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L2-prep-discretization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L3-unsup-outlier-detection.html">
   Anomaly and Outlier Detection
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L3-prep-outlier-detection.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 2 - Dimensionality Reduction
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U2-M1-L1-similarity_metrics.html">
   (Dis)similarity metrics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html">
     <strong>
      Warning
     </strong>
     :
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-1-2-pt">
     Exercise 1 (2 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-2-3-pt">
     Exercise 2 (3 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-3-4-pts">
     Exercise 3 (4 pts)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-4-1-pt">
     Exercise 4 (1 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-5-ungraded">
     Exercise 5 (ungraded)
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U2-M2-L1-dim_red_PCA.html">
   Dimensionality Reduction and PCA
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M2-L1-dim_red_PCA.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Some linear and non-linear variants of PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M2-L3-Other_Linear_Methods_for_DR.html">
   Other Linear Methods for DR
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="freedman-diaconis.html">
   Freedman-Diaconis Rule
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/U2-M2-L2-PCA_variants.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/gperaza/UL-lecture-notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/gperaza/UL-lecture-notes/issues/new?title=Issue%20on%20page%20%2FU2-M2-L2-PCA_variants.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sparse-pca">
   Sparse PCA
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-faces">
     Example: Faces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-sparse-news-data">
     Example: Sparse news data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#robust-pca">
   Robust PCA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-pca">
   Kernel PCA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="some-linear-and-non-linear-variants-of-pca">
<h1>Some linear and non-linear variants of PCA<a class="headerlink" href="#some-linear-and-non-linear-variants-of-pca" title="Permalink to this headline">¶</a></h1>
<div class="section" id="sparse-pca">
<h2>Sparse PCA<a class="headerlink" href="#sparse-pca" title="Permalink to this headline">¶</a></h2>
<p>One disadvantage of PCA is that the PC are linear combinations of all original variables. This hurts interpretability. An variant that aims to recover new features as limited linear combinations of original features, and aid interpretability of the new components and loadings, is sparse PCA. There are many variants of sparse PCA, but all share the property that the new basis vectors are sparse, i.e., contain many zero entries. A recent overview of Sparse PCA variants can be found in <span id="id1">[<a class="reference internal" href="#id71"><span>1</span></a>]</span>, where the optimization algorithms the different variants are also discussed, including the early methods of SCoTLASS <span id="id2">[<a class="reference internal" href="#id76"><span>2</span></a>]</span> and SPCA <span id="id3">[<a class="reference internal" href="#id75"><span>3</span></a>]</span>. In particular, note that approaches of variance maximization and reconstruction error minimization, although equivalent for PCA, are not equivalent for sparse PCA.</p>
<p>A particular approach that shares many characteristics with other methods in this course is called dictionary learning. We will now introduce sparse dictionary learning, but keep in mind that other approaches exist, see <span id="id4">[<a class="reference internal" href="#id71"><span>1</span></a>]</span> and references therein.</p>
<p>Within the dictionary learning framework, the PCA loadings (base vectors) are called a dictionary, and each base vector is called and atom. The principal components are called scores. The PCA algorithm learns a dictionary and scores the solve a particular optimization problem, minimizing the reconstruction error of a low rank representation of the data set. We can code the PCA objective within the dictionary learning framework as follows</p>
<div class="math notranslate nohighlight">
\[
\underset{Y\in \mathbf{R}^{n \times q}, D \in \mathbf{R}^{d \times q}}{\operatorname{argmin}}
\left| X - Y D^{T}  \right |_2^2
,\quad DD^T = 1,
\]</div>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the dictionary, consisting of <span class="math notranslate nohighlight">\(q\)</span> column unit vectors, <span class="math notranslate nohighlight">\(Y\)</span> is the score matrix, containing the coefficients of the dictionary columns. The matrix <span class="math notranslate nohighlight">\(YD^T\)</span> is a low rank reconstruction of the matrix <span class="math notranslate nohighlight">\(X\)</span>. The condition on <span class="math notranslate nohighlight">\(D\)</span> is required to avoid trivial solutions with arbitrarily large entries in <span class="math notranslate nohighlight">\(D\)</span>, and consequently small entries in <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>It can be shown that, for <span class="math notranslate nohighlight">\(q\leq d\)</span>, the solution that minimizes the Frobenius norm of the difference between the original matrix and its low rank reconstruction is obtained from the SVD of X, by keeping only the first <span class="math notranslate nohighlight">\(q\)</span> singular vectors, i.e., leads to the PCA solution. This is not surprising, as the Forbenius norm condition is another way to write the least square reconstruction error. Still, the dictionary learning framework is more general, since we can choose any value for dimension <span class="math notranslate nohighlight">\(q\)</span>, allowing for overcomplete dictionaries (<span class="math notranslate nohighlight">\(q &gt; d\)</span>), useful in some computer vision applications.</p>
<p>To enforce sparse entries in the solution, we just a LASSO penalty to the objective function</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\underset{Y\in \mathbf{R}^{n \times q}, D \in \mathbf{R}^{d \times q}}{\operatorname{argmin}}
\frac{1}{2}\left| X - Y D^{T}  \right |_2^2
- \lambda_D \left| D \right|_1
- \lambda_Y \left | Y \right |_1
\end{align}\]</div>
<p>where the <span class="math notranslate nohighlight">\(l_{1}\)</span> norm is taken for the sum of all entries of each matrix. We distinguish two important cases of the penalized problem:</p>
<ol class="simple">
<li><p>Sparse PCA, <span class="math notranslate nohighlight">\(\lambda_{Y} = 0\)</span>, and sparsity is only enforced in the dictionary. A normalization must be enforced in <span class="math notranslate nohighlight">\(Y\)</span>, <span class="math notranslate nohighlight">\(|Y|_2 &lt; Const\)</span> for example.</p></li>
<li><p>Sparse dictionary learning, <span class="math notranslate nohighlight">\(\lambda_{D} = 0\)</span>, and sparsity is only enforced in the coefficients. A normalization must be enforced in <span class="math notranslate nohighlight">\(D\)</span>, <span class="math notranslate nohighlight">\(|D|_2 &lt; Const\)</span> for example.</p></li>
</ol>
<p>Let’s focus on sparse PCA, sparse dictionary learning being similar. The question is now how to optimize</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\underset{Y\in \mathbf{R}^{n \times q}, D \in \mathbf{R}^{d \times q}}{\operatorname{argmin}}
\frac{1}{2}\left| X - Y D^{T}  \right |_2^2
- \lambda_D \left| D \right|_1
\end{align}\]</div>
<p>Many methods have been proposed, reviews can be found in <span id="id5">[<a class="reference internal" href="#id74"><span>4</span></a>]</span> and <span id="id6">[<a class="reference internal" href="#id77"><span>5</span></a>]</span>. We will discuss an alternating coordinate descent approach, which, although not the fastest, has proven to be effective in practice and relative easy to implement.</p>
<p>The objective function is not convex jointly in both <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, making it hard to optimize jointly. An popular option, that does not guarantees a global minimum, is to alternate optimization with respect to <span class="math notranslate nohighlight">\(D\)</span>, keeping <span class="math notranslate nohighlight">\(Y\)</span> fixed, and optimizing with respect to <span class="math notranslate nohighlight">\(Y\)</span>, keeping <span class="math notranslate nohighlight">\(D\)</span> fixed, repeating both steps until convergence. Both sub-problems are convex, thus solvable by coordinate descent.</p>
<p>Let’s focus first on finding the optimal solution for <span class="math notranslate nohighlight">\(D\)</span>, keeping <span class="math notranslate nohighlight">\(Y\)</span> fixed. The optimization problem can be split into independent sub-problems for the rows of <span class="math notranslate nohighlight">\(D\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{align}
\frac{1}{2}\left| X - Y D^{T}  \right |_2^2
- \lambda \left| D \right|_1
= \sum_{i=1}^q\left[\frac{1}{2} \left| X_{:,i} - Y D_{i}  \right |_2^2
- \lambda \left| D_i \right|_1 \right]
\end{align}\]</div>
<p>and each individual objective inside the summation is in the form of a standard LASSO regression (<span class="math notranslate nohighlight">\(|y-X\beta|_2^2 - \lambda|\beta|_1\)</span>), which can be optimized using sub-derivatives. Differentiating with respect to <span class="math notranslate nohighlight">\(D_{ij}\)</span> we obtain the Karush-Kuhn-Tucker (KKT) conditions,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
0 \in [-(X_{:,i} - Y D_i)^T Y_{:,j} - \lambda, -(X_{:,i} - Y D_i)^T Y_{:,j} + \lambda] \quad D_{ij} = 0\\
-(X_{:,i} - Y D_i)^T Y_{:,j} + \lambda\operatorname{sgn}(D_{ij}) = 0 \quad D_{ij} \neq 0
\end{align}\end{split}\]</div>
<p>or,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\left| (X_{:,i} - Y D_i)^T Y_{:,j}\right| \leq \lambda \quad D_{ij} = 0\\
(X_{:,i} - Y D_i)^T Y_{:,j} = \lambda\operatorname{sgn}(D_{ij}) \quad D_{ij} \neq 0
\end{align}\end{split}\]</div>
<p>Now, coordinate descent acts by optimizing along one coordinate at a time in an iterative manner until convergence. We will minimize for a single <span class="math notranslate nohighlight">\(D_{ij}\)</span>, while keeping all other variables. To simplify notation for the sub-problem, well use the definitions <span class="math notranslate nohighlight">\(X_{:,i} = x\)</span> and <span class="math notranslate nohighlight">\(D_i = d\)</span>, do the objective, row wise, becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\frac{1}{2}\left| x - Yd  \right|_2^2 - \lambda|d|_1
=&amp;\frac{1}{2}\sum_{i=1}^n\left(x_i - \sum_{j=1}^d Y_{ij} d_j   \right)^2 - \lambda\sum_{j=1}^d |d_j|\\
=&amp;\frac{1}{2}\sum_{i=1}^n\left(x_i - \sum_{j \neq k} Y_{ij} d_j - Y_{ik}d_k   \right)^2 - \lambda|d_k| - \lambda\sum_{j\neq k} |d_j|\\
=&amp;\frac{1}{2}\sum_{i=1}^n\left(r_{ik}  - Y_{ik} d_k   \right)^2 - \lambda|d_k| - \lambda\sum_{j\neq k} |d_j|\\
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(r_{ik}=x_i - \sum_{j\neq k} Y_{ij}d_j\)</span> is the partial residual with respect <span class="math notranslate nohighlight">\(d_k\)</span>. Differentiating with respect to <span class="math notranslate nohighlight">\(D_{ik} = d_k\)</span>, we obtain the KTT conditions for the single variable problem</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\left|\sum_{i=1}^n\left(r_{ik}  - Y_{ik} d_k   \right)Y_{ik}\right| \leq \lambda \quad d_{k} = 0\\
\sum_{i=1}^n\left(r_{ik}  - Y_{ik} d_k   \right)Y_{ik} = \lambda\operatorname{sgn}(d_k) \quad d_{k} \neq 0
\end{align}\end{split}\]</div>
<p>which, after some algebra, lead to the solutions</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
d_k =
\begin{cases}
\frac{z_{k} - \lambda}{|Y_{:,k}|_2^2} &amp; z_{k} &gt; \lambda\\
0 &amp; |z_{k}| \leq \lambda\\
\frac{z_{k} + \lambda}{|Y_{:,k}|_2^2} &amp; z_{k} &lt; -\lambda
\end{cases}
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{k}= \sum_{i} r_{ik}Y{ik}\)</span>. The solution can be written in terms of soft-thresholding function,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
S_{\lambda}(x) =
\begin{cases}
x - \lambda &amp; x &gt; \lambda\\
0 &amp; |x| \leq \lambda\\
x + \lambda &amp; x &lt; -\lambda
\end{cases}
\end{align}\end{split}\]</div>
<p>as</p>
<div class="math notranslate nohighlight">
\[
d_k = S_{\frac{\lambda}{|Y_{:,k}|_2^2}}\left(\frac{z_{ik}}{|Y_{:,k}|_2^2}\right)
\]</div>
<p>Going back to the original representation, we obtain for row <span class="math notranslate nohighlight">\(l\)</span>,</p>
<div class="math notranslate nohighlight">
\[
D_{lk} = S_{\frac{\lambda}{|Y_{:,k}|_2^2}}\left(\frac{z^{(l)}_{ik}}{|Y_{:,k}|_2^2}\right)
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
z^{(l)}_{ik} =&amp; \sum_i r^{(l)}_{ik} Y_{ik} \\
=&amp; \sum_i
       \left(
         X_{il} - \sum_j Y_{ij}D_{lj} + Y_{ik}D_{lk}
       \right) Y_{ik} \\
=&amp; \sum_i \left(
            X_{il} - \sum_j Y_{ij}D_{lj}
          \right) Y_{ik}
     + \sum_i Y_{ik}^2 D_{lk}\\
=&amp; \sum_i \left(
            X_{il} - Y_{i:}^T D_{l:}
          \right) Y_{ik}
     + \sum_i Y_{ik}^2 D_{lk}\\
=&amp; \left( X_{:l} - Y D_{l:}
          \right)^T Y_{:k}
     + \sum_i Y_{ik}^2 D_{lk}
\end{align}\end{split}\]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[
D_{lk} = S_{\frac{\lambda}{|Y_{:,k}|_2^2}}
\left( D_{lk} +  \frac{\left( X_{:l} - Y D_{l:} \right)^T Y_{:k}}
{|Y_{:,k}|_2^2}\right)
\]</div>
<p>where the <span class="math notranslate nohighlight">\(D_{lk}\)</span> on the right hand side is the value before the update. The reason for the rearrangement is that we want to define a block coordinate descent algorithm over the columns of <span class="math notranslate nohighlight">\(D\)</span>. Notice that the update only depends on values on each row <span class="math notranslate nohighlight">\(l\)</span>, for <span class="math notranslate nohighlight">\(D\)</span>. This means we can <span class="math notranslate nohighlight">\(D_{kl}\)</span> for all rows <span class="math notranslate nohighlight">\(l\)</span> simultaneously, this is called block coordinate descent, with columns as blocks.</p>
<div class="math notranslate nohighlight">
\[
D_{:k} = S_{\frac{\lambda}{|Y_{:,k}|_2^2}}
\left( D_{:k} +  \frac{\left( X^T - DY^T \right) Y_{:k}}
{|Y_{:,k}|_2^2}\right)
\]</div>
<p>where the soft-thresholding function is applied element-wise to the vector. To optimize with respect to <span class="math notranslate nohighlight">\(D\)</span>, we need to iteratively update the columns of <span class="math notranslate nohighlight">\(D\)</span> until convergence. Notice that pre-computing matrices <span class="math notranslate nohighlight">\(X^T Y\)</span> and <span class="math notranslate nohighlight">\(Y^T Y\)</span> allows to save a lot of repeated calculations. We leave the implementation for the assignments.</p>
<p>Now, we move forward to the optimization step with respect to <span class="math notranslate nohighlight">\(Y\)</span>. To optimize <span class="math notranslate nohighlight">\(Y\)</span>, we will employ a block coordinate descent algorithm, and enforce the restriction that the <span class="math notranslate nohighlight">\(l_2\)</span> norm of each column of <span class="math notranslate nohighlight">\(Y\)</span> is equal or less than 1. The solution is very similar to the solution for <span class="math notranslate nohighlight">\(D\)</span>, now splitting the objective function into rows of <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>Take the objective function</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\frac{1}{2}\left| X - Y D^{T}  \right |_2^2
- \lambda \left| D \right|_1
= \sum_{i=1}^q\left[\frac{1}{2} \left| X_{i:} - D Y_i  \right |_2^2  \right]
- \lambda \left| D \right|_1
\end{align}\]</div>
<p>Realizing that the problems decomposes row-wise on <span class="math notranslate nohighlight">\(Y\)</span>, we can derive the whole objective by the column, akin to taking each row and deriving with respect a single coordinate. Since derivating with respect to <span class="math notranslate nohighlight">\(Y\)</span> will zero out the regularization term, we ignore it from now on. We can expand the Frobenius norm,</p>
<div class="math notranslate nohighlight">
\[\begin{align}
L = \frac{1}{2}\left| X - Y D^{T}  \right |_2^2
 =&amp; \frac{1}{2}\sum_{ij}\left(X_{ij} - \sum_m Y_{im}D_{jm}\right)^{2}
\end{align}\]</div>
<p>Taking the derivative</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
0 = \frac{\partial L}{\partial Y_{lk}}
=&amp; \sum_{ij}\left(X_{ij} - \sum_m Y_{im}D_{jm}\right)D_{jk}\delta_{il}\\
=&amp; \sum_{j}\left(X_{lj} - \sum_m Y_{lm}D_{jm}\right)D_{jk}\\
=&amp; \sum_{j}\left(X_{lj} - \sum_{m \neq k} Y_{lm}D_{jm} - Y_{lk}D_{jk} \right)D_{jk}\\
=&amp; \sum_{j}\left(X_{lj} - \sum_{m \neq k} Y_{lm}D_{jm}\right)D_{jk} - \left(\sum_{j}D_{jk}^2 \right)Y_{lk}
\end{align}\end{split}\]</div>
<p>Solving</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
Y_{lk} =&amp; \frac{\sum_{j}\left(X_{lj} - \sum_{m \neq k} Y_{lm}D_{jm}\right)D_{jk}}{\sum_{j}D_{jk}^2}\\
=&amp; \frac{\sum_{j}\left(X_{lj} - \sum_{m} Y_{lm}D_{jm}\right)D_{jk}}{\sum_{j}D_{jk}^2}
+ \frac{Y_{lk} \sum_{j}D_{jk}^2}{\sum_{j}D_{jk}^2}\\
=&amp; Y_{lk} + \frac{\sum_{j}\left(X_{lj} -  Y_{l}^TD_{j} \right) D_{jk}} {|D_{:k}|^2}\\
=&amp; Y_{lk} + \frac{\left(X_{l} -  D Y_l\right)^T D_{:k}} {|D_{:k}|^2}
\end{align}\end{split}\]</div>
<p>Finally, for the whole column</p>
<div class="math notranslate nohighlight">
\[\begin{align}
Y_{:k} = Y_{:k} + \frac{\left(X -  YD^{T}\right) D_{:k}} {|D_{:k}|^2}
\end{align}\]</div>
<p>Again note that pre-computing <span class="math notranslate nohighlight">\(XD\)</span> and <span class="math notranslate nohighlight">\(D^TD\)</span> allows to avoid repeated calculations.</p>
<p>We still need to enforce the normalization on columns of <span class="math notranslate nohighlight">\(Y\)</span>. It can be shown that re-projecting back to the unit ball is enough to ensure convergence.</p>
<div class="math notranslate nohighlight">
\[\begin{align}
Y_{:k} = \frac{Y_{:k}}{\max(1,|Y_{:k}|_2)}
\end{align}\]</div>
<p>The algorithm then repeats optimizing each column iteratively until convergence. Again, we leave the implementation for the assignments.</p>
<p>Further restrictions can be imposed, for example, to enforce structure on the sparse loadings <span id="id7">[<a class="reference internal" href="#id72"><span>6</span></a>]</span>. Other applications for SPCA include image de-noising by compressed sensing.</p>
<p>We can explore how sparse PCA works using the implementation from sklearn, which implements a different optimization algorithm (<span id="id8">[<a class="reference internal" href="#id73"><span>7</span></a>]</span>), more efficient, but also harder to implement and less general.</p>
<div class="section" id="example-faces">
<h3>Example: Faces<a class="headerlink" href="#example-faces" title="Permalink to this headline">¶</a></h3>
<p>We take the faces example from the scikit-learn documentation. You have already explored faces principal components using standard PCA. Now, let’s take a look at the components using sparse PCA.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Code from</span>
<span class="c1">#https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_olivetti_faces</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">MiniBatchSparsePCA</span>

<span class="n">n_row</span><span class="p">,</span> <span class="n">n_col</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span>
<span class="n">n_components</span> <span class="o">=</span> <span class="n">n_row</span> <span class="o">*</span> <span class="n">n_col</span>
<span class="n">image_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

<span class="n">faces</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">fetch_olivetti_faces</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">faces</span><span class="o">.</span><span class="n">shape</span>

<span class="n">faces_centered</span> <span class="o">=</span> <span class="n">faces</span> <span class="o">-</span> <span class="n">faces</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">faces_centered</span> <span class="o">-=</span> <span class="n">faces_centered</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dataset consists of </span><span class="si">%d</span><span class="s2"> faces&quot;</span> <span class="o">%</span> <span class="n">n_samples</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>Dataset consists of 400 faces
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_gallery</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">n_col</span><span class="o">=</span><span class="n">n_col</span><span class="p">,</span> <span class="n">n_row</span><span class="o">=</span><span class="n">n_row</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">n_col</span><span class="p">,</span> <span class="mf">2.26</span> <span class="o">*</span> <span class="n">n_row</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">comp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_row</span><span class="p">,</span> <span class="n">n_col</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">vmax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="o">-</span><span class="n">comp</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">comp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">image_shape</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
                   <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span>
                   <span class="n">vmin</span><span class="o">=-</span><span class="n">vmax</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(())</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.93</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>


<span class="n">plot_gallery</span><span class="p">(</span><span class="s2">&quot;First centered Olivetti faces&quot;</span><span class="p">,</span> <span class="n">faces_centered</span><span class="p">[:</span><span class="n">n_components</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" src="_images/78f1763fc24a5923870969e7214bfcbf1ef7c28d.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">estimator</span> <span class="o">=</span> <span class="n">MiniBatchSparsePCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                               <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                               <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">faces_centered</span><span class="p">)</span>
<span class="n">components_</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">components_</span>

<span class="n">plot_gallery</span><span class="p">(</span><span class="s2">&quot;First Sparse Components&quot;</span><span class="p">,</span> <span class="n">components_</span><span class="p">[:</span><span class="n">n_components</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" src="_images/c692b93d0ef9ad043cfff0b03d4aac1a5d724494.png" /></p>
</div>
<div class="section" id="example-sparse-news-data">
<h3>Example: Sparse news data<a class="headerlink" href="#example-sparse-news-data" title="Permalink to this headline">¶</a></h3>
<p>We take an example from <span id="id9">[<a class="reference internal" href="#id78"><span>8</span></a>]</span>, dealing with news data. The data set, obtained from <a class="reference external" href="http://cs.nyu.edu/~roweis/data.html">http://cs.nyu.edu/~roweis/data.html</a>, records word appearances in 16242 news postings. Each feature represents one of a hundred words, and takes the value 1 is the word is present in the post, and 0 otherwise.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">news</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Data/20news_w100.csv&#39;</span><span class="p">)</span>
<span class="n">news</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>aids</p></th>
<th class="head"><p>baseball</p></th>
<th class="head"><p>bible</p></th>
<th class="head"><p>bmw</p></th>
<th class="head"><p>cancer</p></th>
<th class="head"><p>car</p></th>
<th class="head"><p>card</p></th>
<th class="head"><p>case</p></th>
<th class="head"><p>children</p></th>
<th class="head"><p>christian</p></th>
<th class="head"><p>computer</p></th>
<th class="head"><p>course</p></th>
<th class="head"><p>data</p></th>
<th class="head"><p>dealer</p></th>
<th class="head"><p>disease</p></th>
<th class="head"><p>disk</p></th>
<th class="head"><p>display</p></th>
<th class="head"><p>doctor</p></th>
<th class="head"><p>dos</p></th>
<th class="head"><p>drive</p></th>
<th class="head"><p>driver</p></th>
<th class="head"><p>earth</p></th>
<th class="head"><p>email</p></th>
<th class="head"><p>engine</p></th>
<th class="head"><p>evidence</p></th>
<th class="head"><p>fact</p></th>
<th class="head"><p>fans</p></th>
<th class="head"><p>files</p></th>
<th class="head"><p>food</p></th>
<th class="head"><p>format</p></th>
<th class="head"><p>ftp</p></th>
<th class="head"><p>games</p></th>
<th class="head"><p>god</p></th>
<th class="head"><p>government</p></th>
<th class="head"><p>graphics</p></th>
<th class="head"><p>gun</p></th>
<th class="head"><p>health</p></th>
<th class="head"><p>help</p></th>
<th class="head"><p>hit</p></th>
<th class="head"><p>hockey</p></th>
<th class="head"><p>honda</p></th>
<th class="head"><p>human</p></th>
<th class="head"><p>image</p></th>
<th class="head"><p>insurance</p></th>
<th class="head"><p>israel</p></th>
<th class="head"><p>jesus</p></th>
<th class="head"><p>jews</p></th>
<th class="head"><p>launch</p></th>
<th class="head"><p>law</p></th>
<th class="head"><p>league</p></th>
<th class="head"><p>lunar</p></th>
<th class="head"><p>mac</p></th>
<th class="head"><p>mars</p></th>
<th class="head"><p>medicine</p></th>
<th class="head"><p>memory</p></th>
<th class="head"><p>mission</p></th>
<th class="head"><p>moon</p></th>
<th class="head"><p>msg</p></th>
<th class="head"><p>nasa</p></th>
<th class="head"><p>nhl</p></th>
<th class="head"><p>number</p></th>
<th class="head"><p>oil</p></th>
<th class="head"><p>orbit</p></th>
<th class="head"><p>patients</p></th>
<th class="head"><p>pc</p></th>
<th class="head"><p>phone</p></th>
<th class="head"><p>players</p></th>
<th class="head"><p>power</p></th>
<th class="head"><p>president</p></th>
<th class="head"><p>problem</p></th>
<th class="head"><p>program</p></th>
<th class="head"><p>puck</p></th>
<th class="head"><p>question</p></th>
<th class="head"><p>religion</p></th>
<th class="head"><p>research</p></th>
<th class="head"><p>rights</p></th>
<th class="head"><p>satellite</p></th>
<th class="head"><p>science</p></th>
<th class="head"><p>scsi</p></th>
<th class="head"><p>season</p></th>
<th class="head"><p>server</p></th>
<th class="head"><p>shuttle</p></th>
<th class="head"><p>software</p></th>
<th class="head"><p>solar</p></th>
<th class="head"><p>space</p></th>
<th class="head"><p>state</p></th>
<th class="head"><p>studies</p></th>
<th class="head"><p>system</p></th>
<th class="head"><p>team</p></th>
<th class="head"><p>technology</p></th>
<th class="head"><p>university</p></th>
<th class="head"><p>version</p></th>
<th class="head"><p>video</p></th>
<th class="head"><p>vitamin</p></th>
<th class="head"><p>war</p></th>
<th class="head"><p>water</p></th>
<th class="head"><p>win</p></th>
<th class="head"><p>windows</p></th>
<th class="head"><p>won</p></th>
<th class="head"><p>world</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>Note the data is already sparse, so we will not standardize it to preserve sparseness. We try regular PCA first.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">news</span><span class="p">)</span>

<span class="n">pca_comp</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pca_comp</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/8f55435b690de5072d90c469e578c6e446eb6a94.png" /></p>
<p>As you can appreciate, the first principal components are dense, hurting interpretability. We now apply sparse PCA.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">SparsePCA</span>

<span class="n">spca</span> <span class="o">=</span> <span class="n">SparsePCA</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_spca</span> <span class="o">=</span> <span class="n">spca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">news</span><span class="p">)</span>

<span class="n">spca_comp</span> <span class="o">=</span> <span class="n">spca</span><span class="o">.</span><span class="n">components_</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">spca_comp</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="_images/a7aed399cdba281bfc8a5b0685c0b45fa76f6184.png" /></p>
<p>We can play with the number of components and the degree of sparseness of the components. The idea is that now each component represents sets of words, that may be associated with specific topics.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="n">news</span><span class="o">.</span><span class="n">columns</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">spca_comp</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Set of words positively associated with </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> component:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">c</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>Set of words positively associated with 1 component:
[]

Set of words positively associated with 2 component:
[&#39;card&#39;, &#39;data&#39;, &#39;disk&#39;, &#39;display&#39;, &#39;dos&#39;, &#39;driver&#39;, &#39;files&#39;, &#39;format&#39;, &#39;ftp&#39;, &#39;graphics&#39;, &#39;image&#39;, &#39;mac&#39;, &#39;memory&#39;, &#39;pc&#39;, &#39;program&#39;, &#39;server&#39;, &#39;software&#39;, &#39;version&#39;, &#39;video&#39;, &#39;win&#39;, &#39;windows&#39;]

Set of words positively associated with 3 component:
[&#39;phone&#39;, &#39;research&#39;, &#39;science&#39;, &#39;university&#39;]

Set of words positively associated with 4 component:
[&#39;question&#39;]

Set of words positively associated with 5 component:
[&#39;help&#39;]

Set of words positively associated with 6 component:
[&#39;bible&#39;, &#39;children&#39;, &#39;christian&#39;, &#39;earth&#39;, &#39;evidence&#39;, &#39;god&#39;, &#39;human&#39;, &#39;jesus&#39;, &#39;jews&#39;, &#39;religion&#39;, &#39;science&#39;]

Set of words positively associated with 7 component:
[&#39;problem&#39;]

Set of words positively associated with 8 component:
[]

Set of words positively associated with 9 component:
[]

Set of words positively associated with 10 component:
[&#39;memory&#39;, &#39;system&#39;]

Set of words positively associated with 11 component:
[]

Set of words positively associated with 12 component:
[&#39;data&#39;, &#39;earth&#39;, &#39;format&#39;, &#39;ftp&#39;, &#39;image&#39;, &#39;launch&#39;, &#39;mars&#39;, &#39;mission&#39;, &#39;moon&#39;, &#39;nasa&#39;, &#39;orbit&#39;, &#39;program&#39;, &#39;research&#39;, &#39;satellite&#39;, &#39;science&#39;, &#39;shuttle&#39;, &#39;solar&#39;, &#39;space&#39;, &#39;technology&#39;]

Set of words positively associated with 13 component:
[]

Set of words positively associated with 14 component:
[&#39;case&#39;]

Set of words positively associated with 15 component:
[]

Set of words positively associated with 16 component:
[&#39;car&#39;, &#39;disk&#39;, &#39;drive&#39;, &#39;mac&#39;, &#39;pc&#39;, &#39;scsi&#39;, &#39;software&#39;]

Set of words positively associated with 17 component:
[]

Set of words positively associated with 18 component:
[&#39;power&#39;]

Set of words positively associated with 19 component:
[]

Set of words positively associated with 20 component:
[]
</pre></div>
</div>
</div>
</div>
<div class="section" id="robust-pca">
<h2>Robust PCA<a class="headerlink" href="#robust-pca" title="Permalink to this headline">¶</a></h2>
<p>Robust PCA is a recent method for matrix separation that has received increasing attention, with applications in Machine Learning and Computer Vision among others. From a matrix separation perspective, classical PCA aims to solve</p>
<div class="math notranslate nohighlight">
\[
M = L_0 + N_0
\]</div>
<p>where <span class="math notranslate nohighlight">\(M\)</span> is the observed data matrix, <span class="math notranslate nohighlight">\(L_0\)</span> is a low rank matrix and <span class="math notranslate nohighlight">\(N_0\)</span> is dense perturbation matrix whose entries are assumed small. One way to solve this problem is to find the solution to the optimization problem</p>
<div class="math notranslate nohighlight">
\[
\underset{S.T.\ rank(L) \leq k}{\operatorname{minimize}} |M - L|_F
\]</div>
<p>for which an analytic solution is obtained by truncating the SVD of <span class="math notranslate nohighlight">\(M=U\Sigma V^T\)</span>, such that <span class="math notranslate nohighlight">\(L = U_{q}\Sigma_{q} V^T_{q}\)</span>.</p>
<p>A problem with classical PCA is that the solution is extremely sensible to outliers. The presence of a single highly corrupted observation takes the solution far away from the true solution. Since real data sets are likely to be contaminated (e.g., sensor errors, adversarial attacks), this is a real issue for PCA.</p>
<div class="figure align-default" id="id85">
<img alt="_images/rpca-004.png" src="_images/rpca-004.png" />
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">Image taken from a presentation by Yuxin Chen (<a class="reference external" href="http://www.princeton.edu/~yc5/ele520_math_data/lectures/robust_PCA.pdf">http://www.princeton.edu/~yc5/ele520_math_data/lectures/robust_PCA.pdf</a>)</span><a class="headerlink" href="#id85" title="Permalink to this image">¶</a></p>
</div>
<p>Robust PCA is a modification of the objective of classical PCA that aims to address the presence of outliers and large perturbations. Robust PCA assumes the data matrix <span class="math notranslate nohighlight">\(M\)</span> is composed as a sum of a <strong>low rank</strong> component <span class="math notranslate nohighlight">\(L_{0}\)</span> and a sparse component of arbitrarily large perturbations or corruptions <span class="math notranslate nohighlight">\(S_{0}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
M = L_0 + S_0
\]</div>
<div class="figure align-default" id="id86">
<img alt="_images/rpca_sum.png" src="_images/rpca_sum.png" />
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">Image taken from a presentation by Yuxin Chen (<a class="reference external" href="http://www.princeton.edu/~yc5/ele520_math_data/lectures/robust_PCA.pdf">http://www.princeton.edu/~yc5/ele520_math_data/lectures/robust_PCA.pdf</a>)</span><a class="headerlink" href="#id86" title="Permalink to this image">¶</a></p>
</div>
<p>In a seminal paper by Candes et. al. <span id="id10">[<a class="reference internal" href="#id79"><span>9</span></a>]</span>, it was proven that, under broad conditions, the recovery of <span class="math notranslate nohighlight">\(L_0\)</span> and <span class="math notranslate nohighlight">\(S_0\)</span> is possible, and the solution is <strong>exact</strong>. This a rather surprising result, since the problem seems intractable at first sight. Twice the number of unknowns than entries in <span class="math notranslate nohighlight">\(M\)</span>, we <strong>do not know the rank</strong> of <span class="math notranslate nohighlight">\(L_0\)</span> <strong>nor the locations (or number)</strong> of the non-zero entries of <span class="math notranslate nohighlight">\(S_{0}\)</span>. The recovery is, of course, not always possible, since one think of examples where it would not make sense.</p>
<p>The objective of an idealized RPCA can be expressed as</p>
<div class="math notranslate nohighlight">
\[
\underset{S.T. L + S = M}{\min} rank(L) + \lambda |S|_{0}
\]</div>
<p>where <span class="math notranslate nohighlight">\(|S|_0\)</span> is the zero-norm, which counts the number of non-zero elements of <span class="math notranslate nohighlight">\(S\)</span>. Unfortunately this is a very hard problem to optimize, since it is not convex, and both the rank and the 0 norm are discontinuous. In <span id="id11">[<a class="reference internal" href="#id79"><span>9</span></a>]</span>, an relaxed approach through Principal Projection Pursuit is proposed,</p>
<div class="math notranslate nohighlight">
\[
\underset{S.T. L + S = M}{\min} |L|_{*} + \lambda |S|_{1}
\]</div>
<p>where <span class="math notranslate nohighlight">\(|L|_{*}| = \sum_i \sigma_i(L)\)</span> is the nuclear norm of <span class="math notranslate nohighlight">\(L\)</span>, i.e., the sum of its singular values, and we use the 1-norm instead of the 0-norm, i.e., a LASSO penalization, to encourage sparsity. The reason the nuclear norm works instead of the rank, is that number of non-zero singular values is exactly the rank of the matrix. The new objective is a convex relaxation of the original one, and can be solved by a number of optimizations algorithms, and is able to recover the exact solution <span class="math notranslate nohighlight">\(L = L_0\)</span> and <span class="math notranslate nohighlight">\(S = S_0\)</span>, given a few conditions discussed below.</p>
<p>First, <span class="math notranslate nohighlight">\(L_0\)</span> must not be sparse, otherwise the identification of the sparse component is undetermined. Also, a low rank sparse model is largely orthogonal, meaning most observations are independent, which makes it impossible to use correlation information to impute the missing values. This is, reconstruction is possible, because a low rank dense matrix necessarily is redundant, meaning information from the un-corrupted entries can be used to reconstruct corrupted entries.</p>
<p>Second, the non-zero components of <span class="math notranslate nohighlight">\(S_0\)</span> must be spread out, as not to be low rank, again, to avoid identifiability issues. An spread out <span class="math notranslate nohighlight">\(S_0\)</span> means the corruptions are not targeted, and cannot delete single rows, for example, so the corrupted values can be recovered from the correlations in the data matrix. A good model for <span class="math notranslate nohighlight">\(S_{0}\)</span> is to required that the locations of its non-zero entries are sampled from a uniform distribution, or that each entry has a constant probability of being zeroed, independently of others (Bernoulli trials.)</p>
<p>A way to measure spareness of <span class="math notranslate nohighlight">\(L_0\)</span> is through the concept of incoherence. Incoherence is based on the correlation between the principal components and the basis vectors. With the SVD given by <span class="math notranslate nohighlight">\(L_0 = U \Sigma V^{*}\)</span>, the incoherence condition demands that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\underset{i}{\max} |U^T e_i|_2^2 \leq \frac{\mu_1 r}{n}\\
\underset{i}{\max} |V^T e_i|_2^2 \leq \frac{\mu_1 r}{n}\\
|UV^T|_\infty \leq \sqrt{\frac{\mu_2 r}{n^2}}
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> are the coherence parameters, i.e., the minimum values that satisfies the requirements. Requiring that the projections are small means that the principal components are spread out among all basis vectors. If this was not the case, and the PC lie along a few basic vectors, then <span class="math notranslate nohighlight">\(L_0\)</span> is sparse.</p>
<p>The main result from <span id="id12">[<a class="reference internal" href="#id79"><span>9</span></a>]</span> proves that Principal Component Pursuit recovers the exact solutions <span class="math notranslate nohighlight">\(L = L_0\)</span>, <span class="math notranslate nohighlight">\(S = S_0\)</span>, with high probability (<span class="math notranslate nohighlight">\(1 - O(n^{-10})\)</span>), given that the following conditions are satisfied:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(rank(L_0) \lesssim \frac{n}{\max{\mu_1,\mu_2}\log^2 n}\)</span></p></li>
<li><p>The non-zero entries of <span class="math notranslate nohighlight">\(S_0\)</span> are randomly located, and the number of entries <span class="math notranslate nohighlight">\(|S_0|_0 \leq \rho_s n^2\)</span>, where <span class="math notranslate nohighlight">\(\rho_s\)</span> is some constant, the non-vanishing fraction of allowed zeros.</p></li>
</ul>
<p>More even, the value of the regularization parameter is universal, given by <span class="math notranslate nohighlight">\(\lambda=n^{-1/2}\)</span>. This conditions turn out to be quite broad, the rank of <span class="math notranslate nohighlight">\(L\)</span> can be quite high, up to <span class="math notranslate nohighlight">\(n/polylog(n)\)</span>, the magnitude of the corruptions can be arbitrarily large and be of any signs, and the proportion of corrupted entries is finite. Numerical simulations show that successful recovery displays a phase transition in the combination of paramters <span class="math notranslate nohighlight">\(rank(L)\)</span> and <span class="math notranslate nohighlight">\(\rho_S\)</span>, i.e., there is a region where the recovery is always successful, and a region where it always fails, see figure below.</p>
<div class="figure align-default" id="id87">
<img alt="_images/rpca-phase.png" src="_images/rpca-phase.png" />
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Images from <span id="id13">[<a class="reference internal" href="#id79"><span>9</span></a>]</span>.</span><a class="headerlink" href="#id87" title="Permalink to this image">¶</a></p>
</div>
<p>More recent work have extended the results above, allowing recovery from a larger fraction of errors <span id="id14">[<a class="reference internal" href="#id80"><span>10</span></a>,<a class="reference internal" href="#id81"><span>11</span></a>]</span>.</p>
<p>Finally, Principal Components Pursuit can also extended to matrix completion with corrupted data. Additionally to sparse corruptions, consider also that only a subset <span class="math notranslate nohighlight">\(\Omega\)</span> of the data is observed, the objective is now</p>
<div class="math notranslate nohighlight">
\[
\underset{S.T.\quad \mathcal{P}_{\Omega}(L + S) = \mathcal{P}_{\Omega}(M)}{\min} |L|_{*} + \lambda |S|_{1}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{P}_{\Omega}\)</span> is the projection operator onto the set of observed entries, meaning that only the correspondence between observed entries is enforced.</p>
<p>Besides dimensionality reduction, RPCA have found applications in:</p>
<ul class="simple">
<li><p>Video surveillance, separation of background (low rank) and foreground (sparse).</p></li>
</ul>
<div class="figure align-default" id="id88">
<img alt="_images/rpca-video.png" src="_images/rpca-video.png" />
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">Images from <span id="id15">[<a class="reference internal" href="#id79"><span>9</span></a>]</span>.</span><a class="headerlink" href="#id88" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p>Face recognition, where shadows and occlusions are interpreted as the sparse component. This works because the column space for images of a face is very low rank.</p></li>
</ul>
<div class="figure align-default" id="id89">
<img alt="_images/rpca-faces.png" src="_images/rpca-faces.png" />
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">Images from <span id="id16">[<a class="reference internal" href="#id79"><span>9</span></a>]</span>.</span><a class="headerlink" href="#id89" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p>Latent Semantic Indexing, where <span class="math notranslate nohighlight">\(L_0\)</span> could capture the common words across documents, while <span class="math notranslate nohighlight">\(S_0\)</span> would capture the words that best represent each individual document.</p></li>
<li><p>Graph clustering, where the low rank component corresponds to edges between elements in a cluster, and the sparse component to elements across clusters.</p></li>
</ul>
<div class="figure align-default" id="id90">
<img alt="_images/rpca-graph.png" src="_images/rpca-graph.png" />
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text">Image taken from a presentation by Yuxin Chen (<a class="reference external" href="http://www.princeton.edu/~yc5/ele520_math_data/lectures/robust_PCA.pdf">http://www.princeton.edu/~yc5/ele520_math_data/lectures/robust_PCA.pdf</a>)</span><a class="headerlink" href="#id90" title="Permalink to this image">¶</a></p>
</div>
<ul class="simple">
<li><p>Gaussian graphical models with sparse covariance matrix and latent variables.</p></li>
</ul>
<div class="figure align-default" id="id91">
<img alt="_images/rpca-gaussian.png" src="_images/rpca-gaussian.png" />
<p class="caption"><span class="caption-number">Fig. 15 </span><span class="caption-text">Image taken from a presentation by Yuxin Chen (<a class="reference external" href="http://www.princeton.edu/~yc5/ele520_math_data/lectures/robust_PCA.pdf">http://www.princeton.edu/~yc5/ele520_math_data/lectures/robust_PCA.pdf</a>)</span><a class="headerlink" href="#id91" title="Permalink to this image">¶</a></p>
</div>
<p>The covariance and precision (<span class="math notranslate nohighlight">\(\Lambda = \Sigma^{-1}\)</span>) matrices can be blocked partitioned as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\Sigma =
\begin{pmatrix}
\Sigma_{o} &amp; \Sigma_{o,h} \\
\Sigma_{o,h}^T &amp; \Sigma_{h}
\end{pmatrix}
=
\begin{pmatrix}
\Lambda_{o} &amp; \Lambda_{o,h} \\
\Lambda_{o,h}^T &amp; \Lambda_{h}
\end{pmatrix}^{-1}
\end{align}\end{split}\]</div>
<p>It is known from linear algebra (short complement formula) that</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\underbrace{\Sigma_{o}^{-1}}_{observed}
= \underbrace{\Lambda_{o}}_{sparse}
- \underbrace{\Lambda_{o,h}\Lambda_{h}^{-1}\Lambda_{h,o}}_{\text{low rank if # hv is small}}
\end{align}\]</div>
<p>Thus, we can recover the hidden component of the graph given that number of hidden variables is small.</p>
<ul class="simple">
<li><p>Recommendation models where user data is deliberately corrupt, and the recommendation matrix is sparse, the Netflix problem.</p></li>
</ul>
<p>For further reading, refer to <span id="id17">[<a class="reference internal" href="#id82"><span>12</span></a>]</span>, <span id="id18">[<a class="reference internal" href="#id83"><span>13</span></a>]</span>, and <span id="id19">[<a class="reference internal" href="#id84"><span>14</span></a>]</span>.</p>
</div>
<div class="section" id="kernel-pca">
<h2>Kernel PCA<a class="headerlink" href="#kernel-pca" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="id20"><dl class="citation">
<dt class="label" id="id71"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Hui Zou and Lingzhou Xue. A selective overview of sparse principal component analysis. <em>Proceedings of the IEEE</em>, 106(8):1311–1320, 2018.</p>
</dd>
<dt class="label" id="id76"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Ian T Jolliffe, Nickolay T Trendafilov, and Mudassir Uddin. A modified principal component technique based on the lasso. <em>Journal of computational and Graphical Statistics</em>, 12(3):531–547, 2003.</p>
</dd>
<dt class="label" id="id75"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. <em>Journal of computational and graphical statistics</em>, 15(2):265–286, 2006.</p>
</dd>
<dt class="label" id="id74"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Optimization with sparsity-inducing penalties. <em>arXiv preprint arXiv:1108.0775</em>, 2011.</p>
</dd>
<dt class="label" id="id77"><span class="brackets"><a class="fn-backref" href="#id6">5</a></span></dt>
<dd><p>Julien Mairal, Francis Bach, and Jean Ponce. Sparse modeling for image and vision processing. <em>arXiv preprint arXiv:1411.3230</em>, 2014.</p>
</dd>
<dt class="label" id="id72"><span class="brackets"><a class="fn-backref" href="#id7">6</a></span></dt>
<dd><p>Rodolphe Jenatton, Guillaume Obozinski, and Francis Bach. Structured sparse principal component analysis. In <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 366–373. JMLR Workshop and Conference Proceedings, 2010.</p>
</dd>
<dt class="label" id="id73"><span class="brackets"><a class="fn-backref" href="#id8">7</a></span></dt>
<dd><p>Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding. In <em>Proceedings of the 26th annual international conference on machine learning</em>, 689–696. 2009.</p>
</dd>
<dt class="label" id="id78"><span class="brackets"><a class="fn-backref" href="#id9">8</a></span></dt>
<dd><p>Youwei Zhang, Alexandre d'Aspremont, and Laurent El Ghaoui. Sparse pca: convex relaxations, algorithms and applications. In <em>Handbook on Semidefinite, Conic and Polynomial Optimization</em>, pages 915–940. Springer, 2012.</p>
</dd>
<dt class="label" id="id79"><span class="brackets">9</span><span class="fn-backref">(<a href="#id10">1</a>,<a href="#id11">2</a>,<a href="#id12">3</a>,<a href="#id13">4</a>,<a href="#id15">5</a>,<a href="#id16">6</a>)</span></dt>
<dd><p>Emmanuel J Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? <em>Journal of the ACM (JACM)</em>, 58(3):1–37, 2011.</p>
</dd>
<dt class="label" id="id80"><span class="brackets"><a class="fn-backref" href="#id14">10</a></span></dt>
<dd><p>Yudong Chen, Ali Jalali, Sujay Sanghavi, and Constantine Caramanis. Low-rank matrix recovery from errors and erasures. <em>IEEE Transactions on Information Theory</em>, 59(7):4324–4337, 2013.</p>
</dd>
<dt class="label" id="id81"><span class="brackets"><a class="fn-backref" href="#id14">11</a></span></dt>
<dd><p>Arvind Ganesh, John Wright, Xiaodong Li, Emmanuel J Candes, and Yi Ma. Dense error correction for low-rank matrices via principal component pursuit. In <em>2010 IEEE international symposium on information theory</em>, 1513–1517. IEEE, 2010.</p>
</dd>
<dt class="label" id="id82"><span class="brackets"><a class="fn-backref" href="#id17">12</a></span></dt>
<dd><p>Venkat Chandrasekaran, Sujay Sanghavi, Pablo A Parrilo, and Alan S Willsky. Rank-sparsity incoherence for matrix decomposition. <em>SIAM Journal on Optimization</em>, 21(2):572–596, 2011.</p>
</dd>
<dt class="label" id="id83"><span class="brackets"><a class="fn-backref" href="#id18">13</a></span></dt>
<dd><p>Venkat Chandrasekaran, Pablo A Parrilo, and Alan S Willsky. Latent variable graphical model selection via convex optimization. In <em>2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</em>, 1610–1613. IEEE, 2010.</p>
</dd>
<dt class="label" id="id84"><span class="brackets"><a class="fn-backref" href="#id19">14</a></span></dt>
<dd><p>Yudong Chen. Incoherence-optimal matrix completion. <em>IEEE Transactions on Information Theory</em>, 61(5):2909–2923, 2015.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="assignments-dummy/U2-M2-L1-dim_red_PCA.html" title="previous page">Principal Component Analysis</a>
    <a class='right-next' id="next-link" href="U2-M2-L3-Other_Linear_Methods_for_DR.html" title="next page">Other Linear Methods for DR</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Gonzalo G. Peraza Mues<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>