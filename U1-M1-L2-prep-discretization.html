
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Discretization &#8212; Lecture Notes in Unsupervised and Reinforcement Learning</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/bokeh-tables-2.3.1.min.js"></script>
    <script src="_static/bokeh-2.3.1.min.js"></script>
    <script src="_static/bokeh-widgets-2.3.1.min.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Assignment: Discretization" href="assignments-dummy/U1-M1-L2-prep-discretization.html" />
    <link rel="prev" title="Assignment: Normalization" href="assignments-dummy/U1-M1-L1-prep-normalization.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/UPY-completo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Lecture Notes in Unsupervised and Reinforcement Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome!
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 1 - Unsupervised Preprocessing
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L1-prep-normalization.html">
   Data Normalization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L1-prep-normalization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="current reference internal" href="#">
   Discretization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L2-prep-discretization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L3-unsup-outlier-detection.html">
   Anomaly and Outlier Detection
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L3-prep-outlier-detection.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 2 - Dimensionality Reduction
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U2-M1-L1-similarity_metrics.html">
   (Dis)similarity metrics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html">
     <strong>
      Warning
     </strong>
     :
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-1-2-pt">
     Exercise 1 (2 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-2-3-pt">
     Exercise 2 (3 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-3-4-pts">
     Exercise 3 (4 pts)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-4-1-pt">
     Exercise 4 (1 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-5-ungraded">
     Exercise 5 (ungraded)
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M1-L3-dim_red_PCA.html">
   Dimensionality Reduction and PCA
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="freedman-diaconis.html">
   Freedman-Diaconis Rule
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/U1-M1-L2-prep-discretization.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/gperaza/UL-lecture-notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/gperaza/UL-lecture-notes/issues/new?title=Issue%20on%20page%20%2FU1-M1-L2-prep-discretization.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-data">
   Test data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#equal-width">
   Equal width
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#equal-frequency">
   Equal frequency
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discretization-by-clustering">
   Discretization by clustering
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-k-means">
     1D K-means
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fisher-jenks">
     Fisher-Jenks
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-density-estimation">
   Kernel density estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#to-do">
   TO-DO
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="discretization">
<h1>Discretization<a class="headerlink" href="#discretization" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>In statistics and machine learning, discretization refers to the process of converting or partitioning continuous attributes, features or variables to discretized or nominal attributes/features/variables/intervals. Whenever continuous data is discretized, there is always some amount of discretization error. The goal is to reduce the amount to a level considered negligible for the modeling purposes at hand. Many machine learning algorithms are known to produce better models by discretizing continuous attributes. <span id="id1">[<a class="reference internal" href="#id16"><span>1</span></a>]</span></p>
<p>In a supervised context, the performance of a particular discretization can be evaluated using class information. Many algorithms use this information to find the minimum number of partitions relative to some metric, such as mutual information. Among these methods we find Fayyad &amp; Irani’s MDL method, CAIM, CACC, and Ameva.</p>
<p>When class information is not available, it becomes difficult to evaluate the partition. Typically data is discretized into partitions of K equal lengths/width (equal intervals) or K% of the total data (equal frequencies).</p>
</div>
<div class="section" id="test-data">
<h2>Test data<a class="headerlink" href="#test-data" title="Permalink to this headline">¶</a></h2>
<p>We begin with our standard imports:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
</pre></div>
</div>
<p>We will create some synthetic data do test different discretization schemes, 3 data sets with different characteristics, an homogeneous data set, one with 2 blobs and a last one with 3 blobs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># taken from sklearn docs: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_strategies.html#sphx-glr-auto-examples-preprocessing-plot-discretization-strategies-py</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">centers_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="n">centers_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">X_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="p">[</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n_samples</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">//</span> <span class="mi">10</span><span class="p">,</span>
                          <span class="n">n_samples</span> <span class="o">//</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n_samples</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">//</span> <span class="mi">10</span><span class="p">],</span>
               <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="n">centers_0</span><span class="p">,</span>
               <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="p">[</span><span class="n">n_samples</span> <span class="o">//</span> <span class="mi">5</span><span class="p">,</span> <span class="n">n_samples</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">//</span> <span class="mi">5</span><span class="p">],</span>
               <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="n">centers_1</span><span class="p">,</span>
               <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
<span class="p">]</span>

<span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.4</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="mf">4.8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">X</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X_list</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_list</span><span class="p">),</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<p><img alt="" src="_images/eea4aa66fe69c5e236464b1a7cd4e5ea0c31e151.png" /></p>
</div>
<div class="section" id="equal-width">
<h2>Equal width<a class="headerlink" href="#equal-width" title="Permalink to this headline">¶</a></h2>
<p>In both methods, arity <span class="math notranslate nohighlight">\(k\)</span> is used to determine the number of bins. Each bin is associated with a distinct discrete value. In equal-width, the continuous range of a feature is evenly divided into intervals that have an equal-width and each interval represents a bin. In equal-frequency, an equal number of continuous values are placed in each bin.</p>
<p>The two methods are very simple but are sensitive for a given k (number of bins). For equal-frequency, for instance, many occurrences of a continuous value could cause the occurrences to be assigned into different bins. One improvement can be that, after continuous values are assigned into bins, boundaries of every pair of neighboring bins are adjusted so that duplicate values should belong to one bin only. Another problem is outliers that take extreme values. One solution can be to remove the outliers using a threshold.</p>
<p>Here we implement equal with discretization. The function <code class="docutils literal notranslate"><span class="pre">disc_eq_width(X,</span> <span class="pre">k)</span></code> takes a data matrix and the number of bins as paramenters. Each feature is discretized individually.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">disc_eq_width</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="c1"># Implement equal width discretization with k bins.</span>
    <span class="c1"># Each feature is discretized individually (univariate discretization).</span>
    <span class="c1"># X is a data matrix with an arbitrary number of unsorted features.</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Find class limits.</span>
    <span class="n">clims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">num</span><span class="o">=</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">clims</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">NINF</span>
    <span class="n">clims</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">Inf</span>

    <span class="n">X_disc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">X_disc</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">clims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span> <span class="n">side</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="c1"># Return the discretized labels in a matrix the same shape as X.</span>
    <span class="c1"># Return the limits in a matrix which rows are the interval classes of each feature.</span>
    <span class="k">return</span> <span class="n">X_disc</span><span class="p">,</span> <span class="n">clims</span>
</pre></div>
</div>
<p>To test the function employ the following test matrix:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span>   <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                   <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span>
                   <span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span>  <span class="mf">0.5</span><span class="p">],</span>
                   <span class="p">[</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">]])</span>

<span class="n">X_disc</span><span class="p">,</span> <span class="n">bin_edges</span> <span class="o">=</span> <span class="n">disc_eq_width</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The discretized matrix is:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_disc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The bin edges are:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bin_edges</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>The discretized matrix is:
[[0. 0. 0. 0.]
 [1. 1. 1. 0.]
 [2. 2. 2. 1.]
 [2. 2. 2. 2.]]

The bin edges are:
[[-inf  -1.   0.  inf]
 [-inf   2.   3.  inf]
 [-inf  -3.  -2.  inf]
 [-inf   0.   1.  inf]]
</pre></div>
</div>
<p>To explore class limits, we use a helper function</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">print_class_limits</span>

<span class="n">print_class_limits</span><span class="p">(</span><span class="n">X_disc</span><span class="p">,</span> <span class="n">bin_edges</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>Feature 0
      Interval         Count
----------------------------
(  -inf,  -1.0) |        1
(  -1.0,   0.0) |        1
(   0.0,   inf) |        2

Feature 1
      Interval         Count
----------------------------
(  -inf,   2.0) |        1
(   2.0,   3.0) |        1
(   3.0,   inf) |        2

Feature 2
      Interval         Count
----------------------------
(  -inf,  -3.0) |        1
(  -3.0,  -2.0) |        1
(  -2.0,   inf) |        2

Feature 3
      Interval         Count
----------------------------
(  -inf,   0.0) |        2
(   0.0,   1.0) |        1
(   1.0,   inf) |        1
</pre></div>
</div>
<p>When applied to the test data with K=4, we obtain:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">disc_eq_width</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">X_list</span><span class="p">]</span>
<span class="n">bin_edges</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>

<span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.4</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="mf">4.8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">X</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X_list</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_list</span><span class="p">),</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">bin_edges</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">bin_edges</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(())</span>
</pre></div>
</div>
<p><img alt="" src="_images/cfdbc0c5a3abb28efe88c3a9f216b05cb7866742.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;########################&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Matrix </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;########################&#39;</span><span class="p">)</span>
    <span class="n">print_class_limits</span><span class="p">(</span><span class="o">*</span><span class="n">res</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>########################
Matrix 0
########################
Feature 0
      Interval         Count
----------------------------
(    -inf,    -1.5) |       49
(    -1.5,  -0.013) |       48
(  -0.013,     1.5) |       49
(     1.5,     inf) |       54

Feature 1
      Interval         Count
----------------------------
(    -inf,    -1.5) |       58
(    -1.5, -0.0022) |       38
( -0.0022,     1.5) |       57
(     1.5,     inf) |       47


########################
Matrix 1
########################
Feature 0
      Interval         Count
----------------------------
(    -inf,     1.3) |      101
(     1.3,     3.9) |       19
(     3.9,     6.5) |        1
(     6.5,     inf) |       79

Feature 1
      Interval         Count
----------------------------
(    -inf,     1.5) |       20
(     1.5,     4.0) |        7
(     4.0,     6.5) |       93
(     6.5,     inf) |       80


########################
Matrix 2
########################
Feature 0
      Interval         Count
----------------------------
(    -inf,   0.057) |       25
(   0.057,     1.4) |       16
(     1.4,     2.8) |       51
(     2.8,     inf) |      108

Feature 1
      Interval         Count
----------------------------
(    -inf,  -0.014) |       22
(  -0.014,    0.97) |       91
(    0.97,     1.9) |       81
(     1.9,     inf) |        6
</pre></div>
</div>
<p>There is a rule of thumb for determining the optimal number of bins in a histogram. The Freedman-Diaconis rule states that the optimal bin width can be estimated as</p>
<div class="math notranslate nohighlight">
\[
h = \frac{2\ IQR}{n^{1/3}}
\]</div>
<p>The asymptotic (large <span class="math notranslate nohighlight">\(n\)</span>) optimal bin width was derived by Scott <span id="id2">[<a class="reference internal" href="freedman-diaconis.html#id9"><span>1</span></a>]</span>, yet, its value depends on the derivative of the theoretical distribution, often not known. Freedman and Diaconis claimed <span id="id3">[<a class="reference internal" href="#id18"><span>3</span></a>]</span> the bin width can be robustly estimated by the formula above, which works well most of the time, under the requirements that the true distribution has squared integrable and continuous first and second derivatives.</p>
<p>It is worth it to provide a rough derivation of the FD rule, as it is an nice exercise in the art of approximation. You can find such derivation in the <a class="reference internal" href="freedman-diaconis.html"><span class="doc std std-doc">Appendix</span></a>.</p>
</div>
<div class="section" id="equal-frequency">
<h2>Equal frequency<a class="headerlink" href="#equal-frequency" title="Permalink to this headline">¶</a></h2>
<p>We now deal with equal frequency binning. To find the bin limits we can use the percentiles of each feature, as to partition with equal number of elements in each interval.</p>
<p>Again, with 4 bins (K=4), but using equal frequency:</p>
<p><img alt="" src="_images/8728237344f6f9856fadeb4255eaf0452c8cc48d.png" /></p>
<p>You’ll implement this method in the assignment.</p>
<p>Equal-width and equal-frequency are simple and easy to implement. This does not come without a price. First, arity k has to be specified beforehand. Because we usually do not know what a proper value k is, we need to resort to trial-and-error or specify a value randomly.</p>
<p>Both this methods are implemented in the sklearn preprocessing module, in the KBinsDiscretizer method. KBinsDiscretizer performs extra safety checks, so should be preferred in production. Refer to: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html</a></p>
</div>
<div class="section" id="discretization-by-clustering">
<h2>Discretization by clustering<a class="headerlink" href="#discretization-by-clustering" title="Permalink to this headline">¶</a></h2>
<div class="section" id="d-k-means">
<h3>1D K-means<a class="headerlink" href="#d-k-means" title="Permalink to this headline">¶</a></h3>
<p>Discretizing a data set is similar to the clustering problem in the sense that we are looking for partitions with large within-class similarity and small intra-class similarity.</p>
<p>One way to discretize a feature matrix is to apply a clustering algorithm to each individual feature. In the following we will implement discretization via the K-Means clustering algorithm. Since we have not yet implemented our own version of K-means (wait until unit 3), we’ll follow a method proposed by Daniela Joita in <span id="id4">[<a class="reference internal" href="#id14"><span>4</span></a>]</span> to cluster one dimensional features. This particular algorithm has a long history within quantitative geography, where it was first developed by Jenks and Caspall <span id="id5">[<a class="reference internal" href="#id19"><span>5</span></a>]</span>, and understandably known as the Jenks-Caspall algorithm, or <a class="reference external" href="https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization#cite_note-6">Jenks natural breaks algorithm</a>.</p>
<p>The idea of the algorithm is to chose initial centers such that they are in increasing order. In this way, the recomputed centers are also in increasing order and therefore to determine the closest cluster for each value of the attribute A, the algorithm does less comparisons than in the general case. The closest cluster either remains the one in which the value belongs to or it is one of the two neighbouring clusters. In this way the number of comparisons done for reallocation of cluster is no longer k (one for each centroid) but 3. Also there is no need to order all the values in dom(A) like in the case of equal- frequency interval discretization.</p>
<p>The cut points are defined as the minimum and maximum of the active domain of the attribute and the midpoints between the centers of the clusters.</p>
<p>In the assignment, you will implement this algorithm starting with a 1D version of K-means, which is applied independently to each feature.</p>
<p>To test the function employ the following test matrix:</p>
<p>The following plots show the results of applying 1D K-means to each feature of the test data, with K=4.</p>
<p><img alt="" src="_images/5e170d37fc5a17d35051231a91e368495a683385.png" /></p>
</div>
<div class="section" id="fisher-jenks">
<h3>Fisher-Jenks<a class="headerlink" href="#fisher-jenks" title="Permalink to this headline">¶</a></h3>
<p>Another modified version of the algorithm that uses dynamic programming to find the optimal partition is the Fisher-Jenks algorithm <span id="id6">[<a class="reference internal" href="#id20"><span>6</span></a>]</span>, for which we are missing an implementation. The FJ algorithm tests all possible <strong>continuous</strong> partitions (reducing the search space) and selects the one that minimizes the clusters variance. Extra point awarded for whom can provide a working well documented implementation (possible project).</p>
<p>While in general there exist <span class="math notranslate nohighlight">\({N \choose K}\)</span> ways of partition N points into K groups, when restricting to ordered continuous partitions the total number of possible partitions is <span class="math notranslate nohighlight">\({N-1 \choose
K-1}\)</span>. This is easily understood by considering the problem as choosing where to place the braking points in an ordered list. But, even with this reduction the number of possible partition is still impractical for moderate N and K.</p>
<p>To address this, we can formulate the problem in terms of smaller sub-problems, involving finding partitions in subsets of the original data. Then reusing this work by aggregating sub-solutions into the optimal solution. This is possible, since the following lemma holds <span id="id7">[<a class="reference internal" href="#id20"><span>6</span></a>]</span>:</p>
<p><em>Suboptimization Lemma</em>: If <span class="math notranslate nohighlight">\(A_1:A_2\)</span> denotes a partition of a set <span class="math notranslate nohighlight">\(A\)</span> into two disjoint subsets <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span>, if <span class="math notranslate nohighlight">\(P_1^{*}\)</span> denotes a least squares partition of <span class="math notranslate nohighlight">\(A_1\)</span> into two <span class="math notranslate nohighlight">\(G_1\)</span> subsets and <span class="math notranslate nohighlight">\(P_2^{*}\)</span> denotes a least squares partition of <span class="math notranslate nohighlight">\(A_2\)</span> into <span class="math notranslate nohighlight">\(G_2\)</span> subsets; then, of the class of subpartitions of <span class="math notranslate nohighlight">\(A_1:A_2\)</span> employing <span class="math notranslate nohighlight">\(G_1\)</span> subsets over <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(G_2\)</span> subsets over <span class="math notranslate nohighlight">\(A_2\)</span> a least squares subpartition is <span class="math notranslate nohighlight">\(P_1^{*}:P_2^{*}\)</span>.</p>
<p>From the lemma follows the recursive relationship</p>
<div class="math notranslate nohighlight">
\[
P^{*}(k, U) = \min_L P^{*}(k-1, L-1) + D(L, U),\quad L=k-1 ,\ldots, U
\]</div>
<p>where <span class="math notranslate nohighlight">\(P^{*}(k,U)\)</span> is the optimal partition of subset <span class="math notranslate nohighlight">\(x_1,\ldots,x_U\)</span> into <span class="math notranslate nohighlight">\(k\)</span> groups, and <span class="math notranslate nohighlight">\(D(a,b)\)</span> is the diameter of the cluster comprised of points <span class="math notranslate nohighlight">\(x_a,\ldots,x_b\)</span>. Using this recursion relation, build optimal partitions for <span class="math notranslate nohighlight">\(k\)</span> from previous calculated partitions for <span class="math notranslate nohighlight">\(k-1\)</span>, starting with <span class="math notranslate nohighlight">\(k=2\)</span>. An implementation can be found in <span id="id8">[<a class="reference internal" href="#id21"><span>7</span></a>]</span>. I have implemented the algorithm below, in such a ways that wastes a lot of memory, but is easier to understand. Try to optimize it as an exercise.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">120</span><span class="p">,</span> <span class="mi">108</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">108</span><span class="p">,</span> <span class="mi">108</span><span class="p">,</span> <span class="mi">108</span><span class="p">,</span> <span class="mi">106</span><span class="p">,</span> <span class="mi">108</span><span class="p">,</span>
              <span class="mi">103</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">104</span><span class="p">,</span> <span class="mi">105</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">99</span><span class="p">])</span>

<span class="c1"># Diameter func to test fisher algorithm</span>
<span class="k">def</span> <span class="nf">diam_median</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_m</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">fisher</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">diam</span><span class="o">=</span><span class="n">diam_median</span><span class="p">,</span> <span class="n">sort</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># Discretize a single vector X into K clusters.</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sort</span><span class="p">:</span>
        <span class="n">X</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="c1"># The P matrix stores the optimal partition</span>
    <span class="c1"># P[k,j] is the (k+1)-partition up to point x[j], inclusive</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">N</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>

    <span class="c1"># The C and C_prev matrix stores Class limits,</span>
    <span class="c1"># at the current value of K,where</span>
    <span class="c1"># C[u] are the limits for P[k,u]</span>
    <span class="c1"># We abuse copying from C_prev to C which is costly</span>
    <span class="c1"># Alternativel it seems is possible to build the limits in place</span>
    <span class="c1"># though It is not clear to me how</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">K</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">C_prev</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">C_prev</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">C_prev</span><span class="p">[</span><span class="n">u</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">u</span><span class="o">+</span><span class="mi">1</span>

    <span class="c1"># Store limits for u=N</span>
    <span class="n">C_final</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">K</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># A matric to store single cluster diameters</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">),</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="c1"># We could avouid this storage space by changin the loop order</span>
    <span class="c1"># yet, we pay the price of increased complexity</span>
    <span class="c1"># see: https://pysal.org/mapclassify/_modules/mapclassify/classifiers.html#FisherJenks</span>

    <span class="c1"># Build the D matrix and store single cluster P values</span>
    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">u</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">D</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="n">diam</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">l</span><span class="p">:</span> <span class="n">u</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># Just calculate it once</span>
    <span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">D</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>

    <span class="c1"># Iteratively build clusters</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">u</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">D</span><span class="p">[</span><span class="n">l</span><span class="p">,</span><span class="n">u</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="n">P</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">u</span><span class="p">]:</span>
                    <span class="n">P</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
                    <span class="n">C</span><span class="p">[</span><span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="n">C_prev</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">C</span><span class="p">[</span><span class="n">u</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">u</span><span class="o">+</span><span class="mi">1</span>
        <span class="n">C_prev</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">C_final</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">C_final</span><span class="p">)</span>

<span class="n">fisher</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  8. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  1.  8. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  1.  8. 14. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  1.  8. 11. 13. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  1.  8. 11. 13. 14. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  1.  8. 11. 12. 13. 14. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  1.  2.  3.  8. 11. 13. 14. 16.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  1.  2.  3.  8. 11. 12. 13. 14. 16.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  1.  2.  3.  6.  7.  8. 11. 13. 14. 16.  0.  0.  0.  0.  0.  0.]
 [ 0.  1.  2.  3.  6.  7.  8. 11. 12. 13. 14. 16.  0.  0.  0.  0.  0.]
 [ 0.  1.  2.  3.  6.  7.  8. 11. 12. 13. 14. 15. 16.  0.  0.  0.  0.]
 [ 0.  1.  2.  3.  4.  6.  7.  8. 11. 12. 13. 14. 15. 16.  0.  0.  0.]
 [ 0.  1.  2.  3.  4.  5.  6.  7.  8. 11. 12. 13. 14. 15. 16.  0.  0.]
 [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 11. 12. 13. 14. 15. 16.  0.]
 [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]]
</pre></div>
</div>
</div>
</div>
<div class="section" id="kernel-density-estimation">
<h2>Kernel density estimation<a class="headerlink" href="#kernel-density-estimation" title="Permalink to this headline">¶</a></h2>
<p>Kernel density estimation <span id="id9">[<a class="reference internal" href="#id15"><span>8</span></a>]</span> is a univariate non-parametric probability density estimation method. It estimates the probability density at point <span class="math notranslate nohighlight">\(x\)</span> through the kernel density function:</p>
<div class="math notranslate nohighlight">
\[
p(x) = \frac{1}{nh}\sum_{i=1}^n K\left( \frac{x- X_i}{h}  \right)
\]</div>
<p>where the kernel K is a weighting function, the <span class="math notranslate nohighlight">\(X_i\)</span> are the <span class="math notranslate nohighlight">\(n\)</span> available data points, and <span class="math notranslate nohighlight">\(h\)</span> is the kernel bandwidth, a hyper-parameter. It is essentially a weighted average, where the weights are given by the kernel function. Each point contributes to the density depending on its distance from <span class="math notranslate nohighlight">\(x\)</span>. The kernel must be a positive function that integrates to 1. The choice of the bandwidth is crucial and heavily influences the results. Intuitively one wants to choose <span class="math notranslate nohighlight">\(h\)</span> as small as the data will allow; however, there is always a trade-off between the bias of the estimator and its variance. While methods have been proposed for bandwidth selection, a simple yet good performing one is k-fold cross-validation.</p>
<p>Here we will employ KDE as a non-parametric unsupervised discretizing method. First, we find the best kernel density estimate for each row of the data using CV. Then, we select the cut-points at the local minima of the KDE. You will implement this in the assignment.</p>
<p>The following code plots the sample data sets with the found partitions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bin_edges</span> <span class="o">=</span> <span class="p">[</span><span class="n">disc_kde</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">X_list</span><span class="p">]</span>

<span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">6.4</span><span class="o">*</span><span class="mi">3</span><span class="p">,</span> <span class="mf">4.8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">X</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X_list</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_list</span><span class="p">),</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">bin_edges</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">bin_edges</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(())</span>
</pre></div>
</div>
<p><img alt="" src="_images/5bf0b9673ee5d2948b0a34de4fd19da8bc73f61b.png" /></p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Discretization: An Enabling Technique, Liu et. al.</p></li>
</ul>
<p id="id10"><dl class="citation">
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Wikipedia contributors. Discretization of continuous features — Wikipedia, the free encyclopedia. 2019. [Online; accessed 11-March-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Discretization_of_continuous_features&amp;oldid=898762851">https://en.wikipedia.org/w/index.php?title=Discretization_of_continuous_features&amp;oldid=898762851</a>.</p>
</dd>
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>David W Scott. On optimal and data-based histograms. <em>Biometrika</em>, 66(3):605–610, 1979.</p>
</dd>
<dt class="label" id="id18"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>David Freedman and Persi Diaconis. On the histogram as a density estimator: l 2 theory. <em>Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete</em>, 57(4):453–476, 1981.</p>
</dd>
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Daniela Joiţa. Unsupervised static discretization methods in data mining. <em>Titu Maiorescu University, Bucharest, Romania</em>, 2010.</p>
</dd>
<dt class="label" id="id19"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>George F Jenks and Fred C Caspall. Error on choroplethic maps: definition, measurement, reduction. <em>Annals of the Association of American Geographers</em>, 61(2):217–244, 1971.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">6</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Walter D Fisher. On grouping for maximum homogeneity. <em>Journal of the American statistical Association</em>, 53(284):789–798, 1958.</p>
</dd>
<dt class="label" id="id21"><span class="brackets"><a class="fn-backref" href="#id8">7</a></span></dt>
<dd><p>John A Hartigan. <em>Clustering algorithms</em>. John Wiley &amp; Sons, Inc., 1975.</p>
</dd>
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id9">8</a></span></dt>
<dd><p>Wikipedia contributors. Kernel density estimation — Wikipedia, the free encyclopedia. 2021. [Online; accessed 11-March-2021]. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Kernel_density_estimation&amp;oldid=1010981873">https://en.wikipedia.org/w/index.php?title=Kernel_density_estimation&amp;oldid=1010981873</a>.</p>
</dd>
</dl>
</p>
</div>
<div class="section" id="to-do">
<h2>TO-DO<a class="headerlink" href="#to-do" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>[ ] Discuss several recent unsupervised methods in the literature (papers available on bibliography folder).</p></li>
<li><p>[ ] Add methods discussed in <a class="reference external" href="https://geographicdata.science/book/notebooks/05_choropleth.html#quantitative-data-classification">https://geographicdata.science/book/notebooks/05_choropleth.html#quantitative-data-classification</a></p></li>
</ul>
<div class="toctree-wrapper compound">
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="assignments-dummy/U1-M1-L1-prep-normalization.html" title="previous page">Assignment: Normalization</a>
    <a class='right-next' id="next-link" href="assignments-dummy/U1-M1-L2-prep-discretization.html" title="next page">Assignment: Discretization</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Gonzalo G. Peraza Mues<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>