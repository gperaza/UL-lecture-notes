
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Other Dimensionality Reduction Methods &#8212; Lecture Notes in Unsupervised and Reinforcement Learning</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Non-linear Dimensionality Reduction" href="U2-M2-L4-Non_Linear_Methods_for_DR.html" />
    <link rel="prev" title="Some linear and non-linear variants of PCA" href="U2-M2-L2-PCA_variants.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/UPY-completo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Lecture Notes in Unsupervised and Reinforcement Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome!
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 1 - Unsupervised Preprocessing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L1-prep-normalization.html">
   Data Normalization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L1-prep-normalization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L2-prep-discretization.html">
   Discretization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L2-prep-discretization.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U1-M1-L3-unsup-outlier-detection.html">
   Anomaly and Outlier Detection
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U1-M1-L3-prep-outlier-detection.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 2 - Dimensionality Reduction
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U2-M1-L1-similarity_metrics.html">
   (Dis)similarity metrics
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html">
     <strong>
      Warning
     </strong>
     :
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-1-2-pt">
     Exercise 1 (2 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-2-3-pt">
     Exercise 2 (3 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-3-4-pts">
     Exercise 3 (4 pts)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-4-1-pt">
     Exercise 4 (1 pt)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M1-L1-similarity_metrics.html#exercise-5-ungraded">
     Exercise 5 (ungraded)
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="U2-M2-L1-dim_red_PCA.html">
   Dimensionality Reduction and PCA
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="assignments-dummy/U2-M2-L1-dim_red_PCA.html">
     Assignment
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M2-L2-PCA_variants.html">
   Some linear and non-linear variants of PCA
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Other Dimensionality Reduction Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U2-M2-L4-Non_Linear_Methods_for_DR.html">
   Non-linear Dimensionality Reduction
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 3 - Clustering
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L1-Prototype_clustering.html">
   Prototype Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L2-Hierarchical_clustering.html">
   Hierarchical Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L3-Density_based_clustering.html">
   Density Based Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M1-L4-Other_clustering_methods.html">
   Other Clustering Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U3-M2-L1-Clustering_evaluation.html">
   Clustering Evaluation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Unit 4 - Probabilistic Methods
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="U4-M1-L1-Latent_variable_models.html">
   Latent Variable Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U4-M1-L2-Independent_Component_Analysis.html">
   Independent Component Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="U4-M2-L1-Gaussian_Mixture_Models.html">
   Gaussian Mixture Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="freedman-diaconis.html">
   Freedman-Diaconis Rule
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/U2-M2-L3-Other_Linear_Methods_for_DR.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/gperaza/UL-lecture-notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/gperaza/UL-lecture-notes/issues/new?title=Issue%20on%20page%20%2FU2-M2-L3-Other_Linear_Methods_for_DR.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multidimensional-scaling-mds">
   Multidimensional Scaling (MDS)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classical-mds">
     Classical MDS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#least-squares-or-kruskal-shephard-scaling">
     Least squares or Kruskal-Shephard scaling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sammon-mapping">
     Sammon mapping
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shephard-kruskal-non-metric-scaling">
     Shephard-Kruskal non-metric scaling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-road-distance">
     Example: Road Distance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-digits">
     Example: Digits
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-correlation-tables">
     Example: Correlation tables
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-negative-matrix-factorization-nnmf">
   Non-negative matrix factorization (NNMF)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-projection">
   Random Projection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="other-dimensionality-reduction-methods">
<h1>Other Dimensionality Reduction Methods<a class="headerlink" href="#other-dimensionality-reduction-methods" title="Permalink to this headline">¶</a></h1>
<div class="section" id="multidimensional-scaling-mds">
<h2>Multidimensional Scaling (MDS)<a class="headerlink" href="#multidimensional-scaling-mds" title="Permalink to this headline">¶</a></h2>
<p>Multidimensional Scaling works directly on the distance or dissimilarity matrix <span class="math notranslate nohighlight">\(D\)</span> and tries to find a set of vectors <span class="math notranslate nohighlight">\(z_i\)</span> that reproduces the distance matrix (preserves pairwise distance) as close as possible, with the liberty of choosing the dimensions of the <span class="math notranslate nohighlight">\(z_i\)</span>.</p>
<p>MDS can be classified onto metric, and non metric MDS. Metric MDS tries to preserve the actual distance values in the distance matrix, while non-metric MDS just preserves the rank. Classical MDS and the least squares scalings are metric scalings, and Shephard-Kruskal is a non-metric scaling.</p>
<div class="section" id="classical-mds">
<h3>Classical MDS<a class="headerlink" href="#classical-mds" title="Permalink to this headline">¶</a></h3>
<p>Classical MDS is a linear dimensionality reduction method. To perform classical MDS, we first transform the distance matrix into a <strong>centered</strong> Gram matrix, through the equation</p>
<div class="math notranslate nohighlight">
\[
G = -\frac{1}{2}(I - \frac{1}{N}\vec{1}\vec{1}^T) D^2 (I - \frac{1}{N}\vec{1}\vec{1}^T)
\]</div>
<p>where <span class="math notranslate nohighlight">\(M = \frac{1}{N}\vec{1}\vec{1}^T\)</span> is the mean operator, and <span class="math notranslate nohighlight">\(D^2\)</span> is the squared distance matrix, not <span class="math notranslate nohighlight">\(DD\)</span>. As en exercise, prove the previous statement.</p>
<p>The Gram matrix, being positive semi-definite, can be factorized as <span class="math notranslate nohighlight">\(G=XX^T\)</span>, so that <span class="math notranslate nohighlight">\(g_{ij} = x_i^T x_j\)</span>. Classical MDS seeks to minimize</p>
<div class="math notranslate nohighlight">
\[
S_C(\{z_i\}) = \sum_{ij} \left( g_{ij} - z_i^T z_j  \right)^2
= \frac{1}{2}|G - ZZ^T|_F^2
\]</div>
<p>But this is same objective as PCA, only applied to the Gram matrix instead of the covariance matrix. The best low rank approximation is given by the eigen-decomposition of the Gram matrix, <span class="math notranslate nohighlight">\(G=U S^2 U^T\)</span>, such that <span class="math notranslate nohighlight">\(Z = U_q S_q\)</span>. From this discussion, it is clear that classical MDS is equivalent to PCA if euclidean distances are used. In fact, if the distances are Euclidean, the full rank model recovers the exact original configuration of points.</p>
<p>Note that classical MDS assumes euclidean distances, so as to have a positive semi-definite Gram matrix. For non-euclidean distances, negative eigenvalues arise from the decomposition, which mean we cannot take <span class="math notranslate nohighlight">\(S = \sqrt(S^2)\)</span> on the diagonal elements. In such cases, we can restrict the solution to the coordinates associated with positive eigenvalues, still obtaining a solution, but the solution is no longer equivalent to PCA.</p>
</div>
<div class="section" id="least-squares-or-kruskal-shephard-scaling">
<h3>Least squares or Kruskal-Shephard scaling<a class="headerlink" href="#least-squares-or-kruskal-shephard-scaling" title="Permalink to this headline">¶</a></h3>
<p>In metric MDS, the coordinates <span class="math notranslate nohighlight">\(z_i\)</span> are found by minimizing the stress function,</p>
<div class="math notranslate nohighlight">
\[
S_M({z_i}) = \sum_{i \neq j} \left(d_{ij} - |z_i - z_j|\right)^2
\]</div>
<p>This objective does not have a closed form solution, and must be optimized numerically. The solution are no longer the same as those obtained from PCA, and can be non-linear.</p>
<p>The stress can be minimized with a number of optimizes, including the ones based on gradient descent. If the Euclidean distance is used, the SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm can be used. SMACOF is a <a class="reference external" href="https://en.wikipedia.org/wiki/MM_algorithm">majorize-minimization</a> (MM) algorithm, in which instead of minimizing the objective directly, we minimize a lower bound, the majorizing function.</p>
<p>A general MM algorithm works as follows. Let <span class="math notranslate nohighlight">\(f(x)\)</span> be the function to be minimized. The majorized function <span class="math notranslate nohighlight">\(g(x, x_m)\)</span> must satisfy</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
g(x | x_m) \leq f(x) \forall x\\
g(x_m | x_m) = f(x_m)
\end{align}\end{split}\]</div>
<p>The point <span class="math notranslate nohighlight">\(x_{m}\)</span> is where <span class="math notranslate nohighlight">\(g(x|x_{m})\)</span> touches <span class="math notranslate nohighlight">\(f(x)\)</span>. At each iteration we find the minimum of <span class="math notranslate nohighlight">\(g\)</span> and move <span class="math notranslate nohighlight">\(x_m\)</span> to this new minimum.</p>
<div class="math notranslate nohighlight">
\[\begin{align}
x_{m+1} = \arg \min_{x} g(x | x_{m})
\end{align}\]</div>
<p>The above iterative method will guarantee that <span class="math notranslate nohighlight">\(f(x_{m})\)</span> will converge to a local optimum or a saddle point as <span class="math notranslate nohighlight">\(m\)</span> goes to infinity.</p>
<div class="math notranslate nohighlight">
\[
f(x _{m+1}) \leq g(x _{m+1}|x _{m}) \leq g(x_{m}|x_{m}) = f(x_{m})
\]</div>
<p>In the following figure from Wikipedia illustrates the process for maximization.</p>
<p><img alt="Source: https://en.wikipedia.org/wiki/File:Mmalgorithm.jpg" src="_images/Mmalgorithm.jpg" /></p>
<p>A possible majorizing function for the stress can be obtained from the stress function,</p>
<div class="math notranslate nohighlight">
\[
S_M({z_i}) = \sum_{i &lt; j} \left(d_{ij} - |z_i - z_j|\right)^2
= \sum_{i &lt; j} d_{ij}^2 + \sum_{i &lt; j} |z_i - z_j|^2 - 2\sum_{i &lt; j} d_{ij}|z_i - z_j|
\]</div>
<p>The first term is constant. The second term is squared in the <span class="math notranslate nohighlight">\(z_i\)</span> and can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{align}
|z_i - z_j|^2 = |z_i|^2 + |z_j|^2 - 2\left&lt;z_i, z_j\right&gt;
\end{align}\]</div>
<p>so,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sum_{i&lt;j} |z_i - z_j|^2 =&amp; (n-1)\sum_{i}|z_i|^2 + - 2\sum_{i&gt;j}\left&lt;z_i, z_j\right&gt;\\
=&amp; (n-1)\sum_{i}|z_i|^2  - \sum_{i\neq j}\left&lt;z_i, z_j\right&gt;\\
=&amp; n\sum_{i}|z_i|^2  - \sum_{i, j}\left&lt;z_i, z_j\right&gt;\\
=&amp; n\left(\sum_{i}|z_i|^2  - \frac{1}{n}\sum_{i, j}\left&lt;z_i, z_j\right&gt;\right)\\
=&amp; tr\left(Z^T Z - \frac{1}{n} Z^T \vec{1}\vec{1}^T Z  \right)\\
=&amp; tr\left(nZ^T (I - M) Z \right)\\
=&amp; tr\left(Z^T (nP) Z \right)\\
=&amp; tr\left(Z^T V Z \right)
\end{align}\end{split}\]</div>
<p>Another way to derive the above expression is taken from the definitive book on MDS <span id="id1">[<a class="reference internal" href="#id68"><span>1</span></a>]</span>. Consider the distance <span class="math notranslate nohighlight">\(|z_i - z_j|\)</span>, it can be expressed as a sum over columns of the matrix <span class="math notranslate nohighlight">\(Z\)</span>, <span class="math notranslate nohighlight">\(z_{il} - z_{jl} = (e_i - e_j)^T Z_{:l}\)</span>, where the <span class="math notranslate nohighlight">\(e_i\)</span> are Cartesian unit versors. Each <span class="math notranslate nohighlight">\(e_i\)</span> selects component <span class="math notranslate nohighlight">\(i\)</span> from each column of <span class="math notranslate nohighlight">\(Z\)</span>, so that <span class="math notranslate nohighlight">\((z_{il} - z_{jl})^2 = Z_{:l}^T (e_i - e_j) (e_i - e_j)^T Z_{:l}\)</span>. The squared distance is then</p>
<div class="math notranslate nohighlight">
\[\begin{align}
| z_{i} - z_j | ^2 = \sum_l Z^T_{:l}(e_i - e_j)(e_i - e_j)^TZ_{:l} |
= \sum_l Z^T_{:l} A_{ij} Z_{:l}
= tr\left( Z^T A_{ij} Z \right)
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(A_{ij} = (e_i - e_j)(e_i - e_j)\)</span> is a matrix with <span class="math notranslate nohighlight">\(a_{ii} = a_{ij}1\)</span> and <span class="math notranslate nohighlight">\(a_{ij} = a_{ji} = -1\)</span>. Then,</p>
<div class="math notranslate nohighlight">
\[\begin{align}
\sum_{i &lt; j} | z_{i} - z_j | ^2
= \sum_{i &lt; j} tr(Z^TA_{ij}Z)
= tr\left(Z^T \left(\sum_{i &lt; j}A_{ij}\right) Z\right)
= tr\left(Z^T V Z\right)
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(V = \sum A_{ij} = nP\)</span>.</p>
<p>This second term is quadratic and easy to optimize.</p>
<p>Using the <a class="reference external" href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy–Schwarz inequality</a>, <span class="math notranslate nohighlight">\(|\left&lt;u,v\right&gt;\leq |u||v|\)</span>, we can obtain a majorizing function for the third term. For an arbitrary matrix <span class="math notranslate nohighlight">\(Y\)</span>, which will act as <span class="math notranslate nohighlight">\(x_{m}\)</span> in the MM algorithm,</p>
<div class="math notranslate nohighlight">
\[\begin{align}
(z_i -  z_j)^T (y_i - y_j) \leq |z_i - z_j| |y_i - y_j|
\end{align}\]</div>
<p>from which we can write</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
-|z_i - z_j| \leq&amp; -\frac{(z_i -  z_j)^T (y_i - y_j)}{|y_i - y_j|}\\
=&amp; -\frac{\sum_l Z^T_{:l} A_{ij} Y_{:l}}{|y_i - y_j|}\\
=&amp; -\frac{tr\left( Z^T A_{ij} Y \right)}{|y_i - y_j|}\\
=&amp; -tr\left( Z^T \frac{A_{ij}}{|y_i - y_j|} Y \right)
\end{align}\end{split}\]</div>
<p>Then, for the third term we have</p>
<p>The third term can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{align}
- \sum_{i &lt; j} d_{ij}|z_i - z_j|
\leq &amp; -tr\left( Z^T \left(\sum_{i &lt; j} b_{ij}A_{ij}\right) Y \right)
= -tr\left( Z^T B(Y) Y \right)
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(B(Y) = \left(\sum_{i &lt; j} b_{ij}A_{ij}\right)\)</span>, and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
b_{ij} =&amp;
\begin{cases}
\frac{d_{ij}}{|y_i - y_j|} &amp; i \neq j \text{ and }|y_i - y_j| &gt; 0\\
0 &amp; i \neq j \text{ and }|y_i - y_j| = 0
\end{cases}\\
b_{ii} =&amp; -\sum_{j=1,j\neq i}^n b_{ij}
\end{align}\end{split}\]</div>
<p>So the third term is majorized by a linear function.</p>
<p>For the complete stress,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
S_M({z_i}) = \sum_{i &lt; j} \left(d_{ij} - |z_i - z_j|\right)^2
=&amp; \sum_{i &lt; j} d_{ij}^2 + tr(Z^T V Z) - 2 tr(Z^T B(Z) Z)\\
\leq&amp; \sum_{i &lt; j} d_{ij}^2 + tr(Z^T V Z) - 2 tr(Z^T B(Y) Y) = \tau(Z,Y)
\end{align}\end{split}\]</div>
<p>Now, to optimize the majorizing function we take its derivative</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \tau}{Z} = 2VZ - 2 B(Z) Z = 0
\]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[
VZ = B(Z) Z
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(V\)</span> is not full rank, so it does not have an inverse. Still, we can use the Moore-Penrose inverse of <span class="math notranslate nohighlight">\(V\)</span>, <span class="math notranslate nohighlight">\(V^+ = (V + \vec{1}\vec{1}^T})^{-1} - \frac{\vec{1}\vec{1}^T}{n^{2} = n(I-M)\)</span>. The update is</p>
<div class="math notranslate nohighlight">
\[
X^u = \frac{1}{n}B(Z)Z
\]</div>
<p>which is called the Guttman transform.</p>
<p>So, to minimize the stress, we apply an MM algorithm using the above update rule. An implementation in PySpark can be found <a class="reference external" href="https://blog.paperspace.com/dimension-reduction-with-multi-dimension-scaling/">here</a>, Scikit Learn also implements SMACOF for MDS.</p>
</div>
<div class="section" id="sammon-mapping">
<h3>Sammon mapping<a class="headerlink" href="#sammon-mapping" title="Permalink to this headline">¶</a></h3>
<p>The Sammon mapping is a modification to the MDS stress function</p>
<div class="math notranslate nohighlight">
\[
S_{Sm}({z_i}) = \sum_{i &lt; j} \frac{\left(d_{ij} - |z_i - z_j|\right)^2}{d_{ij}}
\]</div>
<p>This stress weights each distance difference by the inverse of the original distance, and gives more importance to preserving small distances than large distances.</p>
</div>
<div class="section" id="shephard-kruskal-non-metric-scaling">
<h3>Shephard-Kruskal non-metric scaling<a class="headerlink" href="#shephard-kruskal-non-metric-scaling" title="Permalink to this headline">¶</a></h3>
<p>When the distances are only qualitative and we are interested in preserving only their relative ordering, we can optimize a stress that uses only ranks,</p>
<div class="math notranslate nohighlight">
\[
S_{NM}(\{z_i\}) = \frac{\sum_{i&lt;j}\left(|z_i - z_j| - \theta(d_{ij}) \right)^2}{\sum_{i&lt;j}|z_i-z_j|^2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> is a monotonically increasing function. Optimizing non-metric stress can be done in two steps, optimize over the <span class="math notranslate nohighlight">\(z_{i}\)</span> with fixed <span class="math notranslate nohighlight">\(\theta\)</span>, then finding an appropriate <span class="math notranslate nohighlight">\(\theta\)</span>, for example using isotonic regression.</p>
<div class="figure align-default" id="id69">
<img alt="_images/nm-mds.png" src="_images/nm-mds.png" />
<p class="caption"><span class="caption-number">Fig. 17 </span><span class="caption-text">Source: <a class="reference external" href="https://www.stat.pitt.edu/sungkyu/course/2221Fall13/lec8_mds_combined.pdf">https://www.stat.pitt.edu/sungkyu/course/2221Fall13/lec8_mds_combined.pdf</a></span><a class="headerlink" href="#id69" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="example-road-distance">
<h3>Example: Road Distance<a class="headerlink" href="#example-road-distance" title="Permalink to this headline">¶</a></h3>
<p>The following is a road distance (km) matrix between European cities. We use MDS to approximate coordinates in 2D.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">d_cities</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Data/eurodist.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;city&#39;</span><span class="p">)</span>
<span class="n">d_cities</span>
</pre></div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>city</p></th>
<th class="head"><p>Athens</p></th>
<th class="head"><p>Barcelona</p></th>
<th class="head"><p>Brussels</p></th>
<th class="head"><p>Calais</p></th>
<th class="head"><p>Cherbourg</p></th>
<th class="head"><p>Cologne</p></th>
<th class="head"><p>Copenhagen</p></th>
<th class="head"><p>Geneva</p></th>
<th class="head"><p>Gibraltar</p></th>
<th class="head"><p>Hamburg</p></th>
<th class="head"><p>Hook of Holland</p></th>
<th class="head"><p>Lisbon</p></th>
<th class="head"><p>Lyons</p></th>
<th class="head"><p>Madrid</p></th>
<th class="head"><p>Marseilles</p></th>
<th class="head"><p>Milan</p></th>
<th class="head"><p>Munich</p></th>
<th class="head"><p>Paris</p></th>
<th class="head"><p>Rome</p></th>
<th class="head"><p>Stockholm</p></th>
<th class="head"><p>Vienna</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Athens</p></td>
<td><p>0</p></td>
<td><p>3313</p></td>
<td><p>2963</p></td>
<td><p>3175</p></td>
<td><p>3339</p></td>
<td><p>2762</p></td>
<td><p>3276</p></td>
<td><p>2610</p></td>
<td><p>4485</p></td>
<td><p>2977</p></td>
<td><p>3030</p></td>
<td><p>4532</p></td>
<td><p>2753</p></td>
<td><p>3949</p></td>
<td><p>2865</p></td>
<td><p>2282</p></td>
<td><p>2179</p></td>
<td><p>3000</p></td>
<td><p>817</p></td>
<td><p>3927</p></td>
<td><p>1991</p></td>
</tr>
<tr class="row-odd"><td><p>Barcelona</p></td>
<td><p>3313</p></td>
<td><p>0</p></td>
<td><p>1318</p></td>
<td><p>1326</p></td>
<td><p>1294</p></td>
<td><p>1498</p></td>
<td><p>2218</p></td>
<td><p>803</p></td>
<td><p>1172</p></td>
<td><p>2018</p></td>
<td><p>1490</p></td>
<td><p>1305</p></td>
<td><p>645</p></td>
<td><p>636</p></td>
<td><p>521</p></td>
<td><p>1014</p></td>
<td><p>1365</p></td>
<td><p>1033</p></td>
<td><p>1460</p></td>
<td><p>2868</p></td>
<td><p>1802</p></td>
</tr>
<tr class="row-even"><td><p>Brussels</p></td>
<td><p>2963</p></td>
<td><p>1318</p></td>
<td><p>0</p></td>
<td><p>204</p></td>
<td><p>583</p></td>
<td><p>206</p></td>
<td><p>966</p></td>
<td><p>677</p></td>
<td><p>2256</p></td>
<td><p>597</p></td>
<td><p>172</p></td>
<td><p>2084</p></td>
<td><p>690</p></td>
<td><p>1558</p></td>
<td><p>1011</p></td>
<td><p>925</p></td>
<td><p>747</p></td>
<td><p>285</p></td>
<td><p>1511</p></td>
<td><p>1616</p></td>
<td><p>1175</p></td>
</tr>
<tr class="row-odd"><td><p>Calais</p></td>
<td><p>3175</p></td>
<td><p>1326</p></td>
<td><p>204</p></td>
<td><p>0</p></td>
<td><p>460</p></td>
<td><p>409</p></td>
<td><p>1136</p></td>
<td><p>747</p></td>
<td><p>2224</p></td>
<td><p>714</p></td>
<td><p>330</p></td>
<td><p>2052</p></td>
<td><p>739</p></td>
<td><p>1550</p></td>
<td><p>1059</p></td>
<td><p>1077</p></td>
<td><p>977</p></td>
<td><p>280</p></td>
<td><p>1662</p></td>
<td><p>1786</p></td>
<td><p>1381</p></td>
</tr>
<tr class="row-even"><td><p>Cherbourg</p></td>
<td><p>3339</p></td>
<td><p>1294</p></td>
<td><p>583</p></td>
<td><p>460</p></td>
<td><p>0</p></td>
<td><p>785</p></td>
<td><p>1545</p></td>
<td><p>853</p></td>
<td><p>2047</p></td>
<td><p>1115</p></td>
<td><p>731</p></td>
<td><p>1827</p></td>
<td><p>789</p></td>
<td><p>1347</p></td>
<td><p>1101</p></td>
<td><p>1209</p></td>
<td><p>1160</p></td>
<td><p>340</p></td>
<td><p>1794</p></td>
<td><p>2196</p></td>
<td><p>1588</p></td>
</tr>
<tr class="row-odd"><td><p>Cologne</p></td>
<td><p>2762</p></td>
<td><p>1498</p></td>
<td><p>206</p></td>
<td><p>409</p></td>
<td><p>785</p></td>
<td><p>0</p></td>
<td><p>760</p></td>
<td><p>1662</p></td>
<td><p>2436</p></td>
<td><p>460</p></td>
<td><p>269</p></td>
<td><p>2290</p></td>
<td><p>714</p></td>
<td><p>1764</p></td>
<td><p>1035</p></td>
<td><p>911</p></td>
<td><p>583</p></td>
<td><p>465</p></td>
<td><p>1497</p></td>
<td><p>1403</p></td>
<td><p>937</p></td>
</tr>
<tr class="row-even"><td><p>Copenhagen</p></td>
<td><p>3276</p></td>
<td><p>2218</p></td>
<td><p>966</p></td>
<td><p>1136</p></td>
<td><p>1545</p></td>
<td><p>760</p></td>
<td><p>0</p></td>
<td><p>1418</p></td>
<td><p>3196</p></td>
<td><p>460</p></td>
<td><p>269</p></td>
<td><p>2971</p></td>
<td><p>1458</p></td>
<td><p>2498</p></td>
<td><p>1778</p></td>
<td><p>1537</p></td>
<td><p>1104</p></td>
<td><p>1176</p></td>
<td><p>2050</p></td>
<td><p>650</p></td>
<td><p>1455</p></td>
</tr>
<tr class="row-odd"><td><p>Geneva</p></td>
<td><p>2610</p></td>
<td><p>803</p></td>
<td><p>677</p></td>
<td><p>747</p></td>
<td><p>853</p></td>
<td><p>1662</p></td>
<td><p>1418</p></td>
<td><p>0</p></td>
<td><p>1975</p></td>
<td><p>1118</p></td>
<td><p>895</p></td>
<td><p>1936</p></td>
<td><p>158</p></td>
<td><p>1439</p></td>
<td><p>425</p></td>
<td><p>328</p></td>
<td><p>591</p></td>
<td><p>513</p></td>
<td><p>995</p></td>
<td><p>2068</p></td>
<td><p>1019</p></td>
</tr>
<tr class="row-even"><td><p>Gibraltar</p></td>
<td><p>4485</p></td>
<td><p>1172</p></td>
<td><p>2256</p></td>
<td><p>2224</p></td>
<td><p>2047</p></td>
<td><p>2436</p></td>
<td><p>3196</p></td>
<td><p>1975</p></td>
<td><p>0</p></td>
<td><p>2897</p></td>
<td><p>2428</p></td>
<td><p>676</p></td>
<td><p>1817</p></td>
<td><p>698</p></td>
<td><p>1693</p></td>
<td><p>2185</p></td>
<td><p>2565</p></td>
<td><p>1971</p></td>
<td><p>2631</p></td>
<td><p>3886</p></td>
<td><p>2974</p></td>
</tr>
<tr class="row-odd"><td><p>Hamburg</p></td>
<td><p>2977</p></td>
<td><p>2018</p></td>
<td><p>597</p></td>
<td><p>714</p></td>
<td><p>1115</p></td>
<td><p>460</p></td>
<td><p>460</p></td>
<td><p>1118</p></td>
<td><p>2897</p></td>
<td><p>0</p></td>
<td><p>550</p></td>
<td><p>2671</p></td>
<td><p>1159</p></td>
<td><p>2198</p></td>
<td><p>1479</p></td>
<td><p>1238</p></td>
<td><p>805</p></td>
<td><p>877</p></td>
<td><p>1751</p></td>
<td><p>949</p></td>
<td><p>1155</p></td>
</tr>
<tr class="row-even"><td><p>Hook of Holland</p></td>
<td><p>3030</p></td>
<td><p>1490</p></td>
<td><p>172</p></td>
<td><p>330</p></td>
<td><p>731</p></td>
<td><p>269</p></td>
<td><p>269</p></td>
<td><p>895</p></td>
<td><p>2428</p></td>
<td><p>550</p></td>
<td><p>0</p></td>
<td><p>2280</p></td>
<td><p>863</p></td>
<td><p>1730</p></td>
<td><p>1183</p></td>
<td><p>1098</p></td>
<td><p>851</p></td>
<td><p>457</p></td>
<td><p>1683</p></td>
<td><p>1500</p></td>
<td><p>1205</p></td>
</tr>
<tr class="row-odd"><td><p>Lisbon</p></td>
<td><p>4532</p></td>
<td><p>1305</p></td>
<td><p>2084</p></td>
<td><p>2052</p></td>
<td><p>1827</p></td>
<td><p>2290</p></td>
<td><p>2971</p></td>
<td><p>1936</p></td>
<td><p>676</p></td>
<td><p>2671</p></td>
<td><p>2280</p></td>
<td><p>0</p></td>
<td><p>1178</p></td>
<td><p>668</p></td>
<td><p>1762</p></td>
<td><p>2250</p></td>
<td><p>2507</p></td>
<td><p>1799</p></td>
<td><p>2700</p></td>
<td><p>3231</p></td>
<td><p>2937</p></td>
</tr>
<tr class="row-even"><td><p>Lyons</p></td>
<td><p>2753</p></td>
<td><p>645</p></td>
<td><p>690</p></td>
<td><p>739</p></td>
<td><p>789</p></td>
<td><p>714</p></td>
<td><p>1458</p></td>
<td><p>158</p></td>
<td><p>1817</p></td>
<td><p>1159</p></td>
<td><p>863</p></td>
<td><p>1178</p></td>
<td><p>0</p></td>
<td><p>1281</p></td>
<td><p>320</p></td>
<td><p>328</p></td>
<td><p>724</p></td>
<td><p>471</p></td>
<td><p>1048</p></td>
<td><p>2108</p></td>
<td><p>1157</p></td>
</tr>
<tr class="row-odd"><td><p>Madrid</p></td>
<td><p>3949</p></td>
<td><p>636</p></td>
<td><p>1558</p></td>
<td><p>1550</p></td>
<td><p>1347</p></td>
<td><p>1764</p></td>
<td><p>2498</p></td>
<td><p>1439</p></td>
<td><p>698</p></td>
<td><p>2198</p></td>
<td><p>1730</p></td>
<td><p>668</p></td>
<td><p>1281</p></td>
<td><p>0</p></td>
<td><p>1157</p></td>
<td><p>1724</p></td>
<td><p>2010</p></td>
<td><p>1273</p></td>
<td><p>2097</p></td>
<td><p>3188</p></td>
<td><p>2409</p></td>
</tr>
<tr class="row-even"><td><p>Marseilles</p></td>
<td><p>2865</p></td>
<td><p>521</p></td>
<td><p>1011</p></td>
<td><p>1059</p></td>
<td><p>1101</p></td>
<td><p>1035</p></td>
<td><p>1778</p></td>
<td><p>425</p></td>
<td><p>1693</p></td>
<td><p>1479</p></td>
<td><p>1183</p></td>
<td><p>1762</p></td>
<td><p>320</p></td>
<td><p>1157</p></td>
<td><p>0</p></td>
<td><p>618</p></td>
<td><p>1109</p></td>
<td><p>792</p></td>
<td><p>1011</p></td>
<td><p>2428</p></td>
<td><p>1363</p></td>
</tr>
<tr class="row-odd"><td><p>Milan</p></td>
<td><p>2282</p></td>
<td><p>1014</p></td>
<td><p>925</p></td>
<td><p>1077</p></td>
<td><p>1209</p></td>
<td><p>911</p></td>
<td><p>1537</p></td>
<td><p>328</p></td>
<td><p>2185</p></td>
<td><p>1238</p></td>
<td><p>1098</p></td>
<td><p>2250</p></td>
<td><p>328</p></td>
<td><p>1724</p></td>
<td><p>618</p></td>
<td><p>0</p></td>
<td><p>331</p></td>
<td><p>856</p></td>
<td><p>586</p></td>
<td><p>2187</p></td>
<td><p>898</p></td>
</tr>
<tr class="row-even"><td><p>Munich</p></td>
<td><p>2179</p></td>
<td><p>1365</p></td>
<td><p>747</p></td>
<td><p>977</p></td>
<td><p>1160</p></td>
<td><p>583</p></td>
<td><p>1104</p></td>
<td><p>591</p></td>
<td><p>2565</p></td>
<td><p>805</p></td>
<td><p>851</p></td>
<td><p>2507</p></td>
<td><p>724</p></td>
<td><p>2010</p></td>
<td><p>1109</p></td>
<td><p>331</p></td>
<td><p>0</p></td>
<td><p>821</p></td>
<td><p>946</p></td>
<td><p>1754</p></td>
<td><p>428</p></td>
</tr>
<tr class="row-odd"><td><p>Paris</p></td>
<td><p>3000</p></td>
<td><p>1033</p></td>
<td><p>285</p></td>
<td><p>280</p></td>
<td><p>340</p></td>
<td><p>465</p></td>
<td><p>1176</p></td>
<td><p>513</p></td>
<td><p>1971</p></td>
<td><p>877</p></td>
<td><p>457</p></td>
<td><p>1799</p></td>
<td><p>471</p></td>
<td><p>1273</p></td>
<td><p>792</p></td>
<td><p>856</p></td>
<td><p>821</p></td>
<td><p>0</p></td>
<td><p>1476</p></td>
<td><p>1827</p></td>
<td><p>1249</p></td>
</tr>
<tr class="row-even"><td><p>Rome</p></td>
<td><p>817</p></td>
<td><p>1460</p></td>
<td><p>1511</p></td>
<td><p>1662</p></td>
<td><p>1794</p></td>
<td><p>1497</p></td>
<td><p>2050</p></td>
<td><p>995</p></td>
<td><p>2631</p></td>
<td><p>1751</p></td>
<td><p>1683</p></td>
<td><p>2700</p></td>
<td><p>1048</p></td>
<td><p>2097</p></td>
<td><p>1011</p></td>
<td><p>586</p></td>
<td><p>946</p></td>
<td><p>1476</p></td>
<td><p>0</p></td>
<td><p>2707</p></td>
<td><p>1209</p></td>
</tr>
<tr class="row-odd"><td><p>Stockholm</p></td>
<td><p>3927</p></td>
<td><p>2868</p></td>
<td><p>1616</p></td>
<td><p>1786</p></td>
<td><p>2196</p></td>
<td><p>1403</p></td>
<td><p>650</p></td>
<td><p>2068</p></td>
<td><p>3886</p></td>
<td><p>949</p></td>
<td><p>1500</p></td>
<td><p>3231</p></td>
<td><p>2108</p></td>
<td><p>3188</p></td>
<td><p>2428</p></td>
<td><p>2187</p></td>
<td><p>1754</p></td>
<td><p>1827</p></td>
<td><p>2707</p></td>
<td><p>0</p></td>
<td><p>2105</p></td>
</tr>
<tr class="row-even"><td><p>Vienna</p></td>
<td><p>1991</p></td>
<td><p>1802</p></td>
<td><p>1175</p></td>
<td><p>1381</p></td>
<td><p>1588</p></td>
<td><p>937</p></td>
<td><p>1455</p></td>
<td><p>1019</p></td>
<td><p>2974</p></td>
<td><p>1155</p></td>
<td><p>1205</p></td>
<td><p>2937</p></td>
<td><p>1157</p></td>
<td><p>2409</p></td>
<td><p>1363</p></td>
<td><p>898</p></td>
<td><p>428</p></td>
<td><p>1249</p></td>
<td><p>1209</p></td>
<td><p>2105</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">MDS</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">d_cities</span><span class="o">.</span><span class="n">values</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">MDS</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dissimilarity</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Z</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Z</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">d_cities</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" src="_images/e72706b5162eb5846eae92b35c49d1d7cf48ffe1.png" /></p>
<p>The output approximates the European layout up to rotations and reflections.</p>
</div>
<div class="section" id="example-digits">
<h3>Example: Digits<a class="headerlink" href="#example-digits" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">(</span><span class="n">n_class</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>

<span class="n">mds</span> <span class="o">=</span> <span class="n">MDS</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">mds</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">Zd</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="n">i</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Zd</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Zd</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
<p><img alt="" src="_images/96cb2a5651710982f9fc6c6bba653091958b9b62.png" /></p>
</div>
<div class="section" id="example-correlation-tables">
<h3>Example: Correlation tables<a class="headerlink" href="#example-correlation-tables" title="Permalink to this headline">¶</a></h3>
<p>From Wikipedia</p>
<p>The Big Five personality traits, also known as the five-factor model (FFM) and the OCEAN model, is a taxonomy, or grouping, for personality traits. When factor analysis (a statistical technique) is applied to personality survey data, some words used to describe aspects of personality are often applied to the same person. For example, someone described as conscientious is more likely to be described as “always prepared” rather than “messy”. This theory is based therefore on the association between words but not on neuropsychological experiments. This theory uses descriptors of common language and therefore suggests five broad dimensions commonly used to describe the human personality and psyche.</p>
<p>We load a data set with answers to the Big Five questionnaire, and find the correlation among questions. The idea is that questions corresponding to the same personality trait should be answered in a similar way by similar people. The correlation matrix can be transformed into a distance matrix using <span class="math notranslate nohighlight">\(d = 1 - |\rho|\)</span>, and the MDS analysis is performed. Clusters for questions belonging to similar traits are identifiable.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b5</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Data/bigfive.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s1">&#39;Subnum&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">b5</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-example notranslate"><div class="highlight"><pre><span></span>         Q1  Q2   Q3   Q4   Q5   Q6   Q7   Q8  Q9  Q10  ...  Q35  Q36  Q37  \
Subnum                                                  ...                  
1       3.0   4  4.0  2.0  3.0  4.0  5.0  4.0   5    5  ...  5.0  3.0  3.0   
2       4.0   4  3.0  2.0  3.0  3.0  3.0  4.0   3    5  ...  4.0  5.0  5.0   
3       2.0   2  5.0  1.0  3.0  4.0  4.0  4.0   5    3  ...  5.0  4.0  2.0   
4       3.0   3  4.0  4.0  2.0  4.0  5.0  2.0   4    4  ...  4.0  3.0  2.0   
5       2.0   4  4.0  4.0  4.0  5.0  3.0  2.0   1    4  ...  2.0  2.0  1.0   

        Q38  Q39  Q40  Q41  Q42  Q43  Q44  
Subnum                                     
1       2.0  3.0  1.0  4.0  2.0  4.0    4  
2       4.0  4.0  4.0  3.0  5.0  4.0    5  
3       3.0  2.0  1.0  NaN  5.0  2.0    4  
4       2.0  3.0  3.0  4.0  4.0  4.0    4  
5       3.0  4.0  5.0  4.0  1.0  2.0    4  

[5 rows x 44 columns]
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">b5</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">factors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;E&quot;</span><span class="p">,</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;N&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;E&quot;</span><span class="p">,</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;N&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;E&quot;</span><span class="p">,</span>
           <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;N&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;E&quot;</span><span class="p">,</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;N&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;E&quot;</span><span class="p">,</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span>
           <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;N&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;E&quot;</span><span class="p">,</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;N&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;E&quot;</span><span class="p">,</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span>
           <span class="s2">&quot;N&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;E&quot;</span><span class="p">,</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;N&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">]</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;O&#39;</span><span class="p">:</span> <span class="s1">&#39;cyan&#39;</span><span class="p">,</span> <span class="s1">&#39;E&#39;</span><span class="p">:</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">:</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;A&#39;</span><span class="p">:</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">:</span><span class="s1">&#39;red&#39;</span><span class="p">}</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">cmap</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">factors</span><span class="p">]</span>

<span class="n">mds</span> <span class="o">=</span> <span class="n">MDS</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dissimilarity</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">mds</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Z</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Z</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">factors</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="s1">&#39;xx-large&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="_images/a0ab6f1fa9611687dddc88a4f90907159d48ccb1.png" /></p>
</div>
</div>
<div class="section" id="non-negative-matrix-factorization-nnmf">
<h2>Non-negative matrix factorization (NNMF)<a class="headerlink" href="#non-negative-matrix-factorization-nnmf" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="random-projection">
<h2>Random Projection<a class="headerlink" href="#random-projection" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="id2"><dl class="citation">
<dt class="label" id="id68"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Ingwer Borg and Patrick JF Groenen. <em>Modern multidimensional scaling: Theory and applications</em>. Springer Science &amp; Business Media, 2005.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="U2-M2-L2-PCA_variants.html" title="previous page">Some linear and non-linear variants of PCA</a>
    <a class='right-next' id="next-link" href="U2-M2-L4-Non_Linear_Methods_for_DR.html" title="next page">Non-linear Dimensionality Reduction</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Gonzalo G. Peraza Mues<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>